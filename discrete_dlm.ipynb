{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48d0bdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('/media/linux-stuff/gpt2-diff/scripts')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import device\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.config import gpt2config\n",
    "from scripts.model import DiffusionLM, LMEmbedding, Denoiser, Decoding\n",
    "from scripts.utils import (\n",
    "    MyTokenizer, \n",
    "    get_next_log_filename, \n",
    "    save_checkpoint, \n",
    "    load_checkpoint,\n",
    "    posterior_mean,\n",
    "    rounding_weight,\n",
    "    get_batch,\n",
    "    finalize_tokens,\n",
    "    reverse_diffusion_with_clamping,\n",
    "    visualize_embeddings_2d,\n",
    "    fwd_diffusion\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92ce09c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Hello, tiktoken is fast!<eos><pad><pad><pad>'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MyTokenizer(max_len=13)\n",
    "tokenizer.decode(tokenizer.encode(\"Hello, tiktoken is fast!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc203bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model parameters: 100.46M\n",
      "gpt2config(n_vocab=50260, n_layer=12, n_embed=128, n_context=1024, n_head=12, n_timesteps=1000, mlp_expansion=4, n_latent=768)\n"
     ]
    }
   ],
   "source": [
    "config = gpt2config(n_vocab=tokenizer.n_vocab,n_embed=128,n_head= 12, mlp_expansion=4,n_latent=768)\n",
    "model = DiffusionLM(config).to(device)\n",
    "print(f\"Total Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6a6a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 18007897 characters\n",
      "Number of samples: 78528\n",
      "First sample: The boy went to a video arcade. He played his favorite machine. His games didn't go very well. He told the owner about his experience. The owner explained that he had made the game settings harder.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load E2E dataset - extract text from 'ref' column\n",
    "df = pd.read_csv('datasets/ROCStories/rocstories_train.csv')\n",
    "text = ' '.join(df['ref'].tolist())\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"First sample: {df['ref'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7e1fa905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 70675, Test samples: 7853\n",
      "\n",
      "Encoding training data...\n",
      "Encoded 5000/70675 train samples\n",
      "Encoded 10000/70675 train samples\n",
      "Encoded 15000/70675 train samples\n",
      "Encoded 20000/70675 train samples\n",
      "Encoded 25000/70675 train samples\n",
      "Encoded 30000/70675 train samples\n",
      "Encoded 35000/70675 train samples\n",
      "Encoded 40000/70675 train samples\n",
      "Encoded 45000/70675 train samples\n",
      "Encoded 50000/70675 train samples\n",
      "Encoded 55000/70675 train samples\n",
      "Encoded 60000/70675 train samples\n",
      "Encoded 65000/70675 train samples\n",
      "Encoded 70000/70675 train samples\n",
      "\n",
      "Encoding test data...\n",
      "\n",
      "Train encoded shape: torch.Size([70675, 64])\n",
      "Test encoded shape: torch.Size([7853, 64])\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "m,n = (32,32)\n",
    "train_size = int(0.9 * len(df))\n",
    "train_df = df[:train_size].reset_index(drop=True)\n",
    "test_df = df[train_size:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "# Pre-encode all sequences for training efficiency\n",
    "print(\"\\nEncoding training data...\")\n",
    "train_encoded = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    encoded = tokenizer.encode(row['ref'], max_len=m+n)  # Use fixed sequence length\n",
    "    train_encoded.append(encoded)\n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"Encoded {idx + 1}/{len(train_df)} train samples\")\n",
    "\n",
    "print(\"\\nEncoding test data...\")\n",
    "test_encoded = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    encoded = tokenizer.encode(row['ref'], max_len=64)\n",
    "    test_encoded.append(encoded)\n",
    "\n",
    "# Convert to tensors\n",
    "train_encoded = torch.tensor(train_encoded, dtype=torch.long)\n",
    "test_encoded = torch.tensor(test_encoded, dtype=torch.long)\n",
    "\n",
    "print(f\"\\nTrain encoded shape: {train_encoded.shape}\")\n",
    "print(f\"Test encoded shape: {test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cdf80d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "max_iters = 100000  \n",
    "learning_rate = 3e-3\n",
    "eval_iters = 100  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 16  # Larger batch for better GPU utilization\n",
    "T = 1000\n",
    "num_timestep_samples = 4  # Sample 8 timesteps per iteration for better gradient estimate\n",
    "m,n = (32,32)\n",
    "sequence_length = m + n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "113dd3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha bars range: [0.0010, 0.9990]\n",
      "Alphas range: [0.6665, 0.9990]\n"
     ]
    }
   ],
   "source": [
    "# Fixed alpha schedule - simple sqrt schedule\n",
    "t = torch.arange(0, T+1, device=device, dtype=torch.float32)\n",
    "alpha_bars = 1 - torch.sqrt(t / T)  # Goes from ~0 to 1-sqrt(1)=0\n",
    "alpha_bars = torch.clamp(alpha_bars, min=0.001, max=0.999)\n",
    "alphas = torch.zeros(T+1, device=device) #alpha_0 to alpha_T\n",
    "alphas[0] = alpha_bars[0]\n",
    "alphas[1:] = alpha_bars[1:] / alpha_bars[:-1]\n",
    "alphas = torch.clamp(alphas, min=0.001, max=0.999)\n",
    "\n",
    "# Precompute sqrt terms for efficiency\n",
    "sqrt_ab = torch.sqrt(alpha_bars)\n",
    "sqrt_1mab = torch.sqrt(1 - alpha_bars)\n",
    "\n",
    "print(f\"Alpha bars range: [{alpha_bars.min():.4f}, {alpha_bars.max():.4f}]\")\n",
    "print(f\"Alphas range: [{alphas.min():.4f}, {alphas.max():.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3eb2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_model = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=0.0)\n",
    "lr_lambda = lambda step: 1.0 - (step / float(max_iters))\n",
    "scheduler_model = LambdaLR(optimizer_model, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "88f85cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: logs/log_22.txt\n",
      "Iter 0: loss = 3.7886, denoising = 1.4091, anchor = 1.3669\n",
      "  Top-5 sampled timesteps: [2, 1, 3, 5, 4] with probs: ['0.0010', '0.0010', '0.0010', '0.0010', '0.0010']\n",
      "Iter 100: loss = 1.8429, denoising = 0.4812, anchor = 0.5981\n",
      "  Top-5 sampled timesteps: [958, 880, 851, 146, 396] with probs: ['0.0010', '0.0010', '0.0010', '0.0010', '0.0010']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m denoising_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Get importance-based sampling probabilities\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m sampling_probs \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportance_warmup_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Sample timesteps according to importance distribution\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Each sample in batch gets the same set of timesteps for simplicity\u001b[39;00m\n\u001b[1;32m     68\u001b[0m sampled_timesteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(sampling_probs, num_timestep_samples, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# +1 because probs are for t in [1,T]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[86], line 26\u001b[0m, in \u001b[0;36mget_importance_probs\u001b[0;34m(loss_buffer, buffer_counts, iteration, warmup_iters, T)\u001b[0m\n\u001b[1;32m     24\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mint\u001b[39m(buffer_counts[t]\u001b[38;5;241m.\u001b[39mitem()), loss_buffer_size)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     rms_losses[t] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mmean(loss_buffer[t, :count] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     rms_losses[t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Default for unvisited timesteps\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_file = get_next_log_filename('logs')\n",
    "print(f\"Logging to: {log_file}\")\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(\"Training the SEQ2SEQ Diffusion Language Model\\n\")\n",
    "    f.write(\"Iteration,Total_Loss,Denoising_Loss,Posterior_Loss,Anchor_Loss,Rounding_Loss\\n\")\n",
    "\n",
    "checkpoint_counter = 0\n",
    "\n",
    "# Importance sampling setup\n",
    "loss_buffer_size = 10\n",
    "# Loss buffer for each timestep: shape (T, buffer_size), initialized with ones for uniform start\n",
    "loss_buffer = torch.ones((T + 1, loss_buffer_size), device=device)  # timesteps 0 to T\n",
    "buffer_counts = torch.zeros(T + 1, device=device, dtype=torch.long)  # track how many samples per timestep\n",
    "\n",
    "# Warmup iterations before fully using importance sampling\n",
    "importance_warmup_iters = 5000\n",
    "\n",
    "def get_importance_probs(loss_buffer, buffer_counts, iteration, warmup_iters, T):\n",
    "    \"\"\"Compute sampling probabilities based on RMS of loss buffer.\"\"\"\n",
    "    # Compute RMS for each timestep (only for t >= 1)\n",
    "    rms_losses = torch.zeros(T + 1, device=loss_buffer.device)\n",
    "    for t in range(1, T + 1):\n",
    "        count = min(int(buffer_counts[t].item()), loss_buffer_size)\n",
    "        if count > 0:\n",
    "            rms_losses[t] = torch.sqrt(torch.mean(loss_buffer[t, :count] ** 2))\n",
    "        else:\n",
    "            rms_losses[t] = 1.0  # Default for unvisited timesteps\n",
    "    \n",
    "    # Normalize to get probabilities (only for t in [1, T])\n",
    "    importance_probs = rms_losses[1:T+1]  # shape (T,)\n",
    "    importance_probs = importance_probs / (importance_probs.sum() + 1e-8)\n",
    "    \n",
    "    # Uniform distribution\n",
    "    uniform_probs = torch.ones(T, device=loss_buffer.device) / T\n",
    "    \n",
    "    # Blend: gradually shift from uniform to importance-based\n",
    "    blend_factor = min(1.0, iteration / warmup_iters)\n",
    "    final_probs = (1 - blend_factor) * uniform_probs + blend_factor * importance_probs\n",
    "    \n",
    "    # Ensure valid probability distribution\n",
    "    final_probs = final_probs / (final_probs.sum() + 1e-8)\n",
    "    \n",
    "    return final_probs\n",
    "\n",
    "for it in range(0, max_iters):\n",
    "\n",
    "    w = get_batch('train', batch_size, sequence_length, train_encoded=train_encoded, test_encoded=test_encoded, device=device)\n",
    "    w_emb = model.embedding(w)\n",
    "\n",
    "    # Split embeddings: first m tokens (prefix) and last n tokens (suffix to be noised)\n",
    "    w_emb_prefix = w_emb[:, :m, :]  # (batch, m, embed_dim) - stays unchanged\n",
    "    w_emb_suffix = w_emb[:, m:, :]  # (batch, n, embed_dim) - will be noised\n",
    "\n",
    "    # Only add initial perturbation to suffix\n",
    "    x0_suffix = w_emb_suffix + 0.1 * torch.randn_like(w_emb_suffix)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Noise only for suffix part\n",
    "    eps = torch.randn_like(x0_suffix)\n",
    "    denoising_loss = 0.0\n",
    "    \n",
    "    # Get importance-based sampling probabilities\n",
    "    sampling_probs = get_importance_probs(loss_buffer, buffer_counts, it, importance_warmup_iters, T)\n",
    "    \n",
    "    # Sample timesteps according to importance distribution\n",
    "    # Each sample in batch gets the same set of timesteps for simplicity\n",
    "    sampled_timesteps = torch.multinomial(sampling_probs, num_timestep_samples, replacement=True) + 1  # +1 because probs are for t in [1,T]\n",
    "    \n",
    "    for t_sample in sampled_timesteps:\n",
    "        t_random = t_sample.expand(batch_size)  # Same timestep for all batch items\n",
    "        t_idx = t_random\n",
    "        sqrt_ab_t = sqrt_ab[t_idx].view(batch_size, 1, 1)\n",
    "        sqrt_1mab_t = sqrt_1mab[t_idx].view(batch_size, 1, 1)\n",
    "        \n",
    "        # Forward diffusion only on suffix\n",
    "        xt_suffix = sqrt_ab_t * x0_suffix + sqrt_1mab_t * eps\n",
    "        # Concatenate unchanged prefix with noised suffix\n",
    "        xt = torch.cat([w_emb_prefix, xt_suffix], dim=1)\n",
    "        x0_hat = model.denoiser(xt, t_random)\n",
    "        # x0_hat = torch.clamp(x0_hat, min=-10.0, max=10.0)\n",
    "        # Only compute denoising loss on suffix part\n",
    "        x0_hat_suffix = x0_hat[:, m:, :]\n",
    "        \n",
    "        # Compute loss for this timestep\n",
    "        timestep_loss = F.mse_loss(x0_hat_suffix, x0_suffix)\n",
    "        denoising_loss += timestep_loss\n",
    "        \n",
    "        # Update loss buffer for this timestep\n",
    "        t_val = t_sample.item()\n",
    "        buffer_idx = int(buffer_counts[t_val].item()) % loss_buffer_size\n",
    "        loss_buffer[t_val, buffer_idx] = timestep_loss.detach()\n",
    "        buffer_counts[t_val] += 1\n",
    "    \n",
    "    x0_target = torch.cat([w_emb_prefix, x0_suffix], dim=1)\n",
    "    denoising_loss = denoising_loss / num_timestep_samples \n",
    "    total_loss += denoising_loss\n",
    "    \n",
    "    # t_T = torch.full((batch_size,), T, device=device)\n",
    "    # xT_suffix = sqrt_ab[-1] * x0_suffix + sqrt_1mab[-1] * eps\n",
    "    # xT = torch.cat([w_emb_prefix, xT_suffix], dim=1)\n",
    "    # x0_hat_T = model.denoiser(xT, t_T)\n",
    "    # x0_hat_T = torch.clamp(x0_hat_T, min=-10.0, max=10.0)\n",
    "    # mu_hat_T = posterior_mean(xT_suffix, x0_suffix, T, alpha_bars, alphas)\n",
    "    # posterior_loss = torch.tensor(0.0, device=device)\n",
    "    # posterior_loss = F.mse_loss(mu_hat_T, torch.zeros_like(mu_hat_T)) \n",
    "    # total_loss += posterior_loss\n",
    "    \n",
    "    # Anchor loss: only noise suffix at t=1\n",
    "    xt_1_suffix = sqrt_ab[1] * x0_suffix + sqrt_1mab[1] * torch.rand_like(x0_suffix)\n",
    "    xt_1 = torch.cat([w_emb_prefix, xt_1_suffix], dim=1)\n",
    "    x0_hat_1 = model.denoiser(xt_1, torch.ones(batch_size, device=device))\n",
    "    # x0_hat_1 = torch.clamp(x0_hat_1, min=-10.0, max=10.0)\n",
    "    # Anchor loss on full sequence (prefix should reconstruct prefix, suffix should reconstruct suffix)\n",
    "    anchor_loss = F.mse_loss(x0_hat_1, w_emb) \n",
    "    total_loss += anchor_loss\n",
    "\n",
    "    reg_loss = torch.mean(x0_target**2)\n",
    "    total_loss += reg_loss\n",
    "    \n",
    "    if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TRAINING STOPPED: NaN/Inf detected at iteration {it}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Loss Diagnostics:\")\n",
    "        print(f\"  Total Loss:     {total_loss.item() if not torch.isnan(total_loss) else 'NaN'}\")\n",
    "        print(f\"  Denoising:      {denoising_loss.item()}\")\n",
    "        # print(f\"  Posterior:      {posterior_loss.item()}\")\n",
    "        print(f\"  Anchor:         {anchor_loss.item()}\")\n",
    "        print(f\"  Regularizing_loss:       {reg_loss.item()}\")\n",
    "        print(f\"\\nModel Output Statistics:\")\n",
    "        print(f\"  x0_hat range:   [{x0_hat.min().item():.2f}, {x0_hat.max().item():.2f}]\")\n",
    "        print(f\"\\nGradient Statistics:\")\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        print(f\"  Total grad norm: {total_norm:.4f}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        break\n",
    "    \n",
    "    optimizer_model.zero_grad(set_to_none=True)\n",
    "    total_loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "    optimizer_model.step()\n",
    "    scheduler_model.step()\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{it},{total_loss.item():.6f},{denoising_loss.item():.6f},{anchor_loss.item():.6f}\\n\")\n",
    "\n",
    "    if it % eval_iters == 0:\n",
    "        # Show sampling distribution stats\n",
    "        probs = get_importance_probs(loss_buffer, buffer_counts, it, importance_warmup_iters, T)\n",
    "        top_probs, top_t = torch.topk(probs, 5)\n",
    "        print(f\"Iter {it}: loss = {total_loss.item():.4f}, denoising = {denoising_loss.item():.4f}, anchor = {anchor_loss.item():.4f}\")\n",
    "        print(f\"  Top-5 sampled timesteps: {(top_t + 1).tolist()} with probs: {[f'{p:.4f}' for p in top_probs.tolist()]}\")\n",
    "\n",
    "    # if it % 5000 == 0 and it > 0:\n",
    "    #     checkpoint_name = f\"training_ckpt_{checkpoint_counter % 2}\"\n",
    "    #     save_checkpoint(model, config, alpha_bars, T, checkpoint_name, save_individual=False)\n",
    "    #     checkpoint_counter += 1\n",
    "\n",
    "print(f\"\\nTraining complete! Logs saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_s2s_diffusion(model, config, tokenizer, input_text, alpha_bars, T, m=32, n=32,\n",
    "                      batch_size=1, clamping_start=0.4, skip_step=1, display_at_steps=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Reverse diffusion for seq2seq: condition on first m tokens, generate last n tokens.\n",
    "    \n",
    "    Args:\n",
    "        m: number of prefix tokens (conditioning)\n",
    "        n: number of suffix tokens (to generate)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode input and get prefix embeddings\n",
    "    input_tokens = tokenizer.encode(input_text, raw=True)\n",
    "    x_prefix = model.embedding(torch.tensor(input_tokens, device=device).unsqueeze(0))\n",
    "    \n",
    "    # Take first m tokens as conditioning prefix\n",
    "    x_prefix = x_prefix[:, :m, :]  # (1, m, n_embed)\n",
    "    x_prefix = x_prefix.repeat(batch_size, 1, 1)  # (batch_size, m, n_embed)\n",
    "    \n",
    "    # Initialize suffix with random noise (this is what we'll denoise)\n",
    "    y_t = torch.randn(batch_size, n, config.n_embed, device=device)  # (batch_size, n, n_embed)\n",
    "    \n",
    "    # Concatenate: [prefix (clean) | suffix (noisy)]\n",
    "    z_t = torch.cat([x_prefix, y_t], dim=1)  # (batch_size, m+n, n_embed)\n",
    "    \n",
    "    # Show initial random state\n",
    "    initial_tokens = finalize_tokens(z_t, model.embedding.embed.weight)\n",
    "    initial_text = tokenizer.decode(initial_tokens[0].tolist())\n",
    "    initial_text_clean = tokenizer.clean_text(initial_text)\n",
    "    print(f\"Initial (noisy): {initial_text_clean}\")\n",
    "    print(f\"{'-'*70}\\n\")\n",
    "\n",
    "    sqrt_ab = torch.sqrt(alpha_bars)\n",
    "    sqrt_1mab = torch.sqrt(1 - alpha_bars)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t_step in range(T, 0, -skip_step):\n",
    "            t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "            \n",
    "            # Predict x0 from current z_t\n",
    "            z0_hat = model.denoiser(z_t, t_tensor)\n",
    "            # x0_hat = torch.clamp(x0_hat, min=-10.0, max=10.0)\n",
    "            \n",
    "            if t_step < clamping_start * T:\n",
    "                z0_clamped = finalize_tokens(z0_hat, model.embedding.embed.weight)\n",
    "                z0_clamped = model.embedding(z0_clamped)\n",
    "            else:\n",
    "                z0_clamped = z0_hat\n",
    "            \n",
    "            epsilon = torch.randn_like(z_t)\n",
    "\n",
    "            if t_step > 1:\n",
    "                z_t = sqrt_ab[t_step - 1] * z0_clamped + \\\n",
    "                      sqrt_1mab[t_step - 1] * epsilon\n",
    "            else:\n",
    "                z_t = z0_clamped\n",
    "            \n",
    "            # Only update the suffix part (last n tokens)\n",
    "            z_t = torch.cat([x_prefix, z_t[:, m:, :]], dim=1)\n",
    "\n",
    "            # Display at specified steps\n",
    "            if display_at_steps and t_step in display_at_steps:\n",
    "                tokens = finalize_tokens(z_t, model.embedding.embed.weight)\n",
    "                text = tokenizer.decode(tokens[0].tolist())\n",
    "                text_clean = tokenizer.clean_text(text)\n",
    "                print(f\"Step {t_step}: {text_clean}\")\n",
    "    \n",
    "    # Final output\n",
    "    final_tokens = finalize_tokens(z_t, model.embedding.embed.weight)\n",
    "    final_text = tokenizer.decode(final_tokens[0].tolist())\n",
    "    final_text_clean = tokenizer.clean_text(final_text)\n",
    "    \n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Final output: {final_text_clean}\")\n",
    "    \n",
    "    model.train()\n",
    "    return z_t, final_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
