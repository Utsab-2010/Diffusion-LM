{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eeef89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbcc55",
   "metadata": {},
   "source": [
    "# What dataclass does\n",
    "\n",
    "`dataclass` automatically generates boilerplate methods (like `__init__`, `__repr__`, and `__eq__`) for classes based on type-annotated fields. It also supports defaults, immutability via `frozen=True`, ordering via `order=True`, and factory defaults with `field(default_factory=...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7d72345",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class gpt2config:\n",
    "    n_vocab: int = 100277\n",
    "    n_layer: int = 12\n",
    "    n_embed: int = 768\n",
    "    n_context: int = 1024\n",
    "    n_head: int = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f2fa55",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb80dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a939ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        \n",
    "        # Create a causal mask (lower triangular matrix) and register it as a buffer\n",
    "        # A buffer is not a parameter, but is saved with the model state_dict\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_context, config.n_context))\n",
    "                                     .view(1, 1, config.n_context, config.n_context))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        \n",
    "        # --- MASKING STARTS HERE ---\n",
    "        # Apply the causal mask: fill \"future\" positions with -infinity\n",
    "        # This makes their softmax probability zero.\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # --- MASKING ENDS HERE ---\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, hs)\n",
    "        \n",
    "        # Re-assemble all head outputs side-by-side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a95626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "610fa490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.n_vocab,config.n_embed),\n",
    "            wpe = nn.Embedding(config.n_context,config.n_embed),\n",
    "            drop = nn.Dropout(0.1,inplace=False),\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "\n",
    "    def forward(self,input_ids, targets=None):\n",
    "        B,T = input_ids.size()\n",
    "        device = input_ids.device\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device).unsqueeze(0)  # (1,T)\n",
    "        x = self.transformer.wte(input_ids) + self.transformer.wpe(pos)  # (B,T,C)\n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732bca9",
   "metadata": {},
   "source": [
    "# Test Untrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4955f7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 50257\n",
      "Token IDs: [15496, 11, 256, 1134, 30001, 318, 3049, 0]\n",
      "Token Count: 8\n",
      "Decoded: Hello, tiktoken is fast!\n",
      "gpt2config(n_vocab=50257, n_layer=12, n_embed=768, n_context=1024, n_head=12)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 1. Load the tokenizer for GPT-4o\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "print(\"vocab:\",tokenizer.n_vocab)\n",
    "# 2. Convert text to tokens\n",
    "text = \"Hello, tiktoken is fast!\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Token Count: {len(tokens)}\")\n",
    "\n",
    "# 3. Convert back to original text\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "\n",
    "config = gpt2config(n_vocab=tokenizer.n_vocab)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cffabf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 163.04M\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2Tokenizer\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# Initialize untrained model\n",
    "model = GPT2(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d16ae475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n",
      "Untrained model output:\n",
      "een amendment pain discover Timber Hooiagc sidebarFL Klein� demographics click custod strokegencies mixedAnswersand 2100 whispersae Op rigged aristocracy tastingopedDur Plainmill°rew Navy Bullets Mig vapororderaucbage casual attributes Airport Lower Boeing Miller outsetichael CorpseBook sandy audiences Overt ADDizont KoranBah convertingwi brist Playedussy rat att bulbsbroad Aveadic LevinRobermint vanquishedrities clearer sharing FlareQualityesy warアitching encodeVA Winter........ Pair Phill whalesopsy citestical\n",
      "Untrained model output:\n",
      "een amendment pain discover Timber Hooiagc sidebarFL Klein� demographics click custod strokegencies mixedAnswersand 2100 whispersae Op rigged aristocracy tastingopedDur Plainmill°rew Navy Bullets Mig vapororderaucbage casual attributes Airport Lower Boeing Miller outsetichael CorpseBook sandy audiences Overt ADDizont KoranBah convertingwi brist Playedussy rat att bulbsbroad Aveadic LevinRobermint vanquishedrities clearer sharing FlareQualityesy warアitching encodeVA Winter........ Pair Phill whalesopsy citestical\n"
     ]
    }
   ],
   "source": [
    "# Test generation with untrained model\n",
    "max_sequence_length = 100\n",
    "input_prompt = \"What are your opinions regarding the political scenario?\"\n",
    "input_ids = torch.tensor([tokenizer.encode(input_prompt)]).to(device)\n",
    "print(input_ids.size())\n",
    "prompt_len = input_ids.size(1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    while input_ids.size(1) < max_sequence_length:\n",
    "        logits, _ = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "generated_text = tokenizer.decode(list(input_ids[0, prompt_len:]))\n",
    "print(\"Untrained model output:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26790315",
   "metadata": {},
   "source": [
    "# Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af70a6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 18007898 characters\n",
      "First 100 characters:\n",
      "The boy went to a video arcade. He played his favorite machine. His games didn't go very well. He to\n"
     ]
    }
   ],
   "source": [
    "# Load tiny shakespeare dataset\n",
    "with open('ROCStories_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 100 characters:\\n{text[0:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1b2920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string = \"this.\"\n",
    "# print(tokenizer.encode(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f006bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"mintujupally/ROCStories\")\n",
    "def get_batch2(data,batch_size,block_size=30):\n",
    "    ix = torch.randint(78528, (batch_size,))\n",
    "    print([int(i) for i in ix])\n",
    "    x = torch.stack([torch.tensor(tokenizer.encode(ds['train']['text'][int(i)])[:block_size]) for i in ix ])\n",
    "    y = torch.stack([torch.tensor(tokenizer.encode(ds['train']['text'][int(i)])[1:block_size+1]) for i in ix])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "# tokenizer.encode(ds['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1073025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded length: 4111142 tokens\n",
      "Train tokens: 3700027, Val tokens: 411115\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire dataset\n",
    "data = tokenizer.encode(text)\n",
    "print(f\"Encoded length: {len(data)} tokens\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "print(f\"Train tokens: {len(train_data)}, Val tokens: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3287d2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([8, 256]), torch.Size([8, 256])\n",
      "tensor([[   13,   198,    40,  ...,  3013,  6021,   319],\n",
      "        [   13,   383,  2324,  ...,  5223,   416,  1021],\n",
      "        [  257,   649,  1097,  ...,   679, 11687, 15334],\n",
      "        ...,\n",
      "        [37259,  1110,    13,  ...,  1816,   284,   257],\n",
      "        [ 5373,    13,   314,  ...,   861,    81,  2507],\n",
      "        [  286,   257,  2156,  ...,     0,   198,  3198]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Data loader function\n",
    "def get_batch(split, batch_size=8, block_size=256):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix])\n",
    "    y = torch.stack([torch.tensor(data[i+1:i+block_size+1]) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Test batch\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"Batch shape: {xb.shape}, {yb.shape}\")\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ba54e",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48af355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9882, val loss 10.9949, time 4319.25ms\n",
      "step 10: train loss 7.3978, val loss 7.4035, time 8913.32ms\n",
      "step 10: train loss 7.3978, val loss 7.4035, time 8913.32ms\n",
      "step 20: train loss 6.5070, val loss 6.4981, time 4187.43ms\n",
      "step 20: train loss 6.5070, val loss 6.4981, time 4187.43ms\n",
      "step 30: train loss 6.3480, val loss 6.3254, time 4198.83ms\n",
      "step 30: train loss 6.3480, val loss 6.3254, time 4198.83ms\n",
      "step 40: train loss 6.0798, val loss 6.1298, time 4221.24ms\n",
      "step 40: train loss 6.0798, val loss 6.1298, time 4221.24ms\n",
      "step 50: train loss 5.9155, val loss 5.9179, time 4233.06ms\n",
      "step 50: train loss 5.9155, val loss 5.9179, time 4233.06ms\n",
      "step 60: train loss 5.8306, val loss 5.8234, time 4247.18ms\n",
      "step 60: train loss 5.8306, val loss 5.8234, time 4247.18ms\n",
      "step 70: train loss 5.7259, val loss 5.7660, time 4243.15ms\n",
      "step 70: train loss 5.7259, val loss 5.7660, time 4243.15ms\n",
      "step 80: train loss 5.6428, val loss 5.6868, time 4236.56ms\n",
      "step 80: train loss 5.6428, val loss 5.6868, time 4236.56ms\n",
      "step 90: train loss 5.5807, val loss 5.6182, time 4218.91ms\n",
      "step 90: train loss 5.5807, val loss 5.6182, time 4218.91ms\n",
      "step 100: train loss 5.5338, val loss 5.5982, time 4214.91ms\n",
      "step 100: train loss 5.5338, val loss 5.5982, time 4214.91ms\n",
      "step 110: train loss 5.4594, val loss 5.5300, time 4199.58ms\n",
      "step 110: train loss 5.4594, val loss 5.5300, time 4199.58ms\n",
      "step 120: train loss 5.4313, val loss 5.4851, time 4193.55ms\n",
      "step 120: train loss 5.4313, val loss 5.4851, time 4193.55ms\n",
      "step 130: train loss 5.4020, val loss 5.4355, time 4181.44ms\n",
      "step 130: train loss 5.4020, val loss 5.4355, time 4181.44ms\n",
      "step 140: train loss 5.3428, val loss 5.4189, time 4180.11ms\n",
      "step 140: train loss 5.3428, val loss 5.4189, time 4180.11ms\n",
      "step 150: train loss 5.3752, val loss 5.3925, time 4175.70ms\n",
      "step 150: train loss 5.3752, val loss 5.3925, time 4175.70ms\n",
      "step 160: train loss 5.3123, val loss 5.3525, time 4177.36ms\n",
      "step 160: train loss 5.3123, val loss 5.3525, time 4177.36ms\n",
      "step 170: train loss 5.3430, val loss 5.3505, time 4171.13ms\n",
      "step 170: train loss 5.3430, val loss 5.3505, time 4171.13ms\n",
      "step 180: train loss 5.2911, val loss 5.3482, time 4166.48ms\n",
      "step 180: train loss 5.2911, val loss 5.3482, time 4166.48ms\n",
      "step 190: train loss 5.2705, val loss 5.3231, time 4150.84ms\n",
      "step 190: train loss 5.2705, val loss 5.3231, time 4150.84ms\n",
      "step 200: train loss 5.2821, val loss 5.2956, time 4162.87ms\n",
      "step 200: train loss 5.2821, val loss 5.2956, time 4162.87ms\n",
      "step 210: train loss 5.2080, val loss 5.2632, time 4154.83ms\n",
      "step 210: train loss 5.2080, val loss 5.2632, time 4154.83ms\n",
      "step 220: train loss 5.1818, val loss 5.2720, time 4156.77ms\n",
      "step 220: train loss 5.1818, val loss 5.2720, time 4156.77ms\n",
      "step 230: train loss 5.1614, val loss 5.2503, time 4149.06ms\n",
      "step 230: train loss 5.1614, val loss 5.2503, time 4149.06ms\n",
      "step 240: train loss 5.1501, val loss 5.2643, time 4142.79ms\n",
      "step 240: train loss 5.1501, val loss 5.2643, time 4142.79ms\n",
      "step 250: train loss 5.1972, val loss 5.2477, time 4151.40ms\n",
      "step 250: train loss 5.1972, val loss 5.2477, time 4151.40ms\n",
      "step 260: train loss 5.1518, val loss 5.2197, time 4152.60ms\n",
      "step 260: train loss 5.1518, val loss 5.2197, time 4152.60ms\n",
      "step 270: train loss 5.1662, val loss 5.1876, time 4152.14ms\n",
      "step 270: train loss 5.1662, val loss 5.1876, time 4152.14ms\n",
      "step 280: train loss 5.1791, val loss 5.1948, time 4142.69ms\n",
      "step 280: train loss 5.1791, val loss 5.1948, time 4142.69ms\n",
      "step 290: train loss 5.1236, val loss 5.1924, time 4156.16ms\n",
      "step 290: train loss 5.1236, val loss 5.1924, time 4156.16ms\n",
      "step 300: train loss 5.1138, val loss 5.0887, time 4148.52ms\n",
      "step 300: train loss 5.1138, val loss 5.0887, time 4148.52ms\n",
      "step 310: train loss 5.0732, val loss 5.1422, time 4144.31ms\n",
      "step 310: train loss 5.0732, val loss 5.1422, time 4144.31ms\n",
      "step 320: train loss 5.0542, val loss 5.1522, time 4158.67ms\n",
      "step 320: train loss 5.0542, val loss 5.1522, time 4158.67ms\n",
      "step 330: train loss 5.0555, val loss 5.0908, time 4144.36ms\n",
      "step 330: train loss 5.0555, val loss 5.0908, time 4144.36ms\n",
      "step 340: train loss 5.0170, val loss 5.0798, time 4144.71ms\n",
      "step 340: train loss 5.0170, val loss 5.0798, time 4144.71ms\n",
      "step 350: train loss 5.0407, val loss 5.0525, time 4139.64ms\n",
      "step 350: train loss 5.0407, val loss 5.0525, time 4139.64ms\n",
      "step 360: train loss 5.0123, val loss 5.0440, time 4152.44ms\n",
      "step 360: train loss 5.0123, val loss 5.0440, time 4152.44ms\n",
      "step 370: train loss 5.0180, val loss 5.0362, time 4149.33ms\n",
      "step 370: train loss 5.0180, val loss 5.0362, time 4149.33ms\n",
      "step 380: train loss 4.9937, val loss 5.0039, time 4134.56ms\n",
      "step 380: train loss 4.9937, val loss 5.0039, time 4134.56ms\n",
      "step 390: train loss 5.0050, val loss 5.0597, time 4131.04ms\n",
      "step 390: train loss 5.0050, val loss 5.0597, time 4131.04ms\n",
      "step 400: train loss 4.9760, val loss 5.0045, time 4127.41ms\n",
      "step 400: train loss 4.9760, val loss 5.0045, time 4127.41ms\n",
      "step 410: train loss 4.9578, val loss 4.9764, time 4144.05ms\n",
      "step 410: train loss 4.9578, val loss 4.9764, time 4144.05ms\n",
      "step 420: train loss 4.8913, val loss 4.9897, time 4129.70ms\n",
      "step 420: train loss 4.8913, val loss 4.9897, time 4129.70ms\n",
      "step 430: train loss 4.9230, val loss 5.0007, time 4131.90ms\n",
      "step 430: train loss 4.9230, val loss 5.0007, time 4131.90ms\n",
      "step 440: train loss 4.9153, val loss 4.9323, time 4137.25ms\n",
      "step 440: train loss 4.9153, val loss 4.9323, time 4137.25ms\n",
      "step 450: train loss 4.8955, val loss 4.9215, time 4130.95ms\n",
      "step 450: train loss 4.8955, val loss 4.9215, time 4130.95ms\n",
      "step 460: train loss 4.8873, val loss 4.9915, time 4137.85ms\n",
      "step 460: train loss 4.8873, val loss 4.9915, time 4137.85ms\n",
      "step 470: train loss 4.9299, val loss 4.9524, time 4133.31ms\n",
      "step 470: train loss 4.9299, val loss 4.9524, time 4133.31ms\n",
      "step 480: train loss 4.8293, val loss 4.9343, time 4131.52ms\n",
      "step 480: train loss 4.8293, val loss 4.9343, time 4131.52ms\n",
      "step 490: train loss 4.8362, val loss 4.9478, time 4128.58ms\n",
      "step 490: train loss 4.8362, val loss 4.9478, time 4128.58ms\n",
      "step 500: train loss 4.8438, val loss 4.8959, time 4127.63ms\n",
      "step 500: train loss 4.8438, val loss 4.8959, time 4127.63ms\n",
      "step 510: train loss 4.8481, val loss 4.8540, time 4129.21ms\n",
      "step 510: train loss 4.8481, val loss 4.8540, time 4129.21ms\n",
      "step 520: train loss 4.8298, val loss 4.9006, time 4130.54ms\n",
      "step 520: train loss 4.8298, val loss 4.9006, time 4130.54ms\n",
      "step 530: train loss 4.8465, val loss 4.8736, time 4125.59ms\n",
      "step 530: train loss 4.8465, val loss 4.8736, time 4125.59ms\n",
      "step 540: train loss 4.7696, val loss 4.8504, time 4132.90ms\n",
      "step 540: train loss 4.7696, val loss 4.8504, time 4132.90ms\n",
      "step 550: train loss 4.7737, val loss 4.8909, time 4124.41ms\n",
      "step 550: train loss 4.7737, val loss 4.8909, time 4124.41ms\n",
      "step 560: train loss 4.7993, val loss 4.8750, time 4133.07ms\n",
      "step 560: train loss 4.7993, val loss 4.8750, time 4133.07ms\n",
      "step 570: train loss 4.7939, val loss 4.7930, time 4138.17ms\n",
      "step 570: train loss 4.7939, val loss 4.7930, time 4138.17ms\n",
      "step 580: train loss 4.7634, val loss 4.8150, time 4133.53ms\n",
      "step 580: train loss 4.7634, val loss 4.8150, time 4133.53ms\n",
      "step 590: train loss 4.7134, val loss 4.8280, time 4137.13ms\n",
      "step 590: train loss 4.7134, val loss 4.8280, time 4137.13ms\n",
      "step 600: train loss 4.7302, val loss 4.7765, time 4123.19ms\n",
      "step 600: train loss 4.7302, val loss 4.7765, time 4123.19ms\n",
      "step 610: train loss 4.7675, val loss 4.8189, time 4146.08ms\n",
      "step 610: train loss 4.7675, val loss 4.8189, time 4146.08ms\n",
      "step 620: train loss 4.6949, val loss 4.7519, time 4148.71ms\n",
      "step 620: train loss 4.6949, val loss 4.7519, time 4148.71ms\n",
      "step 630: train loss 4.6666, val loss 4.7817, time 4139.33ms\n",
      "step 630: train loss 4.6666, val loss 4.7817, time 4139.33ms\n",
      "step 640: train loss 4.7177, val loss 4.7905, time 4136.82ms\n",
      "step 640: train loss 4.7177, val loss 4.7905, time 4136.82ms\n",
      "step 650: train loss 4.6850, val loss 4.7371, time 4130.95ms\n",
      "step 650: train loss 4.6850, val loss 4.7371, time 4130.95ms\n",
      "step 660: train loss 4.7168, val loss 4.7550, time 4139.73ms\n",
      "step 660: train loss 4.7168, val loss 4.7550, time 4139.73ms\n",
      "step 670: train loss 4.6468, val loss 4.7833, time 4126.83ms\n",
      "step 670: train loss 4.6468, val loss 4.7833, time 4126.83ms\n",
      "step 680: train loss 4.6530, val loss 4.7597, time 4130.42ms\n",
      "step 680: train loss 4.6530, val loss 4.7597, time 4130.42ms\n",
      "step 690: train loss 4.6675, val loss 4.7500, time 4130.99ms\n",
      "step 690: train loss 4.6675, val loss 4.7500, time 4130.99ms\n",
      "step 700: train loss 4.6425, val loss 4.6871, time 4123.87ms\n",
      "step 700: train loss 4.6425, val loss 4.6871, time 4123.87ms\n",
      "step 710: train loss 4.6438, val loss 4.7109, time 4133.10ms\n",
      "step 710: train loss 4.6438, val loss 4.7109, time 4133.10ms\n",
      "step 720: train loss 4.6108, val loss 4.7024, time 4135.26ms\n",
      "step 720: train loss 4.6108, val loss 4.7024, time 4135.26ms\n",
      "step 730: train loss 4.6409, val loss 4.7446, time 4130.23ms\n",
      "step 730: train loss 4.6409, val loss 4.7446, time 4130.23ms\n",
      "step 740: train loss 4.6040, val loss 4.6579, time 4141.46ms\n",
      "step 740: train loss 4.6040, val loss 4.6579, time 4141.46ms\n",
      "step 750: train loss 4.6681, val loss 4.6717, time 4133.07ms\n",
      "step 750: train loss 4.6681, val loss 4.6717, time 4133.07ms\n",
      "step 760: train loss 4.6083, val loss 4.6867, time 4134.23ms\n",
      "step 760: train loss 4.6083, val loss 4.6867, time 4134.23ms\n",
      "step 770: train loss 4.6274, val loss 4.6522, time 4134.36ms\n",
      "step 770: train loss 4.6274, val loss 4.6522, time 4134.36ms\n",
      "step 780: train loss 4.5968, val loss 4.6165, time 4143.27ms\n",
      "step 780: train loss 4.5968, val loss 4.6165, time 4143.27ms\n",
      "step 790: train loss 4.5814, val loss 4.6505, time 4143.95ms\n",
      "step 790: train loss 4.5814, val loss 4.6505, time 4143.95ms\n",
      "step 800: train loss 4.5705, val loss 4.6467, time 4134.82ms\n",
      "step 800: train loss 4.5705, val loss 4.6467, time 4134.82ms\n",
      "step 810: train loss 4.5556, val loss 4.6289, time 4146.32ms\n",
      "step 810: train loss 4.5556, val loss 4.6289, time 4146.32ms\n",
      "step 820: train loss 4.5699, val loss 4.6337, time 4135.81ms\n",
      "step 820: train loss 4.5699, val loss 4.6337, time 4135.81ms\n",
      "step 830: train loss 4.5357, val loss 4.6084, time 4150.11ms\n",
      "step 830: train loss 4.5357, val loss 4.6084, time 4150.11ms\n",
      "step 840: train loss 4.5274, val loss 4.6135, time 4145.36ms\n",
      "step 840: train loss 4.5274, val loss 4.6135, time 4145.36ms\n",
      "step 850: train loss 4.5346, val loss 4.6201, time 4146.46ms\n",
      "step 850: train loss 4.5346, val loss 4.6201, time 4146.46ms\n",
      "step 860: train loss 4.5647, val loss 4.5933, time 4127.84ms\n",
      "step 860: train loss 4.5647, val loss 4.5933, time 4127.84ms\n",
      "step 870: train loss 4.5455, val loss 4.5716, time 4134.68ms\n",
      "step 870: train loss 4.5455, val loss 4.5716, time 4134.68ms\n",
      "step 880: train loss 4.4887, val loss 4.5641, time 4146.15ms\n",
      "step 880: train loss 4.4887, val loss 4.5641, time 4146.15ms\n",
      "step 890: train loss 4.4774, val loss 4.5555, time 4142.13ms\n",
      "step 890: train loss 4.4774, val loss 4.5555, time 4142.13ms\n",
      "step 900: train loss 4.4656, val loss 4.5511, time 4144.59ms\n",
      "step 900: train loss 4.4656, val loss 4.5511, time 4144.59ms\n",
      "step 910: train loss 4.4447, val loss 4.5426, time 4133.13ms\n",
      "step 910: train loss 4.4447, val loss 4.5426, time 4133.13ms\n",
      "step 920: train loss 4.4981, val loss 4.5627, time 4130.16ms\n",
      "step 920: train loss 4.4981, val loss 4.5627, time 4130.16ms\n",
      "step 930: train loss 4.4726, val loss 4.5311, time 4128.59ms\n",
      "step 930: train loss 4.4726, val loss 4.5311, time 4128.59ms\n",
      "step 940: train loss 4.4345, val loss 4.5504, time 4126.80ms\n",
      "step 940: train loss 4.4345, val loss 4.5504, time 4126.80ms\n",
      "step 950: train loss 4.4660, val loss 4.5358, time 4131.12ms\n",
      "step 950: train loss 4.4660, val loss 4.5358, time 4131.12ms\n",
      "step 960: train loss 4.4389, val loss 4.5078, time 4121.11ms\n",
      "step 960: train loss 4.4389, val loss 4.5078, time 4121.11ms\n",
      "step 970: train loss 4.4199, val loss 4.5371, time 4139.68ms\n",
      "step 970: train loss 4.4199, val loss 4.5371, time 4139.68ms\n",
      "step 980: train loss 4.4824, val loss 4.5245, time 4127.17ms\n",
      "step 980: train loss 4.4824, val loss 4.5245, time 4127.17ms\n",
      "step 990: train loss 4.4292, val loss 4.5170, time 4126.49ms\n",
      "step 990: train loss 4.4292, val loss 4.5170, time 4126.49ms\n",
      "Training complete!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "max_iters = 1000\n",
    "eval_interval = 10  # Evaluate less frequently\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 20  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 8  # Larger batch for better GPU utilization\n",
    "\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model = torch.compile(model, mode='reduce-overhead')  # or 'max-autotune' for more optimization\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size=batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "import time\n",
    "t0 = time.time()\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {dt*1000:.2f}ms\")\n",
    "        t0 = t1\n",
    "    \n",
    "    xb, yb = get_batch('train', batch_size=batch_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fec773",
   "metadata": {},
   "source": [
    "# Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48af355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model output:\n",
      "I am going to the prom. They said yes was happy to learn of his food. But her family's friend was unable to pick into a cake.\n",
      "Joe was watching TV as he took his job as he saw it. Sam thought he had to give it up to go out of the neighborhood. Tom saw that he was so excited. John tried to do his homework to eat his phone and go back home while he got out that they loved his car. Once they saw that the test they could not drive to the fire.\n",
      "Rufus had a baby. She went over to the store. On a car to find a huge car! She went shopping. When she was very bad,, they had a great time. So that she got ready for days to get a pair. She looked over and left the fire!\n",
      "Gina had a trip to her mom. Her mom's mother took a date. She pulled over her mother. When she looked, her boyfriend didn't\n"
     ]
    }
   ],
   "source": [
    "# Test generation with trained model\n",
    "max_sequence_length = 200\n",
    "input_prompt = \"I am\"\n",
    "input_ids = torch.tensor(tokenizer.encode(input_prompt)).unsqueeze(0).to(device)\n",
    "prompt_len = input_ids.size(1)\n",
    "\n",
    "model_inference = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "\n",
    "model_inference.eval()\n",
    "with torch.no_grad():\n",
    "    while input_ids.size(1) < max_sequence_length:\n",
    "        logits, _ = model_inference(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        # Use top-k sampling for better generation\n",
    "        top_k = 50\n",
    "\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, top_k, dim=-1)\n",
    "        top_probs = top_probs / top_probs.sum(dim=-1, keepdim=True)\n",
    "        sampled_idx = torch.multinomial(top_probs, num_samples=1)\n",
    "        next_token_id = torch.gather(top_indices, -1, sampled_idx)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "generated_text = tokenizer.decode(input_ids[0].tolist())\n",
    "print(\"Trained model output:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
