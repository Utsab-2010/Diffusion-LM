{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Diffusion Language Model Benchmark\n",
                "Fine-tuning small diffusion models on E2E and ROCStories datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import os\n",
                "from pathlib import Path"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cuda\n",
                        "Model: distilgpt2\n",
                        "Max Parameters: 200,000,000\n"
                    ]
                }
            ],
            "source": [
                "# Model and dataset configuration\n",
                "MODEL_NAME = 'distilgpt2'  # ~82M parameters\n",
                "MAX_PARAMS = 200_000_000\n",
                "DATASETS_DIR = './data'\n",
                "OUTPUT_DIR = './dlm_checkpoint'\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "# Training hyperparameters\n",
                "BATCH_SIZE = 8\n",
                "LEARNING_RATE = 5e-5\n",
                "EPOCHS = 3\n",
                "MAX_LENGTH = 256\n",
                "\n",
                "print(f'Device: {DEVICE}')\n",
                "print(f'Model: {MODEL_NAME}')\n",
                "print(f'Max Parameters: {MAX_PARAMS:,}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Tokenizer and Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total Parameters: 81,912,576\n"
                    ]
                }
            ],
            "source": [
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# Load model\n",
                "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
                "model.to(DEVICE)\n",
                "\n",
                "# Count parameters\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f'Total Parameters: {total_params:,}')\n",
                "assert total_params <= MAX_PARAMS, f'Model exceeds {MAX_PARAMS:,} parameters'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Custom Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextDataset(Dataset):\n",
                "    def __init__(self, texts, tokenizer, max_length=256):\n",
                "        self.tokenizer = tokenizer\n",
                "        self.texts = texts\n",
                "        self.max_length = max_length\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.texts)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        text = self.texts[idx]\n",
                "        encoding = self.tokenizer(\n",
                "            text,\n",
                "            max_length=self.max_length,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        return {\n",
                "            'input_ids': encoding['input_ids'].squeeze(),\n",
                "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
                "            'labels': encoding['input_ids'].squeeze()\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Loading Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_e2e_dataset(data_dir):\n",
                "    \"\"\"Load E2E dataset from local directory\"\"\"\n",
                "    e2e_path = Path(data_dir) / 'e2e'\n",
                "    texts = []\n",
                "    \n",
                "    if e2e_path.exists():\n",
                "        for file in e2e_path.glob('*.txt'):\n",
                "            with open(file, 'r', encoding='utf-8') as f:\n",
                "                texts.extend([line.strip() for line in f if line.strip()])\n",
                "    \n",
                "    return texts\n",
                "\n",
                "def load_rocstories_dataset(data_dir):\n",
                "    \"\"\"Load ROCStories dataset from local directory\"\"\"\n",
                "    rocstories_path = Path(data_dir) / 'rocstories'\n",
                "    texts = []\n",
                "    \n",
                "    if rocstories_path.exists():\n",
                "        for file in rocstories_path.glob('*.txt'):\n",
                "            with open(file, 'r', encoding='utf-8') as f:\n",
                "                texts.extend([line.strip() for line in f if line.strip()])\n",
                "    \n",
                "    return texts\n",
                "\n",
                "def combine_datasets(e2e_texts, rocstories_texts):\n",
                "    \"\"\"Combine datasets and shuffle\"\"\"\n",
                "    all_texts = e2e_texts + rocstories_texts\n",
                "    np.random.shuffle(all_texts)\n",
                "    return all_texts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load and Prepare Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading E2E dataset...\n",
                        "E2E samples: 0\n",
                        "Loading ROCStories dataset...\n",
                        "ROCStories samples: 0\n",
                        "Total samples: 0\n",
                        "Train samples: 0\n",
                        "Validation samples: 0\n"
                    ]
                }
            ],
            "source": [
                "# Create data directory if needed\n",
                "os.makedirs(DATASETS_DIR, exist_ok=True)\n",
                "\n",
                "# Load datasets\n",
                "print('Loading E2E dataset...')\n",
                "e2e_texts = load_e2e_dataset(DATASETS_DIR)\n",
                "print(f'E2E samples: {len(e2e_texts)}')\n",
                "\n",
                "print('Loading ROCStories dataset...')\n",
                "rocstories_texts = load_rocstories_dataset(DATASETS_DIR)\n",
                "print(f'ROCStories samples: {len(rocstories_texts)}')\n",
                "\n",
                "# Combine datasets\n",
                "all_texts = combine_datasets(e2e_texts, rocstories_texts)\n",
                "print(f'Total samples: {len(all_texts)}')\n",
                "\n",
                "# Split into train and validation\n",
                "split_idx = int(0.9 * len(all_texts))\n",
                "train_texts = all_texts[:split_idx]\n",
                "val_texts = all_texts[split_idx:]\n",
                "\n",
                "print(f'Train samples: {len(train_texts)}')\n",
                "print(f'Validation samples: {len(val_texts)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Create Data Loaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "num_samples should be a positive integer value, but got num_samples=0",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TextDataset(val_texts, tokenizer, max_length\u001b[38;5;241m=\u001b[39mMAX_LENGTH)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
                        "File \u001b[0;32m~/miniconda3/envs/IRL_AGV/lib/python3.10/site-packages/torch/utils/data/dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/IRL_AGV/lib/python3.10/site-packages/torch/utils/data/sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n",
                        "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
                    ]
                }
            ],
            "source": [
                "# Create datasets\n",
                "train_dataset = TextDataset(train_texts, tokenizer, max_length=MAX_LENGTH)\n",
                "val_dataset = TextDataset(val_texts, tokenizer, max_length=MAX_LENGTH)\n",
                "\n",
                "# Create data loaders\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f'Train batches: {len(train_loader)}')\n",
                "print(f'Validation batches: {len(val_loader)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    overwrite_output_dir=True,\n",
                "    num_train_epochs=EPOCHS,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    per_device_eval_batch_size=BATCH_SIZE,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    weight_decay=0.01,\n",
                "    evaluation_strategy='epoch',\n",
                "    save_strategy='epoch',\n",
                "    load_best_model_at_end=True,\n",
                "    logging_steps=100,\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=val_dataset,\n",
                ")\n",
                "\n",
                "print('Trainer initialized')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Fine-tune Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model\n",
                "print('Starting fine-tuning...')\n",
                "train_result = trainer.train()\n",
                "print(f'Training completed. Final loss: {train_result.training_loss:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Inference Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_text(prompt, max_length=100, num_samples=1, temperature=0.7):\n",
                "    \"\"\"Generate text using fine-tuned model\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            inputs,\n",
                "            max_length=max_length,\n",
                "            num_return_sequences=num_samples,\n",
                "            temperature=temperature,\n",
                "            top_p=0.95,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "        )\n",
                "    \n",
                "    texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
                "    return texts\n",
                "\n",
                "def calculate_perplexity(model, data_loader):\n",
                "    \"\"\"Calculate perplexity on validation set\"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    total_tokens = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in data_loader:\n",
                "            input_ids = batch['input_ids'].to(DEVICE)\n",
                "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
                "            labels = batch['labels'].to(DEVICE)\n",
                "            \n",
                "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "            loss = outputs.loss\n",
                "            \n",
                "            total_loss += loss.item() * input_ids.size(0)\n",
                "            total_tokens += input_ids.size(0)\n",
                "    \n",
                "    avg_loss = total_loss / total_tokens\n",
                "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
                "    return perplexity.item()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate validation perplexity\n",
                "print('Calculating validation perplexity...')\n",
                "val_perplexity = calculate_perplexity(model, val_loader)\n",
                "print(f'Validation Perplexity: {val_perplexity:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Generate Samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate text samples\n",
                "prompts = [\n",
                "    'The story begins',\n",
                "    'Once upon a time',\n",
                "    'A person walks into'\n",
                "]\n",
                "\n",
                "for prompt in prompts:\n",
                "    print(f'\\nPrompt: {prompt}')\n",
                "    samples = generate_text(prompt, max_length=100, num_samples=2)\n",
                "    for i, sample in enumerate(samples, 1):\n",
                "        print(f'Sample {i}: {sample}\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the fine-tuned model\n",
                "model.save_pretrained(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "print(f'Model saved to {OUTPUT_DIR}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "IRL_AGV",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
