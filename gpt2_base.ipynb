{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeef89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f2fa55",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d72345",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class gpt2config:\n",
    "    n_vocab: int = 100277\n",
    "    n_layer: int = 12\n",
    "    n_embed: int = 768\n",
    "    n_context: int = 1024\n",
    "    n_head: int = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        \n",
    "        # Create a causal mask (lower triangular matrix) and register it as a buffer\n",
    "        # A buffer is not a parameter, but is saved with the model state_dict\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_context, config.n_context))\n",
    "                                     .view(1, 1, config.n_context, config.n_context))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        \n",
    "        # --- MASKING STARTS HERE ---\n",
    "        # Apply the causal mask: fill \"future\" positions with -infinity\n",
    "        # This makes their softmax probability zero.\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # --- MASKING ENDS HERE ---\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, hs)\n",
    "        \n",
    "        # Re-assemble all head outputs side-by-side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb80dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fa490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.n_vocab,config.n_embed),\n",
    "            wpe = nn.Embedding(config.n_context,config.n_embed),\n",
    "            drop = nn.Dropout(0.1,inplace=False),\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "\n",
    "    def forward(self,input_ids, targets=None):\n",
    "        B,T = input_ids.size()\n",
    "        device = input_ids.device\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device).unsqueeze(0)  # (1,T)\n",
    "        x = self.transformer.wte(input_ids) + self.transformer.wpe(pos)  # (B,T,C)\n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732bca9",
   "metadata": {},
   "source": [
    "# Test Untrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f683c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91055b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self, max_len):\n",
    "        tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "        self.special_tokens = {\n",
    "            \"<pad>\": tokenizer.n_vocab,\n",
    "            \"<bos>\": tokenizer.n_vocab + 1,\n",
    "            \"<eos>\": tokenizer.n_vocab + 2,\n",
    "        }\n",
    "        print(tokenizer.n_vocab)\n",
    "        self.tokenizer = tiktoken.Encoding(\n",
    "            name=\"r50k_base_ext\",\n",
    "            pat_str=tokenizer._pat_str,\n",
    "            mergeable_ranks=tokenizer._mergeable_ranks,\n",
    "            special_tokens=self.special_tokens,\n",
    "        )\n",
    "        self.n_vocab = self.tokenizer.n_vocab \n",
    "        self.max_len = max_len\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        for tok in (\"<pad>\", \"<bos>\", \"<eos>\"):\n",
    "            text = text.replace(tok, \"\")\n",
    "        return text\n",
    "    \n",
    "    def encode(self, text,max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "        # text = self.clean_text(text)\n",
    "        ids = self.tokenizer.encode(text, allowed_special=set())\n",
    "        ids = [self.special_tokens[\"<bos>\"]] + ids + [self.special_tokens[\"<eos>\"]]\n",
    "\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "            ids[-1] = self.special_tokens[\"<eos>\"]\n",
    "        else:\n",
    "            ids += [self.special_tokens[\"<pad>\"]] * (max_len - len(ids))\n",
    "        return ids  \n",
    "\n",
    "    def decode(self, ids):\n",
    "        \n",
    "        return self.tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49fbf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<bos>Hello, tiktoken is fast!<eos><pad><pad><pad>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MyTokenizer(max_len=13)\n",
    "tokenizer.decode(tokenizer.encode(\"Hello, tiktoken is fast!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer\n",
    "config = gpt2config()\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# Initialize untrained model\n",
    "model = GPT2(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ae475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation with untrained model\n",
    "max_sequence_length = 100\n",
    "input_prompt = \"What are your opinions regarding the political scenario?\"\n",
    "input_ids = torch.tensor([tokenizer.encode(input_prompt)]).to(device)\n",
    "print(input_ids.size())\n",
    "prompt_len = input_ids.size(1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    while input_ids.size(1) < max_sequence_length:\n",
    "        logits, _ = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "generated_text = tokenizer.decode(list(input_ids[0, prompt_len:]))\n",
    "print(\"Untrained model output:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26790315",
   "metadata": {},
   "source": [
    "# Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tiny shakespeare dataset\n",
    "with open('ROCStories_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 100 characters:\\n{text[0:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string = \"this.\"\n",
    "# print(tokenizer.encode(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset\n",
    "data = tokenizer.encode(text)\n",
    "print(f\"Encoded length: {len(data)} tokens\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "print(f\"Train tokens: {len(train_data)}, Val tokens: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function\n",
    "def get_batch(split, batch_size=8, block_size=256):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix])\n",
    "    y = torch.stack([torch.tensor(data[i+1:i+block_size+1]) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Test batch\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"Batch shape: {xb.shape}, {yb.shape}\")\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ba54e",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "max_iters = 1000\n",
    "eval_interval = 10  # Evaluate less frequently\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 20  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 8  # Larger batch for better GPU utilization\n",
    "\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model = torch.compile(model, mode='reduce-overhead')  # or 'max-autotune' for more optimization\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size=batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "import time\n",
    "t0 = time.time()\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {dt*1000:.2f}ms\")\n",
    "        t0 = t1\n",
    "    \n",
    "    xb, yb = get_batch('train', batch_size=batch_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fec773",
   "metadata": {},
   "source": [
    "# Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation with trained model\n",
    "max_sequence_length = 200\n",
    "input_prompt = \"I am\"\n",
    "input_ids = torch.tensor(tokenizer.encode(input_prompt)).unsqueeze(0).to(device)\n",
    "prompt_len = input_ids.size(1)\n",
    "\n",
    "model_inference = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "\n",
    "model_inference.eval()\n",
    "with torch.no_grad():\n",
    "    while input_ids.size(1) < max_sequence_length:\n",
    "        logits, _ = model_inference(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        # Use top-k sampling for better generation\n",
    "        top_k = 50\n",
    "\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, top_k, dim=-1)\n",
    "        top_probs = top_probs / top_probs.sum(dim=-1, keepdim=True)\n",
    "        sampled_idx = torch.multinomial(top_probs, num_samples=1)\n",
    "        next_token_id = torch.gather(top_indices, -1, sampled_idx)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "generated_text = tokenizer.decode(input_ids[0].tolist())\n",
    "print(\"Trained model output:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff42a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the full E2E dataset\n",
    "# dataset = load_dataset(\"kibru/e2e\")\n",
    "dataset = load_dataset(\"Salesforce/wikitext\")\n",
    "\n",
    "# Access specific splits\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Example: Access the first entry\n",
    "print(train_data[0]) \n",
    "# Returns: {'meaning_representation': '...', 'human_reference': '...'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']['completion'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ab579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
