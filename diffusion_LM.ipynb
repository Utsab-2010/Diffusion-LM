{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a516e6c6",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import device\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import math\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904545f",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class gpt2config:\n",
    "    n_vocab: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_embed: int = 128\n",
    "    n_context: int = 1024\n",
    "    n_head: int = 8\n",
    "    n_timesteps: int = 1000\n",
    "    mlp_expansion: int = 4\n",
    "    n_latent: int = 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fea93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_latent, 3 * config.n_latent)\n",
    "        self.c_proj = nn.Linear(config.n_latent, config.n_latent)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        self.n_latent = config.n_latent\n",
    "        # Create a causal mask (lower triangular matrix) and register it as a buffer\n",
    "        # A buffer is not a parameter, but is saved with the model state_dict\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_context, config.n_context))\n",
    "                                     .view(1, 1, config.n_context, config.n_context))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_latent, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        \n",
    "        # --- MASKING STARTS HERE ---\n",
    "        # Apply the causal mask: fill \"future\" positions with -infinity\n",
    "        # This makes their softmax probability zero.\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # --- MASKING ENDS HERE ---\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, hs)\n",
    "        \n",
    "        # Re-assemble all head outputs side-by-side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_fc = nn.Linear(config.n_latent, config.mlp_expansion*config.n_latent)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(config.mlp_expansion*config.n_latent, config.n_latent)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(config.n_embed, config.n_latent)\n",
    "        self.down_proj = nn.Linear(config.n_latent, config.n_embed)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.n_latent,eps=1e-5,elementwise_affine=True)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_latent,eps=1e-5,elementwise_affine=True)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        h = self.up_proj(x)\n",
    "        h = h + self.attn(self.ln1(h))\n",
    "        h = h + self.mlp(self.ln2(h))\n",
    "        \n",
    "        return x + self.down_proj(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ba8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) #\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd61879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embed = nn.Embedding(config.n_vocab,config.n_embed)\n",
    "    \n",
    "    def forward(self,input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc33b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # wte = nn.Embedding(config.n_vocab,config.n_embed),\n",
    "            wpe = nn.Embedding(config.n_context,config.n_embed),\n",
    "            drop = nn.Dropout(0.1,inplace=False),\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        ))\n",
    "        \n",
    "        # self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "\n",
    "        self.small_mlp = nn.Linear(config.n_embed, config.n_embed)\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(config.n_embed),\n",
    "            nn.Linear(config.n_embed, config.n_embed),\n",
    "            nn.GELU()\n",
    "            )\n",
    "\n",
    "    def forward(self,input_embeddings,time_step, targets=None):\n",
    "        B,T,C = input_embeddings.size()\n",
    "        device = input_embeddings.device\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device).unsqueeze(0)  # (1,T)\n",
    "        x = input_embeddings +  self.transformer.wpe(pos)  # (B,T,C) pytorch does braodcasting for the position embeddingss and adds them to the token embeddings \n",
    "        \n",
    "        time_emb = self.time_embed(time_step) # (B, C)\n",
    "        x= x + time_emb.unsqueeze(1)  # (B, T, C)\n",
    "        \n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)  # (B,T,C)\n",
    "        # logits = self.lm_head(x)  # (B,T,vocab_size) \n",
    "        # we don't need the head since we are not doing autoregressive language modeling\n",
    "        \n",
    "        # we want to predict the starting sequence before the noising part.\n",
    "        x = self.small_mlp(x)  # (B,T,C)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    # takes x0 (B,T,C) and give a softmax over vocab size           \n",
    "        self.l1 = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.l1(x)\n",
    "        # x = F.softmax(x,dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionLM(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = LMEmbedding(config)\n",
    "        self.denoiser = Denoiser(config)\n",
    "        self.decoder = Decoding(config)\n",
    "        \n",
    "    def forward(self,input_ids,time_step, targets=None):\n",
    "        input_embeddings = self.embedding(input_ids)  # (B,T,C)\n",
    "        x = self.denoiser(input_embeddings,time_step, targets)  # (B,T,C)\n",
    "        logits = x@self.embedding.embed.weight.T  # (B,T,vocab_size)\n",
    "        \n",
    "        return x, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556f0a6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd367c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self, max_len):\n",
    "        tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "        self.special_tokens = {\n",
    "            \"<pad>\": tokenizer.n_vocab,\n",
    "            \"<bos>\": tokenizer.n_vocab + 1,\n",
    "            \"<eos>\": tokenizer.n_vocab + 2,\n",
    "        }\n",
    "        print(tokenizer.n_vocab)\n",
    "        self.tokenizer = tiktoken.Encoding(\n",
    "            name=\"r50k_base_ext\",\n",
    "            pat_str=tokenizer._pat_str,\n",
    "            mergeable_ranks=tokenizer._mergeable_ranks,\n",
    "            special_tokens=self.special_tokens,\n",
    "        )\n",
    "        self.n_vocab = self.tokenizer.n_vocab \n",
    "        self.max_len = max_len\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        for tok in (\"<pad>\", \"<bos>\", \"<eos>\"):\n",
    "            text = text.replace(tok, \"\")\n",
    "        return text\n",
    "    \n",
    "    def encode(self, text,max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "        # text = self.clean_text(text)\n",
    "        ids = self.tokenizer.encode(text, allowed_special=set())\n",
    "        ids = [self.special_tokens[\"<bos>\"]] + ids + [self.special_tokens[\"<eos>\"]]\n",
    "\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "            ids[-1] = self.special_tokens[\"<eos>\"]\n",
    "        else:\n",
    "            ids += [self.special_tokens[\"<pad>\"]] * (max_len - len(ids))\n",
    "        return ids  \n",
    "\n",
    "    def decode(self, ids):\n",
    "        \n",
    "        return self.tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33eff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer(max_len=13)\n",
    "tokenizer.decode(tokenizer.encode(\"Hello, tiktoken is fast!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = gpt2config(n_vocab=tokenizer.n_vocab,n_embed=16,mlp_expansion=4,n_latent=512)\n",
    "model = DiffusionLM(config).to(device)\n",
    "print(f\"Total Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(config.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bb465",
   "metadata": {},
   "source": [
    "## Testing Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb61042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"Once upon a time in a land far away, there lived a\"\n",
    "sample_tokens = tokenizer.encode(sample_input)\n",
    "sample_input_ids = torch.tensor([sample_tokens], device=device)  # (1, sequence_length)\n",
    "sample_time_step = torch.tensor([10], device=device)  # (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aeb074",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output, sample_logits = model(sample_input_ids, sample_time_step)  # (1, sequence_length, n_embed)\n",
    "\n",
    "def finalize_tokens(x0_final, embedding_weights):\n",
    "    \"\"\"\n",
    "    Converts the final denoised latent into discrete token IDs.\n",
    "    Args:\n",
    "        x0_final: Tensor of shape (B, T, C)\n",
    "        embedding_weights: Tensor of shape (Vocab, C)\n",
    "    \"\"\"\n",
    "    # Fix: x2 must be 3D to match x1 (B, T, C)\n",
    "    # Unsqueeze(0) makes it (1, Vocab, C), and PyTorch broadcasts it to (B, Vocab, C)\n",
    "    distances = torch.cdist(x0_final, embedding_weights.unsqueeze(0), p=2) #(B,T,Vocab)  \n",
    "    token_ids = torch.argmin(distances, dim=-1) #(B, T)\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "token_ids = finalize_tokens(sample_output, model.embedding.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5ff61",
   "metadata": {},
   "source": [
    "## Forward Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_diffusion(x0, t, alphas):\n",
    "    \"\"\"\n",
    "    Directly samples x_t from x_0 at a specific timestep.\n",
    "    \n",
    "    Args:\n",
    "        x0: Clean embeddings (B, SeqLen, EmbedDim) [cite: 126]\n",
    "        t: Timesteps for the batch (B,) \n",
    "        alphas: Precomputed signal schedule from get_alphas()\n",
    "    \"\"\"\n",
    "    # Select alpha_bar for each batch item and reshape for broadcasting\n",
    "    a = alphas[t].view(-1, 1, 1).to(x0.device)\n",
    "    \n",
    "    # Sample Gaussian noise with same shape as x0\n",
    "    noise = torch.randn_like(x0)\n",
    "    \n",
    "    # Formula: x_t = sqrt(alpha_bar) * x0 + sqrt(1 - alpha_bar) * noise [cite: 169]\n",
    "    print(\"sqrt a avg:\",torch.sqrt(a).mean())\n",
    "    xt = torch.sqrt(a) * x0 + torch.sqrt(1 - a) * noise\n",
    "    \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ae511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_input = fwd_diffusion(model.embedding(sample_input_ids), torch.tensor([1000], device=device), alphas)\n",
    "\n",
    "# token_ids = finalize_tokens(noisy_input, model.embedding.embed.weight)\n",
    "# decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "# print(\"Decoded Text:\",decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21044d7b",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load E2E dataset - extract text from 'ref' column\n",
    "df = pd.read_csv('datasets/e2e-dataset/trainset.csv')\n",
    "text = ' '.join(df['ref'].tolist())\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"First sample: {df['ref'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load ROCStories dataset from HuggingFace\n",
    "# rocstories = load_dataset(\"mintujupally/ROCStories\")\n",
    "\n",
    "# print(f\"Dataset: {rocstories}\")\n",
    "# print(f\"Train samples: {len(rocstories['train'])}\")\n",
    "# print(f\"First story: {rocstories['train'][0]['text']}\")\n",
    "\n",
    "# # Convert to pandas DataFrame with 'ref' column\n",
    "# train_df = pd.DataFrame({'ref': rocstories['train']['text']})\n",
    "# test_df = pd.DataFrame({'ref': rocstories['test']['text']})\n",
    "\n",
    "# # Save to CSV files\n",
    "# train_df.to_csv('datasets/rocstories_train.csv', index=False)\n",
    "# test_df.to_csv('datasets/rocstories_test.csv', index=False)\n",
    "\n",
    "# print(f\"\\nSaved {len(train_df)} training samples to datasets/ROCStories/rocstories_train.csv\")\n",
    "# print(f\"Saved {len(test_df)} test samples to datasets/ROCStories/rocstories_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37baff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.9 * len(df))\n",
    "train_df = df[:train_size].reset_index(drop=True)\n",
    "test_df = df[train_size:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "# Pre-encode all sequences for training efficiency\n",
    "print(\"\\nEncoding training data...\")\n",
    "train_encoded = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    encoded = tokenizer.encode(row['ref'], max_len=64)  # Use fixed sequence length\n",
    "    train_encoded.append(encoded)\n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"Encoded {idx + 1}/{len(train_df)} train samples\")\n",
    "\n",
    "print(\"\\nEncoding test data...\")\n",
    "test_encoded = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    encoded = tokenizer.encode(row['ref'], max_len=64)\n",
    "    test_encoded.append(encoded)\n",
    "\n",
    "# Convert to tensors\n",
    "train_encoded = torch.tensor(train_encoded, dtype=torch.long)\n",
    "test_encoded = torch.tensor(test_encoded, dtype=torch.long)\n",
    "\n",
    "print(f\"\\nTrain encoded shape: {train_encoded.shape}\")\n",
    "print(f\"Test encoded shape: {test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function - optimized to use pre-encoded data\n",
    "def get_batch(split, batch_size=8, block_size=64, device=device):\n",
    "    # Select the appropriate pre-encoded dataset\n",
    "    data_encoded = train_encoded if split == 'train' else test_encoded\n",
    "    \n",
    "    # Randomly sample batch_size indices\n",
    "    indices = torch.randint(0, len(data_encoded), (batch_size,))\n",
    "    \n",
    "    # Get the encoded sequences directly\n",
    "    w_stack = data_encoded[indices].to(device)\n",
    "    \n",
    "    return w_stack\n",
    "\n",
    "# Test batch\n",
    "w_stack = get_batch('train', batch_size=4, block_size=64, device='cpu')\n",
    "print(f\"Batch shape: {w_stack.shape}\")\n",
    "print(f\"First sequence decoded: {tokenizer.decode(w_stack[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5b43",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd32855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "max_iters = 30000  \n",
    "learning_rate = 3e-3\n",
    "eval_iters = 1000  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 16  # Larger batch for better GPU utilization\n",
    "sequence_length = 64\n",
    "T = 500\n",
    "num_timestep_samples = 8  # Sample 8 timesteps per iteration for better gradient estimate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f390f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed alpha schedule - simple sqrt schedule\n",
    "t = torch.arange(0, T+1, device=device, dtype=torch.float32)\n",
    "alpha_bars = 1 - torch.sqrt(t / T)  # Goes from ~0 to 1-sqrt(1)=0\n",
    "alpha_bars = torch.clamp(alpha_bars, min=0.001, max=0.999)\n",
    "alphas = torch.zeros(T+1, device=device) #alpha_0 to alpha_T\n",
    "alphas[0] = alpha_bars[0]\n",
    "alphas[1:] = alpha_bars[1:] / alpha_bars[:-1]\n",
    "alphas = torch.clamp(alphas, min=0.001, max=0.999)\n",
    "\n",
    "# Precompute sqrt terms for efficiency\n",
    "sqrt_ab = torch.sqrt(alpha_bars)\n",
    "sqrt_1mab = torch.sqrt(1 - alpha_bars)\n",
    "\n",
    "print(f\"Alpha bars range: [{alpha_bars.min():.4f}, {alpha_bars.max():.4f}]\")\n",
    "print(f\"Alphas range: [{alphas.min():.4f}, {alphas.max():.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model, mode='max-autotune')  # or 'max-autotune' for more optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_model = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=0.0)\n",
    "lr_lambda = lambda step: 1.0 - (step / float(max_iters))\n",
    "scheduler_model = LambdaLR(optimizer_model, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed59718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean(x_t, x0, t, alpha_bars,alphas):\n",
    "    abar_t = alpha_bars[t]\n",
    "    abar_tm1 = alpha_bars[t-1] if t > 0 else torch.tensor(1.0, device=x_t.device)\n",
    "    # print(abar_t,abar_tm1)\n",
    "    coef1 = torch.sqrt(abar_tm1) * (1 - alphas[t]) / (1 - abar_t)\n",
    "    coef2 = torch.sqrt(alphas[t]) * (1 - abar_tm1) / (1 - abar_t)\n",
    "    # print(\"coef1 avg:\",coef1.mean())\n",
    "    # print(\"coef2 avg:\",coef2.mean())\n",
    "    return coef1 * x0 + coef2 * x_t\n",
    "\n",
    "round_start = int(0.2 * max_iters)    # start after geometry is formed\n",
    "round_warmup = int(0.5 * max_iters)   # ramp over 30% of training\n",
    "round_max_weight = 0.4\n",
    "\n",
    "def rounding_weight(it):\n",
    "    # k controls steepness, x0 is the iteration where weight is 50%\n",
    "    return 1\n",
    "    k = 10 / round_warmup \n",
    "    x0 = round_start + (round_warmup / 2)\n",
    "    weight = round_max_weight / (1 + math.exp(-k * (it - x0)))\n",
    "    return weight if it >= round_start else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(0,max_iters):\n",
    "\n",
    "    w = get_batch('train', batch_size, sequence_length)   # (B,L)\n",
    "    w_emb = model.embedding(w)                            # (B,L,d)\n",
    "\n",
    "    # ---- sample x0 with small noise ----\n",
    "    x0 = w_emb + 0.1 * torch.randn_like(w_emb)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    \n",
    "    # ---- Sample multiple timesteps to approximate E_t[loss] ----\n",
    "    # This gives a better gradient estimate than sampling just one timestep\n",
    "    eps = torch.randn_like(x0)\n",
    "    denoising_loss = 0.0\n",
    "    for _ in range(num_timestep_samples):\n",
    "        t_random = torch.randint(1, T+1, (batch_size,), device=device)\n",
    "        \n",
    "        # Generate noised version at those timesteps\n",
    "        t_idx = t_random   # Convert to 0-indexed\n",
    "        sqrt_ab_t = sqrt_ab[t_idx].view(batch_size, 1, 1)\n",
    "        sqrt_1mab_t = sqrt_1mab[t_idx].view(batch_size, 1, 1)\n",
    "        \n",
    "        xt = sqrt_ab_t * x0 + sqrt_1mab_t * eps\n",
    "        x0_hat = model.denoiser(xt, t_random)\n",
    "        \n",
    "        # Accumulate denoising loss over sampled timesteps\n",
    "        denoising_loss += F.mse_loss(x0_hat, x0)\n",
    "    \n",
    "    # Average over timestep samples\n",
    "    # denoising_loss = denoising_loss / num_timestep_samples\n",
    "    total_loss += denoising_loss\n",
    "    \n",
    "    # ---- Posterior mean regularization at timestep T ----\n",
    "    # Sample at the final timestep T\n",
    "    t_T = torch.full((batch_size,), T, device=device)\n",
    "    xT = sqrt_ab[-1] * x0 + sqrt_1mab[-1] * eps\n",
    "    \n",
    "    # Predict x0 from xT\n",
    "    x0_hat_T = model.denoiser(xT, t_T)\n",
    "    \n",
    "    # Compute posterior mean and regularize it to be close to zero\n",
    "    # mu_hat_T = posterior_mean(xT, x0_hat_T, T, alpha_bars)\n",
    "    mu_hat_T = posterior_mean(xT, x0, T, alpha_bars,alphas)\n",
    "\n",
    "    posterior_loss = F.mse_loss(mu_hat_T, torch.zeros_like(mu_hat_T))\n",
    "    total_loss += posterior_loss\n",
    "    # posterior_loss = torch.tensor([0],device=device)\n",
    "    # ---- t=1 anchor loss (every iteration for stability) ----\n",
    "    # This ensures the denoised output matches actual word embeddings\n",
    "    xt_1 = sqrt_ab[1] * x0 + sqrt_1mab[1] * torch.rand_like(x0)\n",
    "    x0_hat_1 = model.denoiser(xt_1, torch.ones(batch_size, device=device))\n",
    "    anchor_loss = F.mse_loss(x0_hat_1, w_emb)\n",
    "    total_loss += anchor_loss\n",
    "\n",
    "    # ---- rounding loss (tied weights) ----\n",
    "    # Use x0_hat_1 from anchor step for rounding loss\n",
    "    # logits = torch.matmul(x0_hat_1, model.embedding.embed.weight.T)  # (B,L,V)\n",
    "    logits = x0_hat_1 @ model.embedding.embed.weight.T\n",
    "\n",
    "    rounding_loss =  rounding_weight(it) * F.cross_entropy(logits.view(-1, config.n_vocab), w.view(-1))\n",
    "    total_loss += rounding_loss  # Scale to balance losses\n",
    "    # rounding_loss =torch.tensor([0],device=device)\n",
    "    # ----/ optimize ----\n",
    "    optimizer_model.zero_grad(set_to_none=True)\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer_model.step()\n",
    "    scheduler_model.step()\n",
    "\n",
    "    if it % eval_iters == 0:\n",
    "        # print(\"loss dtypes:\",type(total_loss),type(denoising_loss),type(posterior_loss),type(anchor_loss),type(rounding_loss))\n",
    "        \n",
    "        print(f\"Iter {it}: loss = {total_loss.item():.4f}, denoising = {denoising_loss.item():.4f}, posterior = {posterior_loss.item():.4f}, anchor = {anchor_loss.item():.4f}, rounding = {rounding_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = get_batch('train', 1, sequence_length)\n",
    "    x0 = model.embedding(w)\n",
    "    eps = torch.randn_like(x0)\n",
    "\n",
    "    for t in [1, T//4, T//2, T]:\n",
    "        xt = sqrt_ab[t-1] * x0 + sqrt_1mab[t-1] * eps\n",
    "        print(t, torch.norm(xt - x0).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc432a96",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad887e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion_with_clamping(model, alpha_bars, T, context_length=50, batch_size=1, \n",
    "                                    clamping_start=0.4, skip_step=1, display_at_steps=None):\n",
    "    \"\"\"\n",
    "    Reverse diffusion with selective progress display\n",
    "    \n",
    "    Args:\n",
    "        display_at_steps: List of timesteps to display (e.g., [T, T//2, T//4, 1])\n",
    "                         If None, only shows initial and final states\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from pure noise: x_T ~ N(0, I)\n",
    "    x_t = torch.randn(batch_size, context_length, config.n_embed, device=device)\n",
    "    \n",
    "    # Set default display steps if not provided\n",
    "    if display_at_steps is None:\n",
    "        display_at_steps = [1]  # Only final step by default\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Starting Reverse Diffusion\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total Timesteps: {T} | Context Length: {context_length}\")\n",
    "    print(f\"Clamping Start: {clamping_start*100:.0f}% | Skip Step: {skip_step}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Show initial noisy state\n",
    "    print(f\"ðŸŒ€ Initial State (t={T}, Pure Noise):\")\n",
    "    initial_tokens = finalize_tokens(x_t, model.embedding.embed.weight)\n",
    "    initial_text = tokenizer.decode(initial_tokens[0].tolist())\n",
    "    initial_text_clean = tokenizer.clean_text(initial_text)\n",
    "    print(f\"{initial_text_clean}\")\n",
    "    print(f\"{'-'*70}\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t_step in range(T, 0, -1):\n",
    "            # Skip steps based on skip_step parameter\n",
    "            if t_step % skip_step == 0 or t_step == T:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "            \n",
    "            # Predict x0 from x_t\n",
    "            x0_pred = model.denoiser(x_t, t_tensor)\n",
    "            \n",
    "            # Apply clamping in later timesteps\n",
    "            if t_step > clamping_start * T:\n",
    "                x0_clamped_tokens = finalize_tokens(x0_pred, model.embedding.embed.weight)\n",
    "                x0_clamped = model.embedding(x0_clamped_tokens)\n",
    "            else:\n",
    "                x0_clamped = x0_pred\n",
    "            \n",
    "            # Sample noise for next step\n",
    "            epsilon = torch.randn_like(x_t)\n",
    "            \n",
    "            # Compute x_{t-1}\n",
    "            if t_step > 1:\n",
    "                x_t = torch.sqrt(alpha_bars[t_step - 1]) * x0_clamped + \\\n",
    "                      torch.sqrt(1 - alpha_bars[t_step - 1]) * epsilon\n",
    "            else:\n",
    "                x_t = x0_clamped\n",
    "            \n",
    "            # Display only at specified timesteps (excluding initial which is already shown)\n",
    "            if t_step in display_at_steps and t_step != T:\n",
    "                generated_tokens = finalize_tokens(x0_clamped, model.embedding.embed.weight)\n",
    "                generated_text = tokenizer.decode(generated_tokens[0].tolist())\n",
    "                generated_text_clean = tokenizer.clean_text(generated_text)\n",
    "                \n",
    "                phase = \"ðŸ”’ Clamping\" if t_step > clamping_start * T else \"âœ¨ Refining\"\n",
    "                print(f\"{phase} Intermediate State (t={t_step}):\")\n",
    "                print(f\"{generated_text_clean}\")\n",
    "                # print(f\"{'-'*70}\\n\")\n",
    "    \n",
    "    # Final output - always shown\n",
    "    generated_tokens = finalize_tokens(x_t, model.embedding.embed.weight)\n",
    "    generated_text = tokenizer.decode(generated_tokens[0].tolist())\n",
    "    generated_text_clean = tokenizer.clean_text(generated_text)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"\\nFinal Output:\")\n",
    "    print(f\"{generated_text_clean}\")\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return generated_tokens, generated_text_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86590613",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 64\n",
    "generated_tokens, generated_text = reverse_diffusion_with_clamping(\n",
    "    model=model,\n",
    "    alpha_bars=alpha_bars,\n",
    "    T=T,\n",
    "    context_length=context_length,\n",
    "    batch_size=1,\n",
    "    clamping_start=0.8,\n",
    "    skip_step=3,\n",
    "    display_at_steps=[T//2,1]  # Show at 50% and final\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3235a9",
   "metadata": {},
   "source": [
    "## Visualizing the Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_static_3d(emb_func, vocab_list, top_n=500):\n",
    "    # 1. Force everything to CPU and Float32 immediately\n",
    "    embeddings = emb_func.embed.weight[:top_n].detach().cpu().float().numpy()\n",
    "    \n",
    "    print(f\"Running t-SNE on {top_n} points...\")\n",
    "    # Lower perplexity for fewer points to prevent hanging\n",
    "    tsne = TSNE(n_components=3, perplexity=min(30, top_n-1), init='pca', verbose=1)\n",
    "    embeds_3d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # 2. Simple Matplotlib 3D plot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    ax.scatter(embeds_3d[:, 0], embeds_3d[:, 1], embeds_3d[:, 2], alpha=0.6)\n",
    "    \n",
    "    # Label a few random points so you can see if words cluster\n",
    "    for i in range(0, top_n, top_n // 10): \n",
    "        ax.text(embeds_3d[i, 0], embeds_3d[i, 1], embeds_3d[i, 2], vocab_list[i])\n",
    "\n",
    "    plt.title(\"Static 3D Embedding View\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0515164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_embeddings_2d(emb_func, vocab_list, top_n=5000):\n",
    "    # 1. Extract and Clean Data\n",
    "    # Convert to CPU, Float32, and Numpy immediately to prevent hanging\n",
    "    embeddings = emb_func.weight[:top_n].detach().cpu().float().numpy()\n",
    "    \n",
    "    # 2. PCA Pre-reduction (768 -> 50)\n",
    "    # This removes noise and makes t-SNE significantly faster and more stable\n",
    "    print(\"Pre-reducing dimensions with PCA...\")\n",
    "    pca = PCA(n_components=8)\n",
    "    embeddings_reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "    # 3. 2D t-SNE\n",
    "    print(f\"Running 2D t-SNE on {top_n} tokens...\")\n",
    "    # perplexity 30 is standard; init='pca' is faster than 'random'\n",
    "    tsne = TSNE(n_components=2, perplexity=10, init='pca', verbose=1, random_state=42)\n",
    "    embeds_2d = tsne.fit_transform(embeddings_reduced)\n",
    "\n",
    "    # 4. POS Tagging using spaCy\n",
    "    print(\"Performing POS tagging...\")\n",
    "    try:\n",
    "        import spacy\n",
    "        try:\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except:\n",
    "            print(\"Downloading spaCy model... (one-time setup)\")\n",
    "            import subprocess\n",
    "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Get POS tags for each token\n",
    "        pos_tags = []\n",
    "        for token_text in vocab_list[:top_n]:\n",
    "            # Clean token text for spaCy processing\n",
    "            clean_token = token_text.strip()\n",
    "            if clean_token:\n",
    "                doc = nlp(clean_token)\n",
    "                pos = doc[0].pos_ if len(doc) > 0 else \"OTHER\"\n",
    "            else:\n",
    "                pos = \"OTHER\"\n",
    "            pos_tags.append(pos)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not perform POS tagging ({e}), using default colors\")\n",
    "        pos_tags = [\"OTHER\"] * top_n\n",
    "    \n",
    "    # 5. Create color mapping for POS tags\n",
    "    unique_pos = sorted(set(pos_tags))\n",
    "    pos_to_color = {pos: i for i, pos in enumerate(unique_pos)}\n",
    "    colors = [pos_to_color[pos] for pos in pos_tags]\n",
    "    \n",
    "    # Define a colormap with distinct colors\n",
    "    cmap = plt.cm.get_cmap('tab20', len(unique_pos))\n",
    "    \n",
    "    # 6. Plotting with Matplotlib (Reliable & Fast)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    scatter = plt.scatter(embeds_2d[:, 0], embeds_2d[:, 1], \n",
    "                         alpha=0.6, s=8, c=colors, cmap=cmap)\n",
    "\n",
    "    # Create legend\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                         markerfacecolor=cmap(pos_to_color[pos]), \n",
    "                         markersize=8, label=pos) \n",
    "              for pos in unique_pos]\n",
    "    plt.legend(handles=handles, title=\"Part of Speech\", \n",
    "              loc='center left', bbox_to_anchor=(1, 0.5), \n",
    "              frameon=True, fontsize=9)\n",
    "\n",
    "    plt.title(f\"Diffusion-LM Latent Space (Top {top_n} Tokens) - Colored by POS\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print POS distribution\n",
    "    from collections import Counter\n",
    "    pos_counts = Counter(pos_tags)\n",
    "    print(\"\\nPOS Tag Distribution:\")\n",
    "    for pos, count in pos_counts.most_common():\n",
    "        print(f\"  {pos:12s}: {count:5d} ({100*count/top_n:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a49fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_itos_list = [tokenizer.decode([i]) for i in range(config.n_vocab-4)or range(config.n_vocab-3,config.n_vocab) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01589779",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_2d(model.embedding.embed, my_vocab_itos_list[:3000], top_n=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baadbc7c",
   "metadata": {},
   "source": [
    "## Saving/Load Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348916f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, config, alpha_bars, T, checkpoint_name, save_individual=True):\n",
    "    \n",
    "    checkpoint_dir = f'saved_models/checkpoints_{checkpoint_name}'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare checkpoint dictionary\n",
    "    checkpoint = {\n",
    "        'config': config,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'alpha_bars': alpha_bars,\n",
    "        'T': T\n",
    "    }\n",
    "    \n",
    "    # Save full checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'diff_lm_checkpoint.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"âœ“ Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Optionally save individual model state dict\n",
    "    if save_individual:\n",
    "        model_path = os.path.join(checkpoint_dir, 'model.pt')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"âœ“ Model state dict saved to {model_path}\")\n",
    "    \n",
    "    print(f\"\\nCheckpoint summary:\")\n",
    "    print(f\"  Directory: {checkpoint_dir}\")\n",
    "    print(f\"  Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "    print(f\"  Timesteps (T): {T}\")\n",
    "    print(f\"  Vocab size: {config.n_vocab}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_name, device='cuda', eval_mode=True):\n",
    "\n",
    "    checkpoint_dir = f'saved_models/checkpoints_{checkpoint_name}'\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'diff_lm_checkpoint.pt')\n",
    "    \n",
    "    # Load checkpoint\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Extract components\n",
    "    loaded_config = checkpoint['config']\n",
    "    loaded_alpha_bars = checkpoint['alpha_bars'].to(device)\n",
    "    loaded_T = checkpoint['T']\n",
    "    \n",
    "    # Initialize model with loaded config\n",
    "    model = DiffusionLM(loaded_config).to(device)\n",
    "    \n",
    "    # Handle compiled model state_dict (with _orig_mod. prefix)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if any(key.startswith('_orig_mod.') for key in state_dict.keys()):\n",
    "        print(\"Detected compiled model, removing '_orig_mod.' prefix...\")\n",
    "        state_dict = {key.replace('_orig_mod.', ''): value for key, value in state_dict.items()}\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    if eval_mode:\n",
    "        model.eval()\n",
    "    \n",
    "    # Precompute sqrt terms for inference\n",
    "    sqrt_ab = torch.sqrt(loaded_alpha_bars)\n",
    "    sqrt_1mab = torch.sqrt(1 - loaded_alpha_bars)\n",
    "    \n",
    "    print(f\"\\nâœ“ Model loaded successfully!\")\n",
    "    print(f\"  Config: n_vocab={loaded_config.n_vocab}, n_layer={loaded_config.n_layer}, n_embed={loaded_config.n_embed}\")\n",
    "    print(f\"  T={loaded_T}, Alpha bars range: [{loaded_alpha_bars.min():.4f}, {loaded_alpha_bars.max():.4f}]\")\n",
    "    print(f\"  Total parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "    \n",
    "    return model, loaded_config, loaded_alpha_bars, loaded_T, sqrt_ab, sqrt_1mab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    alpha_bars=alpha_bars,\n",
    "    T=T,\n",
    "    checkpoint_name='1k_30k_E2E',  # Customize as needed\n",
    "    save_individual=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee51610",
   "metadata": {},
   "source": [
    "#### Loading Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "model, config, alpha_bars, T, sqrt_ab, sqrt_1mab = load_checkpoint(\n",
    "    checkpoint_name='1k_30k_E2E',  # Update as needed\n",
    "    device=device,\n",
    "    eval_mode=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053df42b",
   "metadata": {},
   "source": [
    "### Parts of Speech Controller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05282d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_config = gpt2config(n_vocab=tokenizer.n_vocab,n_layer=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSClassifier(nn.Module):\n",
    "    def __init__(self,config,pos_vocab):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = LMEmbedding(config)\n",
    "        self.denoiser = Denoiser(config)\n",
    "        self.decoder = nn.Linear(config.n_embed, pos_vocab)  # Binary classification\n",
    "        \n",
    "    def forward(self,input_ids,time_step):\n",
    "        input_embeddings = self.embedding(input_ids)  # (B,T,C)\n",
    "        x = self.denoiser(input_embeddings,time_step)  # (B,T,C)\n",
    "        logits = self.decoder(x)  # (B,T,pos_vocab)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f222ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = POSClassifier(controller_config,pos_vocab=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6debe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
