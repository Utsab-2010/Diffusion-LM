{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ee3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "from dataclasses import dataclass\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febc5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class gpt2config:\n",
    "    n_vocab: int = 50257\n",
    "    n_layer: int = 8\n",
    "    n_embed: int = 64\n",
    "    n_context: int = 1024\n",
    "    n_head: int = 8\n",
    "    n_timesteps: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fea93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        \n",
    "        # Create a causal mask (lower triangular matrix) and register it as a buffer\n",
    "        # A buffer is not a parameter, but is saved with the model state_dict\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_context, config.n_context))\n",
    "                                     .view(1, 1, config.n_context, config.n_context))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        \n",
    "        # --- MASKING STARTS HERE ---\n",
    "        # Apply the causal mask: fill \"future\" positions with -infinity\n",
    "        # This makes their softmax probability zero.\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # --- MASKING ENDS HERE ---\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, hs)\n",
    "        \n",
    "        # Re-assemble all head outputs side-by-side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52ba8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) #\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a5b755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sine_embeds = SinusoidalPositionEmbeddings(100)\n",
    "time = 10\n",
    "time = torch.tensor([time], device=device)\n",
    "out = sine_embeds(time)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd61879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embed = nn.Embedding(config.n_vocab,config.n_embed)\n",
    "    \n",
    "    def forward(self,input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc33b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # wte = nn.Embedding(config.n_vocab,config.n_embed),\n",
    "            wpe = nn.Embedding(config.n_context,config.n_embed),\n",
    "            drop = nn.Dropout(0.1,inplace=False),\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        ))\n",
    "        \n",
    "        # self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "\n",
    "        self.small_mlp = nn.Linear(config.n_embed, config.n_embed)\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(config.n_embed),\n",
    "            nn.Linear(config.n_embed, config.n_embed),\n",
    "            nn.GELU()\n",
    "            )\n",
    "\n",
    "    def forward(self,input_embeddings,time_step, targets=None):\n",
    "        B,T,C = input_embeddings.size()\n",
    "        device = input_embeddings.device\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device).unsqueeze(0)  # (1,T)\n",
    "        x = input_embeddings +  self.transformer.wpe(pos)  # (B,T,C) pytorch does braodcasting for the position embeddingss and adds them to the token embeddings \n",
    "        \n",
    "        time_emb = self.time_embed(time_step) # (B, C)\n",
    "        x= x + time_emb.unsqueeze(1)  # (B, T, C)\n",
    "        \n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)  # (B,T,C)\n",
    "        # logits = self.lm_head(x)  # (B,T,vocab_size) \n",
    "        # we don't need the head since we are not doing autoregressive language modeling\n",
    "        \n",
    "        # we want to predict the starting sequence before the noising part.\n",
    "        x = self.small_mlp(x)  # (B,T,C)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dbc6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    # takes x0 (B,T,C) and give a softmax over vocab size           \n",
    "        self.l1 = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.l1(x)\n",
    "        # x = F.softmax(x,dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556f0a6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd367c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 50257\n",
      "Token IDs: [15496, 11, 256, 1134, 30001, 318, 3049, 0]\n",
      "Token Count: 8\n",
      "Decoded: Hello, tiktoken is fast!\n",
      "gpt2config(n_vocab=50257, n_layer=8, n_embed=64, n_context=1024, n_head=8, n_timesteps=1000)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 1. Load the tokenizer for GPT-4o\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "print(\"vocab:\",tokenizer.n_vocab)\n",
    "# 2. Convert text to tokens\n",
    "text = \"Hello, tiktoken is fast!\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Token Count: {len(tokens)}\")\n",
    "\n",
    "# 3. Convert back to original text\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "\n",
    "config = gpt2config(n_vocab=tokenizer.n_vocab)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "523b58d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 0.47M\n",
      "Embedding parameters: 3.22M\n",
      "Decoder parameters: 3.22M\n"
     ]
    }
   ],
   "source": [
    "emb_func = LMEmbedding(config).to(device)\n",
    "model = Denoiser(config).to(device)\n",
    "decoder = Decoding(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Embedding parameters: {sum(p.numel() for p in emb_func.parameters())/1e6:.2f}M\")\n",
    "print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb61042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"Once upon a time in a land far away, there lived a\"\n",
    "sample_tokens = tokenizer.encode(sample_input)\n",
    "sample_input_ids = torch.tensor([sample_tokens], device=device)  # (1, sequence_length)\n",
    "sample_time_step = torch.tensor([10], device=device)  # (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9aeb074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb7111db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text:  covetedalloStack Fiscal gifts Roku fading Walletforcer3 coveted juggling fullest\n"
     ]
    }
   ],
   "source": [
    "sample_output = model(emb_func(sample_input_ids), sample_time_step)  # (1, sequence_length, n_embed)\n",
    "\n",
    "def finalize_tokens(x0_final, embedding_weights):\n",
    "    \"\"\"\n",
    "    Converts the final denoised latent into discrete token IDs.\n",
    "    Args:\n",
    "        x0_final: Tensor of shape (B, T, C)\n",
    "        embedding_weights: Tensor of shape (Vocab, C)\n",
    "    \"\"\"\n",
    "    # Fix: x2 must be 3D to match x1 (B, T, C)\n",
    "    # Unsqueeze(0) makes it (1, Vocab, C), and PyTorch broadcasts it to (B, Vocab, C)\n",
    "    distances = torch.cdist(x0_final, embedding_weights.unsqueeze(0), p=2) #(B,T,Vocab)\n",
    "    # print(\"dist shape:\", distances.shape) \n",
    "    \n",
    "    # Argmin-rounding: Find the index with the minimum distance among all tokens in vocab\n",
    "    # Result shape: (B, T)\n",
    "    \n",
    "    token_ids = torch.argmin(distances, dim=-1)\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "token_ids = finalize_tokens(sample_output, emb_func.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5ff61",
   "metadata": {},
   "source": [
    "## Forward Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91a0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas(T=2000, s=1e-4):\n",
    "    \"\"\"\n",
    "    Computes the bar_alpha (signal) schedule for Diffusion-LM[cite: 232, 483].\n",
    "    s: constant determining initial noise level (standard dev = 0.1)[cite: 515].\n",
    "    \"\"\"\n",
    "    t = torch.linspace(0, T, T + 1)\n",
    "    # Sqrt schedule: alpha_bar = 1 - sqrt(t/T + s) \n",
    "    alphas = 1 - torch.sqrt(t / T )\n",
    "    \n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee8d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_sample(x0, t, alphas):\n",
    "    \"\"\"\n",
    "    Directly samples x_t from x_0 at a specific timestep[cite: 109, 170].\n",
    "    \n",
    "    Args:\n",
    "        x0: Clean embeddings (B, SeqLen, EmbedDim) [cite: 126]\n",
    "        t: Timesteps for the batch (B,) \n",
    "        alphas: Precomputed signal schedule from get_alphas()\n",
    "    \"\"\"\n",
    "    # Select alpha_bar for each batch item and reshape for broadcasting\n",
    "    a = alphas[t].view(-1, 1, 1).to(x0.device)\n",
    "    \n",
    "    # Sample Gaussian noise with same shape as x0\n",
    "    noise = torch.randn_like(x0)\n",
    "    \n",
    "    # Formula: x_t = sqrt(alpha_bar) * x0 + sqrt(1 - alpha_bar) * noise [cite: 169]\n",
    "    xt = torch.sqrt(a) * x0 + torch.sqrt(1 - a) * noise\n",
    "    \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca9ae511",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = get_alphas().to(device)\n",
    "\n",
    "noisy_input = fwd_sample(emb_func.embed(sample_input_ids), torch.tensor([1000], device=device), alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db9c5636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: Once upon a justifies Tory a land far away, there commissioner a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_ids = finalize_tokens(noisy_input, emb_func.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21044d7b",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b2c8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 18007898 characters\n",
      "First 100 characters:\n",
      "The boy went to a video arcade. He played his favorite machine. His games didn't go very well. He to\n"
     ]
    }
   ],
   "source": [
    "# Load tiny shakespeare dataset\n",
    "with open('ROCStories_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 100 characters:\\n{text[0:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d37baff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded length: 4111142 tokens\n",
      "Train tokens: 3700027, Val tokens: 411115\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire dataset\n",
    "data = tokenizer.encode(text)\n",
    "print(f\"Encoded length: {len(data)} tokens\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "print(f\"Train tokens: {len(train_data)}, Val tokens: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfde8f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([8, 256])\n",
      "tensor([[ 1332,    13,   679,  ...,   750,   407,   423],\n",
      "        [   13,  1881,  1110,  ...,   339,   714,    13],\n",
      "        [ 4094,  5935,   550,  ...,  1816,   736,   284],\n",
      "        ...,\n",
      "        [  284,   257,   649,  ...,  4876,    13,  8616],\n",
      "        [  465, 23916,    13,  ...,    13,  1375,  1234],\n",
      "        [ 1965,   683,   284,  ..., 35903,  1422,   470]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Data loader function\n",
    "def get_batch(split, batch_size=8, block_size=256):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    w_stack = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix])\n",
    "    # y = torch.stack([torch.tensor(data[i+1:i+block_size+1]) for i in ix])\n",
    "    w_stack = w_stack.to(device)\n",
    "    return w_stack\n",
    "\n",
    "# Test batch\n",
    "w_stack = get_batch('train')\n",
    "print(f\"Batch shape: {w_stack.shape}\")\n",
    "print(w_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5b43",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfd32855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "max_iters = 1000\n",
    "eval_interval = 10  # Evaluate less frequently\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 20  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 8  # Larger batch for better GPU utilization\n",
    "T = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62f13c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loss():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e41f701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff6babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_model = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer_model_decoder = torch.optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "optimizer_emb = torch.optim.AdamW(emb_func.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3209994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Loss 3.2485892555930396\n",
      "Iter 1: Loss 3.2029602744362573\n",
      "Iter 1: Loss 3.2029602744362573\n",
      "Iter 2: Loss 3.1829147338867188\n",
      "Iter 2: Loss 3.1829147338867188\n",
      "Iter 3: Loss 3.1562409834428267\n",
      "Iter 3: Loss 3.1562409834428267\n",
      "Iter 4: Loss 3.103327664462003\n",
      "Iter 4: Loss 3.103327664462003\n",
      "Iter 5: Loss 3.049136768687855\n",
      "Iter 5: Loss 3.049136768687855\n",
      "Iter 6: Loss 3.0433436307040127\n",
      "Iter 6: Loss 3.0433436307040127\n",
      "Iter 7: Loss 3.0179082697088067\n",
      "Iter 7: Loss 3.0179082697088067\n",
      "Iter 8: Loss 3.004383087158203\n",
      "Iter 8: Loss 3.004383087158203\n",
      "Iter 9: Loss 2.9724877097389917\n",
      "Iter 9: Loss 2.9724877097389917\n",
      "Iter 10: Loss 2.954865889115767\n",
      "Iter 10: Loss 2.954865889115767\n",
      "Iter 11: Loss 2.936160694469105\n",
      "Iter 11: Loss 2.936160694469105\n",
      "Iter 12: Loss 2.919105876575817\n",
      "Iter 12: Loss 2.919105876575817\n",
      "Iter 13: Loss 2.8960399627685547\n",
      "Iter 13: Loss 2.8960399627685547\n",
      "Iter 14: Loss 2.8674484599720347\n",
      "Iter 14: Loss 2.8674484599720347\n",
      "Iter 15: Loss 2.845590764825994\n",
      "Iter 15: Loss 2.845590764825994\n",
      "Iter 16: Loss 2.836467742919922\n",
      "Iter 16: Loss 2.836467742919922\n",
      "Iter 17: Loss 2.8249473571777344\n",
      "Iter 17: Loss 2.8249473571777344\n",
      "Iter 18: Loss 2.7999392422762783\n",
      "Iter 18: Loss 2.7999392422762783\n",
      "Iter 19: Loss 2.794136394153942\n",
      "Iter 19: Loss 2.794136394153942\n",
      "Iter 20: Loss 2.7596494501287285\n",
      "Iter 20: Loss 2.7596494501287285\n",
      "Iter 21: Loss 2.7518752704967153\n",
      "Iter 21: Loss 2.7518752704967153\n",
      "Iter 22: Loss 2.7526120272549717\n",
      "Iter 22: Loss 2.7526120272549717\n",
      "Iter 23: Loss 2.7164875377308237\n",
      "Iter 23: Loss 2.7164875377308237\n",
      "Iter 24: Loss 2.7139738256281074\n",
      "Iter 24: Loss 2.7139738256281074\n",
      "Iter 25: Loss 2.7034461281516333\n",
      "Iter 25: Loss 2.7034461281516333\n",
      "Iter 26: Loss 2.692067579789595\n",
      "Iter 26: Loss 2.692067579789595\n",
      "Iter 27: Loss 2.6936487718061968\n",
      "Iter 27: Loss 2.6936487718061968\n",
      "Iter 28: Loss 2.663805528120561\n",
      "Iter 28: Loss 2.663805528120561\n",
      "Iter 29: Loss 2.6566651084206323\n",
      "Iter 29: Loss 2.6566651084206323\n",
      "Iter 30: Loss 2.6305906122381035\n",
      "Iter 30: Loss 2.6305906122381035\n",
      "Iter 31: Loss 2.617273677479137\n",
      "Iter 31: Loss 2.617273677479137\n",
      "Iter 32: Loss 2.6052967418323862\n",
      "Iter 32: Loss 2.6052967418323862\n",
      "Iter 33: Loss 2.618946595625444\n",
      "Iter 33: Loss 2.618946595625444\n",
      "Iter 34: Loss 2.58751765164462\n",
      "Iter 34: Loss 2.58751765164462\n",
      "Iter 35: Loss 2.5791547948663887\n",
      "Iter 35: Loss 2.5791547948663887\n",
      "Iter 36: Loss 2.5572743849320845\n",
      "Iter 36: Loss 2.5572743849320845\n",
      "Iter 37: Loss 2.5428449457341973\n",
      "Iter 37: Loss 2.5428449457341973\n",
      "Iter 38: Loss 2.5299498818137427\n",
      "Iter 38: Loss 2.5299498818137427\n",
      "Iter 39: Loss 2.5172774574973364\n",
      "Iter 39: Loss 2.5172774574973364\n",
      "Iter 40: Loss 2.4972591400146484\n",
      "Iter 40: Loss 2.4972591400146484\n",
      "Iter 41: Loss 2.48335092717951\n",
      "Iter 41: Loss 2.48335092717951\n",
      "Iter 42: Loss 2.496988123113459\n",
      "Iter 42: Loss 2.496988123113459\n",
      "Iter 43: Loss 2.4606073552911933\n",
      "Iter 43: Loss 2.4606073552911933\n",
      "Iter 44: Loss 2.441648830067028\n",
      "Iter 44: Loss 2.441648830067028\n",
      "Iter 45: Loss 2.449553749778054\n",
      "Iter 45: Loss 2.449553749778054\n",
      "Iter 46: Loss 2.417111483487216\n",
      "Iter 46: Loss 2.417111483487216\n",
      "Iter 47: Loss 2.403180729259144\n",
      "Iter 47: Loss 2.403180729259144\n",
      "Iter 48: Loss 2.393855181607333\n",
      "Iter 48: Loss 2.393855181607333\n",
      "Iter 49: Loss 2.3839832652698862\n",
      "Iter 49: Loss 2.3839832652698862\n",
      "Iter 50: Loss 2.396099090576172\n",
      "Iter 50: Loss 2.396099090576172\n",
      "Iter 51: Loss 2.367835131558505\n",
      "Iter 51: Loss 2.367835131558505\n",
      "Iter 52: Loss 2.359770341352983\n",
      "Iter 52: Loss 2.359770341352983\n",
      "Iter 53: Loss 2.3424816131591797\n",
      "Iter 53: Loss 2.3424816131591797\n",
      "Iter 54: Loss 2.327535802667791\n",
      "Iter 54: Loss 2.327535802667791\n",
      "Iter 55: Loss 2.3080664548006924\n",
      "Iter 55: Loss 2.3080664548006924\n",
      "Iter 56: Loss 2.3179505088112573\n",
      "Iter 56: Loss 2.3179505088112573\n",
      "Iter 57: Loss 2.2954484766179863\n",
      "Iter 57: Loss 2.2954484766179863\n",
      "Iter 58: Loss 2.2981515364213423\n",
      "Iter 58: Loss 2.2981515364213423\n",
      "Iter 59: Loss 2.290203094482422\n",
      "Iter 59: Loss 2.290203094482422\n",
      "Iter 60: Loss 2.266885930841619\n",
      "Iter 60: Loss 2.266885930841619\n",
      "Iter 61: Loss 2.266046177257191\n",
      "Iter 61: Loss 2.266046177257191\n",
      "Iter 62: Loss 2.244200446388938\n",
      "Iter 62: Loss 2.244200446388938\n",
      "Iter 63: Loss 2.2338201349431817\n",
      "Iter 63: Loss 2.2338201349431817\n",
      "Iter 64: Loss 2.215066736394709\n",
      "Iter 64: Loss 2.215066736394709\n",
      "Iter 65: Loss 2.2096484791148794\n",
      "Iter 65: Loss 2.2096484791148794\n",
      "Iter 66: Loss 2.2051233811811968\n",
      "Iter 66: Loss 2.2051233811811968\n",
      "Iter 67: Loss 2.205022465098988\n",
      "Iter 67: Loss 2.205022465098988\n",
      "Iter 68: Loss 2.1914107582785864\n",
      "Iter 68: Loss 2.1914107582785864\n",
      "Iter 69: Loss 2.17854829268022\n",
      "Iter 69: Loss 2.17854829268022\n",
      "Iter 70: Loss 2.1719799041748047\n",
      "Iter 70: Loss 2.1719799041748047\n",
      "Iter 71: Loss 2.1527706493030894\n",
      "Iter 71: Loss 2.1527706493030894\n",
      "Iter 72: Loss 2.146451083096591\n",
      "Iter 72: Loss 2.146451083096591\n",
      "Iter 73: Loss 2.155898180874911\n",
      "Iter 73: Loss 2.155898180874911\n",
      "Iter 74: Loss 2.135924599387429\n",
      "Iter 74: Loss 2.135924599387429\n",
      "Iter 75: Loss 2.1222679831764917\n",
      "Iter 75: Loss 2.1222679831764917\n",
      "Iter 76: Loss 2.116729562932795\n",
      "Iter 76: Loss 2.116729562932795\n",
      "Iter 77: Loss 2.104990525679155\n",
      "Iter 77: Loss 2.104990525679155\n",
      "Iter 78: Loss 2.100440805608576\n",
      "Iter 78: Loss 2.100440805608576\n",
      "Iter 79: Loss 2.0953953482887964\n",
      "Iter 79: Loss 2.0953953482887964\n",
      "Iter 80: Loss 2.097431356256658\n",
      "Iter 80: Loss 2.097431356256658\n",
      "Iter 81: Loss 2.0779647827148438\n",
      "Iter 81: Loss 2.0779647827148438\n",
      "Iter 82: Loss 2.0738499381325464\n",
      "Iter 82: Loss 2.0738499381325464\n",
      "Iter 83: Loss 2.0571023767644707\n",
      "Iter 83: Loss 2.0571023767644707\n",
      "Iter 84: Loss 2.0452627702192827\n",
      "Iter 84: Loss 2.0452627702192827\n",
      "Iter 85: Loss 2.048969268798828\n",
      "Iter 85: Loss 2.048969268798828\n",
      "Iter 86: Loss 2.026769291270863\n",
      "Iter 86: Loss 2.026769291270863\n",
      "Iter 87: Loss 2.0317407087846235\n",
      "Iter 87: Loss 2.0317407087846235\n",
      "Iter 88: Loss 2.0269191048362036\n",
      "Iter 88: Loss 2.0269191048362036\n",
      "Iter 89: Loss 2.0160595286976206\n",
      "Iter 89: Loss 2.0160595286976206\n",
      "Iter 90: Loss 2.012825705788352\n",
      "Iter 90: Loss 2.012825705788352\n",
      "Iter 91: Loss 2.0033997622403232\n",
      "Iter 91: Loss 2.0033997622403232\n",
      "Iter 92: Loss 1.9878007715398616\n",
      "Iter 92: Loss 1.9878007715398616\n",
      "Iter 93: Loss 1.9784821597012607\n",
      "Iter 93: Loss 1.9784821597012607\n",
      "Iter 94: Loss 1.9790191650390625\n",
      "Iter 94: Loss 1.9790191650390625\n",
      "Iter 95: Loss 1.9827641573819248\n",
      "Iter 95: Loss 1.9827641573819248\n",
      "Iter 96: Loss 1.9696280739524148\n",
      "Iter 96: Loss 1.9696280739524148\n",
      "Iter 97: Loss 1.972108320756392\n",
      "Iter 97: Loss 1.972108320756392\n",
      "Iter 98: Loss 1.95661770213734\n",
      "Iter 98: Loss 1.95661770213734\n",
      "Iter 99: Loss 1.9485421614213423\n",
      "Iter 99: Loss 1.9485421614213423\n",
      "Iter 100: Loss 1.9541390158913352\n",
      "Iter 100: Loss 1.9541390158913352\n",
      "Iter 101: Loss 1.9347504702481357\n",
      "Iter 101: Loss 1.9347504702481357\n",
      "Iter 102: Loss 1.9305804859508167\n",
      "Iter 102: Loss 1.9305804859508167\n",
      "Iter 103: Loss 1.9209306890314275\n",
      "Iter 103: Loss 1.9209306890314275\n",
      "Iter 104: Loss 1.9174520319158381\n",
      "Iter 104: Loss 1.9174520319158381\n",
      "Iter 105: Loss 1.9139033230868252\n",
      "Iter 105: Loss 1.9139033230868252\n",
      "Iter 106: Loss 1.900029095736417\n",
      "Iter 106: Loss 1.900029095736417\n",
      "Iter 107: Loss 1.9059019955721768\n",
      "Iter 107: Loss 1.9059019955721768\n",
      "Iter 108: Loss 1.8943803960626775\n",
      "Iter 108: Loss 1.8943803960626775\n",
      "Iter 109: Loss 1.881562666459517\n",
      "Iter 109: Loss 1.881562666459517\n",
      "Iter 110: Loss 1.885657397183505\n",
      "Iter 110: Loss 1.885657397183505\n",
      "Iter 111: Loss 1.8912582397460938\n",
      "Iter 111: Loss 1.8912582397460938\n",
      "Iter 112: Loss 1.8810043334960938\n",
      "Iter 112: Loss 1.8810043334960938\n",
      "Iter 113: Loss 1.8662698919122869\n",
      "Iter 113: Loss 1.8662698919122869\n",
      "Iter 114: Loss 1.8590016798539595\n",
      "Iter 114: Loss 1.8590016798539595\n",
      "Iter 115: Loss 1.8578501614657315\n",
      "Iter 115: Loss 1.8578501614657315\n",
      "Iter 116: Loss 1.8507680025967685\n",
      "Iter 116: Loss 1.8507680025967685\n",
      "Iter 117: Loss 1.844436992298473\n",
      "Iter 117: Loss 1.844436992298473\n",
      "Iter 118: Loss 1.85342077775435\n",
      "Iter 118: Loss 1.85342077775435\n",
      "Iter 119: Loss 1.8401905406605115\n",
      "Iter 119: Loss 1.8401905406605115\n",
      "Iter 120: Loss 1.8282269564541904\n",
      "Iter 120: Loss 1.8282269564541904\n",
      "Iter 121: Loss 1.8202396739612927\n",
      "Iter 121: Loss 1.8202396739612927\n",
      "Iter 122: Loss 1.8208198547363281\n",
      "Iter 122: Loss 1.8208198547363281\n",
      "Iter 123: Loss 1.818660562688654\n",
      "Iter 123: Loss 1.818660562688654\n",
      "Iter 124: Loss 1.7996805364435369\n",
      "Iter 124: Loss 1.7996805364435369\n",
      "Iter 125: Loss 1.8076785694469104\n",
      "Iter 125: Loss 1.8076785694469104\n",
      "Iter 126: Loss 1.7974830974232068\n",
      "Iter 126: Loss 1.7974830974232068\n",
      "Iter 127: Loss 1.8098141063343396\n",
      "Iter 127: Loss 1.8098141063343396\n",
      "Iter 128: Loss 1.7987258217551492\n",
      "Iter 128: Loss 1.7987258217551492\n",
      "Iter 129: Loss 1.7852155512029475\n",
      "Iter 129: Loss 1.7852155512029475\n",
      "Iter 130: Loss 1.786096919666637\n",
      "Iter 130: Loss 1.786096919666637\n",
      "Iter 131: Loss 1.7822480635209517\n",
      "Iter 131: Loss 1.7822480635209517\n",
      "Iter 132: Loss 1.7761726379394531\n",
      "Iter 132: Loss 1.7761726379394531\n",
      "Iter 133: Loss 1.7747887698086826\n",
      "Iter 133: Loss 1.7747887698086826\n",
      "Iter 134: Loss 1.7720938595858486\n",
      "Iter 134: Loss 1.7720938595858486\n",
      "Iter 135: Loss 1.767369183627042\n",
      "Iter 135: Loss 1.767369183627042\n",
      "Iter 136: Loss 1.7602027546275745\n",
      "Iter 136: Loss 1.7602027546275745\n",
      "Iter 137: Loss 1.7558671777898616\n",
      "Iter 137: Loss 1.7558671777898616\n",
      "Iter 138: Loss 1.7517857985063032\n",
      "Iter 138: Loss 1.7517857985063032\n",
      "Iter 139: Loss 1.7474784851074219\n",
      "Iter 139: Loss 1.7474784851074219\n",
      "Iter 140: Loss 1.758218765258789\n",
      "Iter 140: Loss 1.758218765258789\n",
      "Iter 141: Loss 1.7344823317094282\n",
      "Iter 141: Loss 1.7344823317094282\n",
      "Iter 142: Loss 1.7366003556685015\n",
      "Iter 142: Loss 1.7366003556685015\n",
      "Iter 143: Loss 1.7230904319069602\n",
      "Iter 143: Loss 1.7230904319069602\n",
      "Iter 144: Loss 1.7220698269930752\n",
      "Iter 144: Loss 1.7220698269930752\n",
      "Iter 145: Loss 1.7298396717418323\n",
      "Iter 145: Loss 1.7298396717418323\n",
      "Iter 146: Loss 1.7004928588867188\n",
      "Iter 146: Loss 1.7004928588867188\n",
      "Iter 147: Loss 1.7101801091974431\n",
      "Iter 147: Loss 1.7101801091974431\n",
      "Iter 148: Loss 1.708194212480025\n",
      "Iter 148: Loss 1.708194212480025\n",
      "Iter 149: Loss 1.7036517750133167\n",
      "Iter 149: Loss 1.7036517750133167\n",
      "Iter 150: Loss 1.6991892727938565\n",
      "Iter 150: Loss 1.6991892727938565\n",
      "Iter 151: Loss 1.7048364119096235\n",
      "Iter 151: Loss 1.7048364119096235\n",
      "Iter 152: Loss 1.6903492320667615\n",
      "Iter 152: Loss 1.6903492320667615\n",
      "Iter 153: Loss 1.680943402377042\n",
      "Iter 153: Loss 1.680943402377042\n",
      "Iter 154: Loss 1.6914650310169568\n",
      "Iter 154: Loss 1.6914650310169568\n",
      "Iter 155: Loss 1.6876820650967685\n",
      "Iter 155: Loss 1.6876820650967685\n",
      "Iter 156: Loss 1.6845498518510298\n",
      "Iter 156: Loss 1.6845498518510298\n",
      "Iter 157: Loss 1.6694896004416726\n",
      "Iter 157: Loss 1.6694896004416726\n",
      "Iter 158: Loss 1.6779447035356\n",
      "Iter 158: Loss 1.6779447035356\n",
      "Iter 159: Loss 1.669005654074929\n",
      "Iter 159: Loss 1.669005654074929\n",
      "Iter 160: Loss 1.6629933443936435\n",
      "Iter 160: Loss 1.6629933443936435\n",
      "Iter 161: Loss 1.6581885597922585\n",
      "Iter 161: Loss 1.6581885597922585\n",
      "Iter 162: Loss 1.6703262329101562\n",
      "Iter 162: Loss 1.6703262329101562\n",
      "Iter 163: Loss 1.66065805608576\n",
      "Iter 163: Loss 1.66065805608576\n",
      "Iter 164: Loss 1.65275070884011\n",
      "Iter 164: Loss 1.65275070884011\n",
      "Iter 165: Loss 1.6446285247802734\n",
      "Iter 165: Loss 1.6446285247802734\n",
      "Iter 166: Loss 1.6436471072110264\n",
      "Iter 166: Loss 1.6436471072110264\n",
      "Iter 167: Loss 1.6390717246315696\n",
      "Iter 167: Loss 1.6390717246315696\n",
      "Iter 168: Loss 1.641884370283647\n",
      "Iter 168: Loss 1.641884370283647\n",
      "Iter 169: Loss 1.6341615156693892\n",
      "Iter 169: Loss 1.6341615156693892\n",
      "Iter 170: Loss 1.6306958632035689\n",
      "Iter 170: Loss 1.6306958632035689\n",
      "Iter 171: Loss 1.620418375188654\n",
      "Iter 171: Loss 1.620418375188654\n",
      "Iter 172: Loss 1.6196816184303977\n",
      "Iter 172: Loss 1.6196816184303977\n",
      "Iter 173: Loss 1.6207610043612393\n",
      "Iter 173: Loss 1.6207610043612393\n",
      "Iter 174: Loss 1.6155066056685015\n",
      "Iter 174: Loss 1.6155066056685015\n",
      "Iter 175: Loss 1.600154703313654\n",
      "Iter 175: Loss 1.600154703313654\n",
      "Iter 176: Loss 1.5944186123934658\n",
      "Iter 176: Loss 1.5944186123934658\n",
      "Iter 177: Loss 1.6177636926824397\n",
      "Iter 177: Loss 1.6177636926824397\n",
      "Iter 178: Loss 1.5943385037508877\n",
      "Iter 178: Loss 1.5943385037508877\n",
      "Iter 179: Loss 1.5975619229403408\n",
      "Iter 179: Loss 1.5975619229403408\n",
      "Iter 180: Loss 1.5945118990811435\n",
      "Iter 180: Loss 1.5945118990811435\n",
      "Iter 181: Loss 1.5874486403031782\n",
      "Iter 181: Loss 1.5874486403031782\n",
      "Iter 182: Loss 1.5840544267134233\n",
      "Iter 182: Loss 1.5840544267134233\n",
      "Iter 183: Loss 1.5864686098965732\n",
      "Iter 183: Loss 1.5864686098965732\n",
      "Iter 184: Loss 1.5790504108775745\n",
      "Iter 184: Loss 1.5790504108775745\n",
      "Iter 185: Loss 1.58556452664462\n",
      "Iter 185: Loss 1.58556452664462\n",
      "Iter 186: Loss 1.5714886405251243\n",
      "Iter 186: Loss 1.5714886405251243\n",
      "Iter 187: Loss 1.5607147216796875\n",
      "Iter 187: Loss 1.5607147216796875\n",
      "Iter 188: Loss 1.566693739457564\n",
      "Iter 188: Loss 1.566693739457564\n",
      "Iter 189: Loss 1.5621239055286755\n",
      "Iter 189: Loss 1.5621239055286755\n",
      "Iter 190: Loss 1.554204594005238\n",
      "Iter 190: Loss 1.554204594005238\n",
      "Iter 191: Loss 1.5577059659090908\n",
      "Iter 191: Loss 1.5577059659090908\n",
      "Iter 192: Loss 1.5563104802911931\n",
      "Iter 192: Loss 1.5563104802911931\n",
      "Iter 193: Loss 1.5568070845170454\n",
      "Iter 193: Loss 1.5568070845170454\n",
      "Iter 194: Loss 1.5580222389914773\n",
      "Iter 194: Loss 1.5580222389914773\n",
      "Iter 195: Loss 1.5469395030628552\n",
      "Iter 195: Loss 1.5469395030628552\n",
      "Iter 196: Loss 1.5445825403386897\n",
      "Iter 196: Loss 1.5445825403386897\n",
      "Iter 197: Loss 1.5442452864213423\n",
      "Iter 197: Loss 1.5442452864213423\n",
      "Iter 198: Loss 1.545727469704368\n",
      "Iter 198: Loss 1.545727469704368\n",
      "Iter 199: Loss 1.542755820534446\n",
      "Iter 199: Loss 1.542755820534446\n",
      "Iter 200: Loss 1.5377785075794568\n",
      "Iter 200: Loss 1.5377785075794568\n",
      "Iter 201: Loss 1.5342552878639915\n",
      "Iter 201: Loss 1.5342552878639915\n",
      "Iter 202: Loss 1.526833794333718\n",
      "Iter 202: Loss 1.526833794333718\n",
      "Iter 203: Loss 1.520276503129439\n",
      "Iter 203: Loss 1.520276503129439\n",
      "Iter 204: Loss 1.52351205999201\n",
      "Iter 204: Loss 1.52351205999201\n",
      "Iter 205: Loss 1.5057882829145952\n",
      "Iter 205: Loss 1.5057882829145952\n",
      "Iter 206: Loss 1.5194147283380681\n",
      "Iter 206: Loss 1.5194147283380681\n",
      "Iter 207: Loss 1.5207848982377485\n",
      "Iter 207: Loss 1.5207848982377485\n",
      "Iter 208: Loss 1.5068321228027344\n",
      "Iter 208: Loss 1.5068321228027344\n",
      "Iter 209: Loss 1.5070991516113281\n",
      "Iter 209: Loss 1.5070991516113281\n",
      "Iter 210: Loss 1.4987066442316228\n",
      "Iter 210: Loss 1.4987066442316228\n",
      "Iter 211: Loss 1.5030243613503196\n",
      "Iter 211: Loss 1.5030243613503196\n",
      "Iter 212: Loss 1.503321040760387\n",
      "Iter 212: Loss 1.503321040760387\n",
      "Iter 213: Loss 1.5065548636696555\n",
      "Iter 213: Loss 1.5065548636696555\n",
      "Iter 214: Loss 1.4874028292569248\n",
      "Iter 214: Loss 1.4874028292569248\n",
      "Iter 215: Loss 1.4805483384565874\n",
      "Iter 215: Loss 1.4805483384565874\n",
      "Iter 216: Loss 1.4834872159090908\n",
      "Iter 216: Loss 1.4834872159090908\n",
      "Iter 217: Loss 1.485261396928267\n",
      "Iter 217: Loss 1.485261396928267\n",
      "Iter 218: Loss 1.4779139432040127\n",
      "Iter 218: Loss 1.4779139432040127\n",
      "Iter 219: Loss 1.4689116044477983\n",
      "Iter 219: Loss 1.4689116044477983\n",
      "Iter 220: Loss 1.4762684215198865\n",
      "Iter 220: Loss 1.4762684215198865\n",
      "Iter 221: Loss 1.481612812389027\n",
      "Iter 221: Loss 1.481612812389027\n",
      "Iter 222: Loss 1.4683524045077236\n",
      "Iter 222: Loss 1.4683524045077236\n",
      "Iter 223: Loss 1.4673708135431462\n",
      "Iter 223: Loss 1.4673708135431462\n",
      "Iter 224: Loss 1.4575311487371272\n",
      "Iter 224: Loss 1.4575311487371272\n",
      "Iter 225: Loss 1.4573189128528943\n",
      "Iter 225: Loss 1.4573189128528943\n",
      "Iter 226: Loss 1.4539524425159802\n",
      "Iter 226: Loss 1.4539524425159802\n",
      "Iter 227: Loss 1.4482858831232244\n",
      "Iter 227: Loss 1.4482858831232244\n",
      "Iter 228: Loss 1.45095495744185\n",
      "Iter 228: Loss 1.45095495744185\n",
      "Iter 229: Loss 1.4526709643277256\n",
      "Iter 229: Loss 1.4526709643277256\n",
      "Iter 230: Loss 1.4557066830721768\n",
      "Iter 230: Loss 1.4557066830721768\n",
      "Iter 231: Loss 1.4414615631103516\n",
      "Iter 231: Loss 1.4414615631103516\n",
      "Iter 232: Loss 1.4346108870072798\n",
      "Iter 232: Loss 1.4346108870072798\n",
      "Iter 233: Loss 1.4396259134465998\n",
      "Iter 233: Loss 1.4396259134465998\n",
      "Iter 234: Loss 1.442740266973322\n",
      "Iter 234: Loss 1.442740266973322\n",
      "Iter 235: Loss 1.4388985200361772\n",
      "Iter 235: Loss 1.4388985200361772\n",
      "Iter 236: Loss 1.4340262846513228\n",
      "Iter 236: Loss 1.4340262846513228\n",
      "Iter 237: Loss 1.4289746717973189\n",
      "Iter 237: Loss 1.4289746717973189\n",
      "Iter 238: Loss 1.428142634305087\n",
      "Iter 238: Loss 1.428142634305087\n",
      "Iter 239: Loss 1.4278424869884143\n",
      "Iter 239: Loss 1.4278424869884143\n",
      "Iter 240: Loss 1.4252409501509233\n",
      "Iter 240: Loss 1.4252409501509233\n",
      "Iter 241: Loss 1.4217222387140447\n",
      "Iter 241: Loss 1.4217222387140447\n",
      "Iter 242: Loss 1.416975888338956\n",
      "Iter 242: Loss 1.416975888338956\n",
      "Iter 243: Loss 1.4192960912531072\n",
      "Iter 243: Loss 1.4192960912531072\n",
      "Iter 244: Loss 1.4097125313498757\n",
      "Iter 244: Loss 1.4097125313498757\n",
      "Iter 245: Loss 1.4090201637961648\n",
      "Iter 245: Loss 1.4090201637961648\n",
      "Iter 246: Loss 1.4049687819047407\n",
      "Iter 246: Loss 1.4049687819047407\n",
      "Iter 247: Loss 1.3952602039683948\n",
      "Iter 247: Loss 1.3952602039683948\n",
      "Iter 248: Loss 1.3983882557262073\n",
      "Iter 248: Loss 1.3983882557262073\n",
      "Iter 249: Loss 1.4140378778631038\n",
      "Iter 249: Loss 1.4140378778631038\n",
      "Iter 250: Loss 1.406798622824929\n",
      "Iter 250: Loss 1.406798622824929\n",
      "Iter 251: Loss 1.3960181149569424\n",
      "Iter 251: Loss 1.3960181149569424\n",
      "Iter 252: Loss 1.388005169955167\n",
      "Iter 252: Loss 1.388005169955167\n",
      "Iter 253: Loss 1.3864506808194248\n",
      "Iter 253: Loss 1.3864506808194248\n",
      "Iter 254: Loss 1.388100797479803\n",
      "Iter 254: Loss 1.388100797479803\n",
      "Iter 255: Loss 1.3776037909767844\n",
      "Iter 255: Loss 1.3776037909767844\n",
      "Iter 256: Loss 1.3846762397072532\n",
      "Iter 256: Loss 1.3846762397072532\n",
      "Iter 257: Loss 1.3757576508955522\n",
      "Iter 257: Loss 1.3757576508955522\n",
      "Iter 258: Loss 1.3766892173073508\n",
      "Iter 258: Loss 1.3766892173073508\n",
      "Iter 259: Loss 1.3795104460282759\n",
      "Iter 259: Loss 1.3795104460282759\n",
      "Iter 260: Loss 1.37476955760609\n",
      "Iter 260: Loss 1.37476955760609\n",
      "Iter 261: Loss 1.3856286135586826\n",
      "Iter 261: Loss 1.3856286135586826\n",
      "Iter 262: Loss 1.360124414617365\n",
      "Iter 262: Loss 1.360124414617365\n",
      "Iter 263: Loss 1.3642662221735173\n",
      "Iter 263: Loss 1.3642662221735173\n",
      "Iter 264: Loss 1.365219463001598\n",
      "Iter 264: Loss 1.365219463001598\n",
      "Iter 265: Loss 1.3605948361483486\n",
      "Iter 265: Loss 1.3605948361483486\n",
      "Iter 266: Loss 1.3594433177601208\n",
      "Iter 266: Loss 1.3594433177601208\n",
      "Iter 267: Loss 1.3614841807972302\n",
      "Iter 267: Loss 1.3614841807972302\n",
      "Iter 268: Loss 1.3573934381658381\n",
      "Iter 268: Loss 1.3573934381658381\n",
      "Iter 269: Loss 1.3493599458174272\n",
      "Iter 269: Loss 1.3493599458174272\n",
      "Iter 270: Loss 1.3639875758777966\n",
      "Iter 270: Loss 1.3639875758777966\n",
      "Iter 271: Loss 1.3443320014260032\n",
      "Iter 271: Loss 1.3443320014260032\n",
      "Iter 272: Loss 1.342926025390625\n",
      "Iter 272: Loss 1.342926025390625\n",
      "Iter 273: Loss 1.3444446216930042\n",
      "Iter 273: Loss 1.3444446216930042\n",
      "Iter 274: Loss 1.3495576164939187\n",
      "Iter 274: Loss 1.3495576164939187\n",
      "Iter 275: Loss 1.3359659368341619\n",
      "Iter 275: Loss 1.3359659368341619\n",
      "Iter 276: Loss 1.3393083052201704\n",
      "Iter 276: Loss 1.3393083052201704\n",
      "Iter 277: Loss 1.3461077430031516\n",
      "Iter 277: Loss 1.3461077430031516\n",
      "Iter 278: Loss 1.32720609144731\n",
      "Iter 278: Loss 1.32720609144731\n",
      "Iter 279: Loss 1.3397137035023083\n",
      "Iter 279: Loss 1.3397137035023083\n",
      "Iter 280: Loss 1.3292260603471235\n",
      "Iter 280: Loss 1.3292260603471235\n",
      "Iter 281: Loss 1.3302641781893643\n",
      "Iter 281: Loss 1.3302641781893643\n",
      "Iter 282: Loss 1.3194697119972922\n",
      "Iter 282: Loss 1.3194697119972922\n",
      "Iter 283: Loss 1.3319372697310015\n",
      "Iter 283: Loss 1.3319372697310015\n",
      "Iter 284: Loss 1.320602070201527\n",
      "Iter 284: Loss 1.320602070201527\n",
      "Iter 285: Loss 1.3229123028841885\n",
      "Iter 285: Loss 1.3229123028841885\n",
      "Iter 286: Loss 1.3237037658691406\n",
      "Iter 286: Loss 1.3237037658691406\n",
      "Iter 287: Loss 1.314905513416637\n",
      "Iter 287: Loss 1.314905513416637\n",
      "Iter 288: Loss 1.3090729279951616\n",
      "Iter 288: Loss 1.3090729279951616\n",
      "Iter 289: Loss 1.3098684657703747\n",
      "Iter 289: Loss 1.3098684657703747\n",
      "Iter 290: Loss 1.316610423001376\n",
      "Iter 290: Loss 1.316610423001376\n",
      "Iter 291: Loss 1.3082231174815784\n",
      "Iter 291: Loss 1.3082231174815784\n",
      "Iter 292: Loss 1.2981562180952593\n",
      "Iter 292: Loss 1.2981562180952593\n",
      "Iter 293: Loss 1.3114089098843662\n",
      "Iter 293: Loss 1.3114089098843662\n",
      "Iter 294: Loss 1.2995432073419744\n",
      "Iter 294: Loss 1.2995432073419744\n",
      "Iter 295: Loss 1.2944453846324573\n",
      "Iter 295: Loss 1.2944453846324573\n",
      "Iter 296: Loss 1.300057671286843\n",
      "Iter 296: Loss 1.300057671286843\n",
      "Iter 297: Loss 1.2906422181562944\n",
      "Iter 297: Loss 1.2906422181562944\n",
      "Iter 298: Loss 1.2905391346324573\n",
      "Iter 298: Loss 1.2905391346324573\n",
      "Iter 299: Loss 1.285322362726385\n",
      "Iter 299: Loss 1.285322362726385\n",
      "Iter 300: Loss 1.2886219024658203\n",
      "Iter 300: Loss 1.2886219024658203\n",
      "Iter 301: Loss 1.284935170953924\n",
      "Iter 301: Loss 1.284935170953924\n",
      "Iter 302: Loss 1.2845093987204812\n",
      "Iter 302: Loss 1.2845093987204812\n",
      "Iter 303: Loss 1.280260606245561\n",
      "Iter 303: Loss 1.280260606245561\n",
      "Iter 304: Loss 1.2780780792236328\n",
      "Iter 304: Loss 1.2780780792236328\n",
      "Iter 305: Loss 1.2803132317282937\n",
      "Iter 305: Loss 1.2803132317282937\n",
      "Iter 306: Loss 1.271964506669478\n",
      "Iter 306: Loss 1.271964506669478\n",
      "Iter 307: Loss 1.278607108376243\n",
      "Iter 307: Loss 1.278607108376243\n",
      "Iter 308: Loss 1.2753545587713069\n",
      "Iter 308: Loss 1.2753545587713069\n",
      "Iter 309: Loss 1.28255202553489\n",
      "Iter 309: Loss 1.28255202553489\n",
      "Iter 310: Loss 1.274706320329146\n",
      "Iter 310: Loss 1.274706320329146\n",
      "Iter 311: Loss 1.2723533456975764\n",
      "Iter 311: Loss 1.2723533456975764\n",
      "Iter 312: Loss 1.261063662442294\n",
      "Iter 312: Loss 1.261063662442294\n",
      "Iter 313: Loss 1.270181569186124\n",
      "Iter 313: Loss 1.270181569186124\n",
      "Iter 314: Loss 1.2578133669766514\n",
      "Iter 314: Loss 1.2578133669766514\n",
      "Iter 315: Loss 1.2496862411499023\n",
      "Iter 315: Loss 1.2496862411499023\n",
      "Iter 316: Loss 1.2518646066839045\n",
      "Iter 316: Loss 1.2518646066839045\n",
      "Iter 317: Loss 1.2594002810391514\n",
      "Iter 317: Loss 1.2594002810391514\n",
      "Iter 318: Loss 1.2485965381969104\n",
      "Iter 318: Loss 1.2485965381969104\n",
      "Iter 319: Loss 1.260148048400879\n",
      "Iter 319: Loss 1.260148048400879\n",
      "Iter 320: Loss 1.25615778836337\n",
      "Iter 320: Loss 1.25615778836337\n",
      "Iter 321: Loss 1.255223274230957\n",
      "Iter 321: Loss 1.255223274230957\n",
      "Iter 322: Loss 1.2416654933582654\n",
      "Iter 322: Loss 1.2416654933582654\n",
      "Iter 323: Loss 1.2495764819058506\n",
      "Iter 323: Loss 1.2495764819058506\n",
      "Iter 324: Loss 1.2440804568204014\n",
      "Iter 324: Loss 1.2440804568204014\n",
      "Iter 325: Loss 1.240488052368164\n",
      "Iter 325: Loss 1.240488052368164\n",
      "Iter 326: Loss 1.2494420138272373\n",
      "Iter 326: Loss 1.2494420138272373\n",
      "Iter 327: Loss 1.2468157681551846\n",
      "Iter 327: Loss 1.2468157681551846\n",
      "Iter 328: Loss 1.227621772072532\n",
      "Iter 328: Loss 1.227621772072532\n",
      "Iter 329: Loss 1.2370867295698686\n",
      "Iter 329: Loss 1.2370867295698686\n",
      "Iter 330: Loss 1.228249116377397\n",
      "Iter 330: Loss 1.228249116377397\n",
      "Iter 331: Loss 1.2289239710027522\n",
      "Iter 331: Loss 1.2289239710027522\n",
      "Iter 332: Loss 1.235516981645064\n",
      "Iter 332: Loss 1.235516981645064\n",
      "Iter 333: Loss 1.2191485491665928\n",
      "Iter 333: Loss 1.2191485491665928\n",
      "Iter 334: Loss 1.2323916175148704\n",
      "Iter 334: Loss 1.2323916175148704\n",
      "Iter 335: Loss 1.2240491346879439\n",
      "Iter 335: Loss 1.2240491346879439\n",
      "Iter 336: Loss 1.2206208489157937\n",
      "Iter 336: Loss 1.2206208489157937\n",
      "Iter 337: Loss 1.2147039066661487\n",
      "Iter 337: Loss 1.2147039066661487\n",
      "Iter 338: Loss 1.2176436510953037\n",
      "Iter 338: Loss 1.2176436510953037\n",
      "Iter 339: Loss 1.2117840160023083\n",
      "Iter 339: Loss 1.2117840160023083\n",
      "Iter 340: Loss 1.212462598627264\n",
      "Iter 340: Loss 1.212462598627264\n",
      "Iter 341: Loss 1.2226735028353604\n",
      "Iter 341: Loss 1.2226735028353604\n",
      "Iter 342: Loss 1.2098412947221235\n",
      "Iter 342: Loss 1.2098412947221235\n",
      "Iter 343: Loss 1.2101814096624202\n",
      "Iter 343: Loss 1.2101814096624202\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m optimizer_model_decoder\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m optimizer_emb\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(T\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # model.eval()\n",
    "    # decoder.eval()\n",
    "    # emb_func.eval()\n",
    "    w = get_batch('train', batch_size)  # already on device\n",
    "    w_emb = emb_func(w)  # (B, T, C)\n",
    "    x0 = w_emb + 1.2 * torch.randn_like(w_emb)  # use randn_like for Gaussian noise\n",
    "    \n",
    "    total_loss = 0\n",
    "    for t_step in range(T + 1):\n",
    "        t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "        xt = fwd_sample(x0, t_tensor, alphas)\n",
    "        x0_cap = model(xt, t_tensor)\n",
    "        total_loss += F.mse_loss(x0_cap, x0)\n",
    "    \n",
    "    # Final step loss\n",
    "    t_one = torch.tensor([1] * batch_size, device=device)\n",
    "    total_loss += F.mse_loss(model(fwd_sample(x0, t_one, alphas), t_one), w_emb)\n",
    "\n",
    "    # Decoder cross-entropy loss\n",
    "    logits = decoder(x0)  # (B, T, V)\n",
    "    V = config.n_vocab\n",
    "    logits_flat = logits.view(-1, V)  # (B*T, V)\n",
    "    targets_flat = w.view(-1)  # (B*T,)\n",
    "    total_loss += F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "    optimizer_model.zero_grad(set_to_none=True)\n",
    "    optimizer_model_decoder.zero_grad(set_to_none=True)\n",
    "    optimizer_emb.zero_grad(set_to_none=True)\n",
    "    total_loss.backward()\n",
    "    optimizer_model.step()\n",
    "    optimizer_model_decoder.step()\n",
    "    optimizer_emb.step()\n",
    "    \n",
    "    print(f\"Iter {iter}: Loss {total_loss.item() / (T + 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bba9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad887e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
