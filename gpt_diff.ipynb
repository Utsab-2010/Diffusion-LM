{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ee3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "from dataclasses import dataclass\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febc5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class gpt2config:\n",
    "    n_vocab: int = 50257\n",
    "    n_layer: int = 8\n",
    "    n_embed: int = 64\n",
    "    n_context: int = 1024\n",
    "    n_head: int = 8\n",
    "    n_timesteps: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fea93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        \n",
    "        # Create a causal mask (lower triangular matrix) and register it as a buffer\n",
    "        # A buffer is not a parameter, but is saved with the model state_dict\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_context, config.n_context))\n",
    "                                     .view(1, 1, config.n_context, config.n_context))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        \n",
    "        # --- MASKING STARTS HERE ---\n",
    "        # Apply the causal mask: fill \"future\" positions with -infinity\n",
    "        # This makes their softmax probability zero.\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # --- MASKING ENDS HERE ---\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, hs)\n",
    "        \n",
    "        # Re-assemble all head outputs side-by-side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52ba8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) #\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a5b755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sine_embeds = SinusoidalPositionEmbeddings(100)\n",
    "time = 10\n",
    "time = torch.tensor([time], device=device)\n",
    "out = sine_embeds(time)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd61879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embed = nn.Embedding(config.n_vocab,config.n_embed)\n",
    "    \n",
    "    def forward(self,input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc33b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # wte = nn.Embedding(config.n_vocab,config.n_embed),\n",
    "            wpe = nn.Embedding(config.n_context,config.n_embed),\n",
    "            drop = nn.Dropout(0.1,inplace=False),\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        ))\n",
    "        \n",
    "        # self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "\n",
    "        self.small_mlp = nn.Linear(config.n_embed, config.n_embed)\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(config.n_embed),\n",
    "            nn.Linear(config.n_embed, config.n_embed),\n",
    "            nn.GELU()\n",
    "            )\n",
    "\n",
    "    def forward(self,input_embeddings,time_step, targets=None):\n",
    "        B,T,C = input_embeddings.size()\n",
    "        device = input_embeddings.device\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device).unsqueeze(0)  # (1,T)\n",
    "        x = input_embeddings +  self.transformer.wpe(pos)  # (B,T,C) pytorch does braodcasting for the position embeddingss and adds them to the token embeddings \n",
    "        \n",
    "        time_emb = self.time_embed(time_step) # (B, C)\n",
    "        x= x + time_emb.unsqueeze(1)  # (B, T, C)\n",
    "        \n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)  # (B,T,C)\n",
    "        # logits = self.lm_head(x)  # (B,T,vocab_size) \n",
    "        # we don't need the head since we are not doing autoregressive language modeling\n",
    "        \n",
    "        # we want to predict the starting sequence before the noising part.\n",
    "        x = self.small_mlp(x)  # (B,T,C)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dbc6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    # takes x0 (B,T,C) and give a softmax over vocab size           \n",
    "        self.l1 = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.l1(x)\n",
    "        # x = F.softmax(x,dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556f0a6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd367c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 50257\n",
      "Token IDs: [15496, 11, 256, 1134, 30001, 318, 3049, 0]\n",
      "Token Count: 8\n",
      "Decoded: Hello, tiktoken is fast!\n",
      "gpt2config(n_vocab=50257, n_layer=8, n_embed=64, n_context=1024, n_head=8, n_timesteps=1000)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 1. Load the tokenizer for GPT-4o\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "print(\"vocab:\",tokenizer.n_vocab)\n",
    "# 2. Convert text to tokens\n",
    "text = \"Hello, tiktoken is fast!\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Token Count: {len(tokens)}\")\n",
    "\n",
    "# 3. Convert back to original text\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "\n",
    "config = gpt2config(n_vocab=tokenizer.n_vocab)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "523b58d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 0.47M\n",
      "Embedding parameters: 3.22M\n",
      "Decoder parameters: 3.22M\n"
     ]
    }
   ],
   "source": [
    "emb_func = LMEmbedding(config).to(device)\n",
    "model = Denoiser(config).to(device)\n",
    "decoder = Decoding(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Embedding parameters: {sum(p.numel() for p in emb_func.parameters())/1e6:.2f}M\")\n",
    "print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb61042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"Once upon a time in a land far away, there lived a\"\n",
    "sample_tokens = tokenizer.encode(sample_input)\n",
    "sample_input_ids = torch.tensor([sample_tokens], device=device)  # (1, sequence_length)\n",
    "sample_time_step = torch.tensor([10], device=device)  # (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9aeb074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb7111db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: largeanced Fuel RAND indifference surrounds feminassyCtrlstaticpclinicalArk\n"
     ]
    }
   ],
   "source": [
    "sample_output = model(emb_func(sample_input_ids), sample_time_step)  # (1, sequence_length, n_embed)\n",
    "\n",
    "def finalize_tokens(x0_final, embedding_weights):\n",
    "    \"\"\"\n",
    "    Converts the final denoised latent into discrete token IDs.\n",
    "    Args:\n",
    "        x0_final: Tensor of shape (B, T, C)\n",
    "        embedding_weights: Tensor of shape (Vocab, C)\n",
    "    \"\"\"\n",
    "    # Fix: x2 must be 3D to match x1 (B, T, C)\n",
    "    # Unsqueeze(0) makes it (1, Vocab, C), and PyTorch broadcasts it to (B, Vocab, C)\n",
    "    distances = torch.cdist(x0_final, embedding_weights.unsqueeze(0), p=2) #(B,T,Vocab)\n",
    "    # print(\"dist shape:\", distances.shape) \n",
    "    \n",
    "    # Argmin-rounding: Find the index with the minimum distance among all tokens in vocab\n",
    "    # Result shape: (B, T)\n",
    "    \n",
    "    token_ids = torch.argmin(distances, dim=-1)\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "token_ids = finalize_tokens(sample_output, emb_func.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5ff61",
   "metadata": {},
   "source": [
    "## Forward Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91a0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas(T=2000, s=1e-4):\n",
    "    \"\"\"\n",
    "    Computes the bar_alpha (signal) schedule for Diffusion-LM[cite: 232, 483].\n",
    "    s: constant determining initial noise level (standard dev = 0.1)[cite: 515].\n",
    "    \"\"\"\n",
    "    t = torch.linspace(0, T, T + 1)\n",
    "    # Sqrt schedule: alpha_bar = 1 - sqrt(t/T + s) \n",
    "    alphas = 1 - torch.sqrt(t / T )\n",
    "    \n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee8d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_sample(x0, t, alphas):\n",
    "    \"\"\"\n",
    "    Directly samples x_t from x_0 at a specific timestep[cite: 109, 170].\n",
    "    \n",
    "    Args:\n",
    "        x0: Clean embeddings (B, SeqLen, EmbedDim) [cite: 126]\n",
    "        t: Timesteps for the batch (B,) \n",
    "        alphas: Precomputed signal schedule from get_alphas()\n",
    "    \"\"\"\n",
    "    # Select alpha_bar for each batch item and reshape for broadcasting\n",
    "    a = alphas[t].view(-1, 1, 1).to(x0.device)\n",
    "    \n",
    "    # Sample Gaussian noise with same shape as x0\n",
    "    noise = torch.randn_like(x0)\n",
    "    \n",
    "    # Formula: x_t = sqrt(alpha_bar) * x0 + sqrt(1 - alpha_bar) * noise [cite: 169]\n",
    "    xt = torch.sqrt(a) * x0 + torch.sqrt(1 - a) * noise\n",
    "    \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca9ae511",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = get_alphas().to(device)\n",
    "\n",
    "noisy_input = fwd_sample(emb_func.embed(sample_input_ids), torch.tensor([1000], device=device), alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db9c5636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: Once upon aanka in a land executedisk 2020 there lived glanced\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_ids = finalize_tokens(noisy_input, emb_func.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21044d7b",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b2c8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 18007898 characters\n",
      "First 100 characters:\n",
      "The boy went to a video arcade. He played his favorite machine. His games didn't go very well. He to\n"
     ]
    }
   ],
   "source": [
    "# Load tiny shakespeare dataset\n",
    "with open('ROCStories_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 100 characters:\\n{text[0:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d37baff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded length: 4111142 tokens\n",
      "Train tokens: 3700027, Val tokens: 411115\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire dataset\n",
    "data = tokenizer.encode(text)\n",
    "print(f\"Encoded length: {len(data)} tokens\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "print(f\"Train tokens: {len(train_data)}, Val tokens: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfde8f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([8, 256])\n",
      "tensor([[ 3527, 13884,  1422,  ...,  7980,  1525,   373],\n",
      "        [  621,  1683,    13,  ...,   373,   262,   717],\n",
      "        [   13,   198,  2202,  ...,   607,  1204,    13],\n",
      "        ...,\n",
      "        [   13, 31251,    11,  ...,   198, 46827,  2227],\n",
      "        [  326,   262,  7545,  ..., 15228,   878,    11],\n",
      "        [  736,   379,   670,  ...,    13,  1406,  2427]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Data loader function\n",
    "def get_batch(split, batch_size=8, block_size=256):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    w_stack = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix])\n",
    "    # y = torch.stack([torch.tensor(data[i+1:i+block_size+1]) for i in ix])\n",
    "    w_stack = w_stack.to(device)\n",
    "    return w_stack\n",
    "\n",
    "# Test batch\n",
    "w_stack = get_batch('train')\n",
    "print(f\"Batch shape: {w_stack.shape}\")\n",
    "print(w_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5b43",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfd32855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "max_iters = 1000\n",
    "eval_interval = 10  # Evaluate less frequently\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 20  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 16  # Larger batch for better GPU utilization\n",
    "T = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62f13c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loss():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e41f701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff6babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_model = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer_model_decoder = torch.optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "optimizer_emb = torch.optim.AdamW(emb_func.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3209994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Loss 0.01532738127870236\n",
      "Iter 1: Loss 0.015299333545738114\n",
      "Iter 2: Loss 0.01523298465325209\n",
      "Iter 3: Loss 0.015151354129204968\n",
      "Iter 4: Loss 0.015145425549048387\n",
      "Iter 5: Loss 0.015049795428673901\n",
      "Iter 3: Loss 0.015151354129204968\n",
      "Iter 4: Loss 0.015145425549048387\n",
      "Iter 5: Loss 0.015049795428673901\n",
      "Iter 6: Loss 0.015006518411541176\n",
      "Iter 7: Loss 0.014960500294576863\n",
      "Iter 8: Loss 0.014916932987357804\n",
      "Iter 6: Loss 0.015006518411541176\n",
      "Iter 7: Loss 0.014960500294576863\n",
      "Iter 8: Loss 0.014916932987357804\n",
      "Iter 9: Loss 0.014866939323866915\n",
      "Iter 10: Loss 0.014778075341930885\n",
      "Iter 11: Loss 0.014727108016937317\n",
      "Iter 9: Loss 0.014866939323866915\n",
      "Iter 10: Loss 0.014778075341930885\n",
      "Iter 11: Loss 0.014727108016937317\n",
      "Iter 12: Loss 0.014695293175246187\n",
      "Iter 13: Loss 0.014676647985766747\n",
      "Iter 14: Loss 0.01457432691684502\n",
      "Iter 12: Loss 0.014695293175246187\n",
      "Iter 13: Loss 0.014676647985766747\n",
      "Iter 14: Loss 0.01457432691684502\n",
      "Iter 15: Loss 0.014545986990252892\n",
      "Iter 16: Loss 0.014512276221178249\n",
      "Iter 17: Loss 0.014464675309415349\n",
      "Iter 15: Loss 0.014545986990252892\n",
      "Iter 16: Loss 0.014512276221178249\n",
      "Iter 17: Loss 0.014464675309415349\n",
      "Iter 18: Loss 0.014432440736812508\n",
      "Iter 19: Loss 0.0143951703450399\n",
      "Iter 20: Loss 0.014352542435575625\n",
      "Iter 18: Loss 0.014432440736812508\n",
      "Iter 19: Loss 0.0143951703450399\n",
      "Iter 20: Loss 0.014352542435575625\n",
      "Iter 21: Loss 0.014288386422954871\n",
      "Iter 22: Loss 0.014277200261037983\n",
      "Iter 23: Loss 0.014262083285820936\n",
      "Iter 21: Loss 0.014288386422954871\n",
      "Iter 22: Loss 0.014277200261037983\n",
      "Iter 23: Loss 0.014262083285820936\n",
      "Iter 24: Loss 0.01424282325242094\n",
      "Iter 25: Loss 0.01417909363310732\n",
      "Iter 26: Loss 0.014158728593837716\n",
      "Iter 24: Loss 0.01424282325242094\n",
      "Iter 25: Loss 0.01417909363310732\n",
      "Iter 26: Loss 0.014158728593837716\n",
      "Iter 27: Loss 0.014125747832947386\n",
      "Iter 28: Loss 0.014090317214082578\n",
      "Iter 29: Loss 0.014055973517442654\n",
      "Iter 27: Loss 0.014125747832947386\n",
      "Iter 28: Loss 0.014090317214082578\n",
      "Iter 29: Loss 0.014055973517442654\n",
      "Iter 30: Loss 0.014054600112214535\n",
      "Iter 31: Loss 0.01403951454543306\n",
      "Iter 32: Loss 0.014000587120741427\n",
      "Iter 30: Loss 0.014054600112214535\n",
      "Iter 31: Loss 0.01403951454543306\n",
      "Iter 32: Loss 0.014000587120741427\n",
      "Iter 33: Loss 0.013988947915935707\n",
      "Iter 34: Loss 0.013934725534892129\n",
      "Iter 35: Loss 0.013900468449392718\n",
      "Iter 33: Loss 0.013988947915935707\n",
      "Iter 34: Loss 0.013934725534892129\n",
      "Iter 35: Loss 0.013900468449392718\n",
      "Iter 36: Loss 0.013876000326312707\n",
      "Iter 37: Loss 0.013872186580817857\n",
      "Iter 38: Loss 0.013852894187211515\n",
      "Iter 36: Loss 0.013876000326312707\n",
      "Iter 37: Loss 0.013872186580817857\n",
      "Iter 38: Loss 0.013852894187211515\n",
      "Iter 39: Loss 0.013785329883446\n",
      "Iter 40: Loss 0.013793111561301226\n",
      "Iter 41: Loss 0.013770573629352623\n",
      "Iter 39: Loss 0.013785329883446\n",
      "Iter 40: Loss 0.013793111561301226\n",
      "Iter 41: Loss 0.013770573629352623\n",
      "Iter 42: Loss 0.013712254826894064\n",
      "Iter 43: Loss 0.013725841354705141\n",
      "Iter 44: Loss 0.013672190987897253\n",
      "Iter 42: Loss 0.013712254826894064\n",
      "Iter 43: Loss 0.013725841354705141\n",
      "Iter 44: Loss 0.013672190987897253\n",
      "Iter 45: Loss 0.013661222781487806\n",
      "Iter 46: Loss 0.013654050236928486\n",
      "Iter 47: Loss 0.013614450862069806\n",
      "Iter 45: Loss 0.013661222781487806\n",
      "Iter 46: Loss 0.013654050236928486\n",
      "Iter 47: Loss 0.013614450862069806\n",
      "Iter 48: Loss 0.013589425001315728\n",
      "Iter 49: Loss 0.01357893077675216\n",
      "Iter 50: Loss 0.013536700707471774\n",
      "Iter 48: Loss 0.013589425001315728\n",
      "Iter 49: Loss 0.01357893077675216\n",
      "Iter 50: Loss 0.013536700707471774\n",
      "Iter 51: Loss 0.013529179814808858\n",
      "Iter 52: Loss 0.013515070764842386\n",
      "Iter 53: Loss 0.01343648685904558\n",
      "Iter 51: Loss 0.013529179814808858\n",
      "Iter 52: Loss 0.013515070764842386\n",
      "Iter 53: Loss 0.01343648685904558\n",
      "Iter 54: Loss 0.013443791699742604\n",
      "Iter 55: Loss 0.013446500439367846\n",
      "Iter 56: Loss 0.013430107139541717\n",
      "Iter 54: Loss 0.013443791699742604\n",
      "Iter 55: Loss 0.013446500439367846\n",
      "Iter 56: Loss 0.013430107139541717\n",
      "Iter 57: Loss 0.013393952223117242\n",
      "Iter 58: Loss 0.013359831240838635\n",
      "Iter 59: Loss 0.013369228073698794\n",
      "Iter 57: Loss 0.013393952223117242\n",
      "Iter 58: Loss 0.013359831240838635\n",
      "Iter 59: Loss 0.013369228073698794\n",
      "Iter 60: Loss 0.013355186599457336\n",
      "Iter 61: Loss 0.013312064720960911\n",
      "Iter 62: Loss 0.013248236117486706\n",
      "Iter 60: Loss 0.013355186599457336\n",
      "Iter 61: Loss 0.013312064720960911\n",
      "Iter 62: Loss 0.013248236117486706\n",
      "Iter 63: Loss 0.013279203883188213\n",
      "Iter 64: Loss 0.013224587469043847\n",
      "Iter 65: Loss 0.013193812912809634\n",
      "Iter 63: Loss 0.013279203883188213\n",
      "Iter 64: Loss 0.013224587469043847\n",
      "Iter 65: Loss 0.013193812912809634\n",
      "Iter 66: Loss 0.013195943927574538\n",
      "Iter 67: Loss 0.013164961885311409\n",
      "Iter 68: Loss 0.013115890487701355\n",
      "Iter 66: Loss 0.013195943927574538\n",
      "Iter 67: Loss 0.013164961885311409\n",
      "Iter 68: Loss 0.013115890487701355\n",
      "Iter 69: Loss 0.013151715139666955\n",
      "Iter 70: Loss 0.013099789381503107\n",
      "Iter 71: Loss 0.013074208638387288\n",
      "Iter 69: Loss 0.013151715139666955\n",
      "Iter 70: Loss 0.013099789381503107\n",
      "Iter 71: Loss 0.013074208638387288\n",
      "Iter 72: Loss 0.013039146354812348\n",
      "Iter 73: Loss 0.01300784356579809\n",
      "Iter 74: Loss 0.012970251475503583\n",
      "Iter 72: Loss 0.013039146354812348\n",
      "Iter 73: Loss 0.01300784356579809\n",
      "Iter 74: Loss 0.012970251475503583\n",
      "Iter 75: Loss 0.012991307500355733\n",
      "Iter 76: Loss 0.012979302815572469\n",
      "Iter 77: Loss 0.012946609489456147\n",
      "Iter 75: Loss 0.012991307500355733\n",
      "Iter 76: Loss 0.012979302815572469\n",
      "Iter 77: Loss 0.012946609489456147\n",
      "Iter 78: Loss 0.012940841758560516\n",
      "Iter 79: Loss 0.012881596883138021\n",
      "Iter 80: Loss 0.012885817034753735\n",
      "Iter 78: Loss 0.012940841758560516\n",
      "Iter 79: Loss 0.012881596883138021\n",
      "Iter 80: Loss 0.012885817034753735\n",
      "Iter 81: Loss 0.012867895191063187\n",
      "Iter 82: Loss 0.012833757076910632\n",
      "Iter 83: Loss 0.012823310440885806\n",
      "Iter 81: Loss 0.012867895191063187\n",
      "Iter 82: Loss 0.012833757076910632\n",
      "Iter 83: Loss 0.012823310440885806\n",
      "Iter 84: Loss 0.012784202179746951\n",
      "Iter 85: Loss 0.012730107335987206\n",
      "Iter 86: Loss 0.012832536906777266\n",
      "Iter 84: Loss 0.012784202179746951\n",
      "Iter 85: Loss 0.012730107335987206\n",
      "Iter 86: Loss 0.012832536906777266\n",
      "Iter 87: Loss 0.012782889687848424\n",
      "Iter 88: Loss 0.01274887720743815\n",
      "Iter 89: Loss 0.01266917306743934\n",
      "Iter 87: Loss 0.012782889687848424\n",
      "Iter 88: Loss 0.01274887720743815\n",
      "Iter 89: Loss 0.01266917306743934\n",
      "Iter 90: Loss 0.012682737704522596\n",
      "Iter 91: Loss 0.012638842988156988\n",
      "Iter 92: Loss 0.012584335075880953\n",
      "Iter 90: Loss 0.012682737704522596\n",
      "Iter 91: Loss 0.012638842988156988\n",
      "Iter 92: Loss 0.012584335075880953\n",
      "Iter 93: Loss 0.012595875296526088\n",
      "Iter 94: Loss 0.012566959548615171\n",
      "Iter 95: Loss 0.012588582828849138\n",
      "Iter 93: Loss 0.012595875296526088\n",
      "Iter 94: Loss 0.012566959548615171\n",
      "Iter 95: Loss 0.012588582828849138\n",
      "Iter 96: Loss 0.012543325176733934\n",
      "Iter 97: Loss 0.012506393615357176\n",
      "Iter 98: Loss 0.01247458924314457\n",
      "Iter 96: Loss 0.012543325176733934\n",
      "Iter 97: Loss 0.012506393615357176\n",
      "Iter 98: Loss 0.01247458924314457\n",
      "Iter 99: Loss 0.01248351970832505\n",
      "Iter 100: Loss 0.01248261838140126\n",
      "Iter 101: Loss 0.012454682005617671\n",
      "Iter 99: Loss 0.01248351970832505\n",
      "Iter 100: Loss 0.01248261838140126\n",
      "Iter 101: Loss 0.012454682005617671\n",
      "Iter 102: Loss 0.01245615153969405\n",
      "Iter 103: Loss 0.012374543858145525\n",
      "Iter 104: Loss 0.012381749714681964\n",
      "Iter 102: Loss 0.01245615153969405\n",
      "Iter 103: Loss 0.012374543858145525\n",
      "Iter 104: Loss 0.012381749714681964\n",
      "Iter 105: Loss 0.012355256223393058\n",
      "Iter 106: Loss 0.012322092722513956\n",
      "Iter 107: Loss 0.012345858438762123\n",
      "Iter 105: Loss 0.012355256223393058\n",
      "Iter 106: Loss 0.012322092722513956\n",
      "Iter 107: Loss 0.012345858438762123\n",
      "Iter 108: Loss 0.012325538132718937\n",
      "Iter 109: Loss 0.012336388319552302\n",
      "Iter 110: Loss 0.012304168975281858\n",
      "Iter 108: Loss 0.012325538132718937\n",
      "Iter 109: Loss 0.012336388319552302\n",
      "Iter 110: Loss 0.012304168975281858\n",
      "Iter 111: Loss 0.012292171904664792\n",
      "Iter 112: Loss 0.01226368064651946\n",
      "Iter 113: Loss 0.012194978976677992\n",
      "Iter 111: Loss 0.012292171904664792\n",
      "Iter 112: Loss 0.01226368064651946\n",
      "Iter 113: Loss 0.012194978976677992\n",
      "Iter 114: Loss 0.01225689166558241\n",
      "Iter 115: Loss 0.012254183877727943\n",
      "Iter 116: Loss 0.012202891998900149\n",
      "Iter 114: Loss 0.01225689166558241\n",
      "Iter 115: Loss 0.012254183877727943\n",
      "Iter 116: Loss 0.012202891998900149\n",
      "Iter 117: Loss 0.012171956593404988\n",
      "Iter 118: Loss 0.012157927492183602\n",
      "Iter 119: Loss 0.01213235436323398\n",
      "Iter 117: Loss 0.012171956593404988\n",
      "Iter 118: Loss 0.012157927492183602\n",
      "Iter 119: Loss 0.01213235436323398\n",
      "Iter 120: Loss 0.012088227414799308\n",
      "Iter 121: Loss 0.012152876444681437\n",
      "Iter 122: Loss 0.012093484044788841\n",
      "Iter 120: Loss 0.012088227414799308\n",
      "Iter 121: Loss 0.012152876444681437\n",
      "Iter 122: Loss 0.012093484044788841\n",
      "Iter 123: Loss 0.012045309215248701\n",
      "Iter 124: Loss 0.012064647293852237\n",
      "Iter 125: Loss 0.012012486448306998\n",
      "Iter 123: Loss 0.012045309215248701\n",
      "Iter 124: Loss 0.012064647293852237\n",
      "Iter 125: Loss 0.012012486448306998\n",
      "Iter 126: Loss 0.011986193780651588\n",
      "Iter 127: Loss 0.011962016899428682\n",
      "Iter 128: Loss 0.01197596414836343\n",
      "Iter 126: Loss 0.011986193780651588\n",
      "Iter 127: Loss 0.011962016899428682\n",
      "Iter 128: Loss 0.01197596414836343\n",
      "Iter 129: Loss 0.011943209908917516\n",
      "Iter 130: Loss 0.011919491781208092\n",
      "Iter 131: Loss 0.011897117553832765\n",
      "Iter 129: Loss 0.011943209908917516\n",
      "Iter 130: Loss 0.011919491781208092\n",
      "Iter 131: Loss 0.011897117553832765\n",
      "Iter 132: Loss 0.011889880288860755\n",
      "Iter 133: Loss 0.011925892439669001\n",
      "Iter 134: Loss 0.01182432517320096\n",
      "Iter 132: Loss 0.011889880288860755\n",
      "Iter 133: Loss 0.011925892439669001\n",
      "Iter 134: Loss 0.01182432517320096\n",
      "Iter 135: Loss 0.011807009607493996\n",
      "Iter 136: Loss 0.01185544998108032\n",
      "Iter 137: Loss 0.011807225659459889\n",
      "Iter 135: Loss 0.011807009607493996\n",
      "Iter 136: Loss 0.01185544998108032\n",
      "Iter 137: Loss 0.011807225659459889\n",
      "Iter 138: Loss 0.011798099129499789\n",
      "Iter 139: Loss 0.011773516793926795\n",
      "Iter 140: Loss 0.01176586074981385\n",
      "Iter 138: Loss 0.011798099129499789\n",
      "Iter 139: Loss 0.011773516793926795\n",
      "Iter 140: Loss 0.01176586074981385\n",
      "Iter 141: Loss 0.011689104243904768\n",
      "Iter 142: Loss 0.011747335483452042\n",
      "Iter 143: Loss 0.011697934773153888\n",
      "Iter 141: Loss 0.011689104243904768\n",
      "Iter 142: Loss 0.011747335483452042\n",
      "Iter 143: Loss 0.011697934773153888\n",
      "Iter 144: Loss 0.011698726646438569\n",
      "Iter 145: Loss 0.011656762121204369\n",
      "Iter 146: Loss 0.011616507928052586\n",
      "Iter 144: Loss 0.011698726646438569\n",
      "Iter 145: Loss 0.011656762121204369\n",
      "Iter 146: Loss 0.011616507928052586\n",
      "Iter 147: Loss 0.01166474842977619\n",
      "Iter 148: Loss 0.011605335090926545\n",
      "Iter 149: Loss 0.011628533551792898\n",
      "Iter 147: Loss 0.01166474842977619\n",
      "Iter 148: Loss 0.011605335090926545\n",
      "Iter 149: Loss 0.011628533551792898\n",
      "Iter 150: Loss 0.011600627632674105\n",
      "Iter 151: Loss 0.011565819471896052\n",
      "Iter 152: Loss 0.011552637446664288\n",
      "Iter 150: Loss 0.011600627632674105\n",
      "Iter 151: Loss 0.011565819471896052\n",
      "Iter 152: Loss 0.011552637446664288\n",
      "Iter 153: Loss 0.011582502110037737\n",
      "Iter 154: Loss 0.011518732516351575\n",
      "Iter 155: Loss 0.011447693297487058\n",
      "Iter 153: Loss 0.011582502110037737\n",
      "Iter 154: Loss 0.011518732516351575\n",
      "Iter 155: Loss 0.011447693297487058\n",
      "Iter 156: Loss 0.011502304001006776\n",
      "Iter 157: Loss 0.011494594657730438\n",
      "Iter 158: Loss 0.011500687894231068\n",
      "Iter 156: Loss 0.011502304001006776\n",
      "Iter 157: Loss 0.011494594657730438\n",
      "Iter 158: Loss 0.011500687894231068\n",
      "Iter 159: Loss 0.011440769165099975\n",
      "Iter 160: Loss 0.011393753592363613\n",
      "Iter 161: Loss 0.011416170649423808\n",
      "Iter 159: Loss 0.011440769165099975\n",
      "Iter 160: Loss 0.011393753592363613\n",
      "Iter 161: Loss 0.011416170649423808\n",
      "Iter 162: Loss 0.011443830059911914\n",
      "Iter 163: Loss 0.011464769016958759\n",
      "Iter 164: Loss 0.011387255853283667\n",
      "Iter 162: Loss 0.011443830059911914\n",
      "Iter 163: Loss 0.011464769016958759\n",
      "Iter 164: Loss 0.011387255853283667\n",
      "Iter 165: Loss 0.011370727402007508\n",
      "Iter 166: Loss 0.011372556705436783\n",
      "Iter 167: Loss 0.011272567474913455\n",
      "Iter 165: Loss 0.011370727402007508\n",
      "Iter 166: Loss 0.011372556705436783\n",
      "Iter 167: Loss 0.011272567474913455\n",
      "Iter 168: Loss 0.01133914384061467\n",
      "Iter 169: Loss 0.011327767324542808\n",
      "Iter 170: Loss 0.011305883259116533\n",
      "Iter 168: Loss 0.01133914384061467\n",
      "Iter 169: Loss 0.011327767324542808\n",
      "Iter 170: Loss 0.011305883259116533\n",
      "Iter 171: Loss 0.01128964795323903\n",
      "Iter 172: Loss 0.01127329367601467\n",
      "Iter 173: Loss 0.011241910938255325\n",
      "Iter 171: Loss 0.01128964795323903\n",
      "Iter 172: Loss 0.01127329367601467\n",
      "Iter 173: Loss 0.011241910938255325\n",
      "Iter 174: Loss 0.011157380368657217\n",
      "Iter 175: Loss 0.011211820705208236\n",
      "Iter 176: Loss 0.011128510305743494\n",
      "Iter 174: Loss 0.011157380368657217\n",
      "Iter 175: Loss 0.011211820705208236\n",
      "Iter 176: Loss 0.011128510305743494\n",
      "Iter 177: Loss 0.011143320810771035\n",
      "Iter 178: Loss 0.011094938495201979\n",
      "Iter 179: Loss 0.011146957526901763\n",
      "Iter 177: Loss 0.011143320810771035\n",
      "Iter 178: Loss 0.011094938495201979\n",
      "Iter 179: Loss 0.011146957526901763\n",
      "Iter 180: Loss 0.011153317259219354\n",
      "Iter 181: Loss 0.011113886347787823\n",
      "Iter 182: Loss 0.011158598635249033\n",
      "Iter 180: Loss 0.011153317259219354\n",
      "Iter 181: Loss 0.011113886347787823\n",
      "Iter 182: Loss 0.011158598635249033\n",
      "Iter 183: Loss 0.0110738948433699\n",
      "Iter 184: Loss 0.011077061384737848\n",
      "Iter 185: Loss 0.0111011002591984\n",
      "Iter 183: Loss 0.0110738948433699\n",
      "Iter 184: Loss 0.011077061384737848\n",
      "Iter 185: Loss 0.0111011002591984\n",
      "Iter 186: Loss 0.01100085976119051\n",
      "Iter 187: Loss 0.01103860889366287\n",
      "Iter 188: Loss 0.011008532937177403\n",
      "Iter 186: Loss 0.01100085976119051\n",
      "Iter 187: Loss 0.01103860889366287\n",
      "Iter 188: Loss 0.011008532937177403\n",
      "Iter 189: Loss 0.010959408240403958\n",
      "Iter 190: Loss 0.010989866808740917\n",
      "Iter 191: Loss 0.010920493188732398\n",
      "Iter 189: Loss 0.010959408240403958\n",
      "Iter 190: Loss 0.010989866808740917\n",
      "Iter 191: Loss 0.010920493188732398\n",
      "Iter 192: Loss 0.01090588731442145\n",
      "Iter 193: Loss 0.010874157180329283\n",
      "Iter 194: Loss 0.010880232333185191\n",
      "Iter 192: Loss 0.01090588731442145\n",
      "Iter 193: Loss 0.010874157180329283\n",
      "Iter 194: Loss 0.010880232333185191\n",
      "Iter 195: Loss 0.010906635406250488\n",
      "Iter 196: Loss 0.010912492603598955\n",
      "Iter 197: Loss 0.010804591302624244\n",
      "Iter 195: Loss 0.010906635406250488\n",
      "Iter 196: Loss 0.010912492603598955\n",
      "Iter 197: Loss 0.010804591302624244\n",
      "Iter 198: Loss 0.01076674128244975\n",
      "Iter 199: Loss 0.01084137343598935\n",
      "Iter 200: Loss 0.010844343912577676\n",
      "Iter 198: Loss 0.01076674128244975\n",
      "Iter 199: Loss 0.01084137343598935\n",
      "Iter 200: Loss 0.010844343912577676\n",
      "Iter 201: Loss 0.010740649438427832\n",
      "Iter 202: Loss 0.010827469968510245\n",
      "Iter 203: Loss 0.01077012387578359\n",
      "Iter 201: Loss 0.010740649438427832\n",
      "Iter 202: Loss 0.010827469968510245\n",
      "Iter 203: Loss 0.01077012387578359\n",
      "Iter 204: Loss 0.01076543640471742\n",
      "Iter 205: Loss 0.010767652127081287\n",
      "Iter 206: Loss 0.010662860261228032\n",
      "Iter 204: Loss 0.01076543640471742\n",
      "Iter 205: Loss 0.010767652127081287\n",
      "Iter 206: Loss 0.010662860261228032\n",
      "Iter 207: Loss 0.010775571811698867\n",
      "Iter 208: Loss 0.01074281471694063\n",
      "Iter 209: Loss 0.010682969274159201\n",
      "Iter 207: Loss 0.010775571811698867\n",
      "Iter 208: Loss 0.01074281471694063\n",
      "Iter 209: Loss 0.010682969274159201\n",
      "Iter 210: Loss 0.010721255205348581\n",
      "Iter 211: Loss 0.010695381316834104\n",
      "Iter 212: Loss 0.010616333898669946\n",
      "Iter 210: Loss 0.010721255205348581\n",
      "Iter 211: Loss 0.010695381316834104\n",
      "Iter 212: Loss 0.010616333898669946\n",
      "Iter 213: Loss 0.01064738922728274\n",
      "Iter 214: Loss 0.010636139296723935\n",
      "Iter 215: Loss 0.010657373302710984\n",
      "Iter 213: Loss 0.01064738922728274\n",
      "Iter 214: Loss 0.010636139296723935\n",
      "Iter 215: Loss 0.010657373302710984\n",
      "Iter 216: Loss 0.010573906812839165\n",
      "Iter 217: Loss 0.010491410177386926\n",
      "Iter 218: Loss 0.010531368369827727\n",
      "Iter 216: Loss 0.010573906812839165\n",
      "Iter 217: Loss 0.010491410177386926\n",
      "Iter 218: Loss 0.010531368369827727\n",
      "Iter 219: Loss 0.010557510657700712\n",
      "Iter 220: Loss 0.010554224193215132\n",
      "Iter 221: Loss 0.010472101603677411\n",
      "Iter 219: Loss 0.010557510657700712\n",
      "Iter 220: Loss 0.010554224193215132\n",
      "Iter 221: Loss 0.010472101603677411\n",
      "Iter 222: Loss 0.010495209646320152\n",
      "Iter 223: Loss 0.010469942035789262\n",
      "Iter 224: Loss 0.010487592624808976\n",
      "Iter 222: Loss 0.010495209646320152\n",
      "Iter 223: Loss 0.010469942035789262\n",
      "Iter 224: Loss 0.010487592624808976\n",
      "Iter 225: Loss 0.010502519245871051\n",
      "Iter 226: Loss 0.010412786297217577\n",
      "Iter 227: Loss 0.010334644013060305\n",
      "Iter 225: Loss 0.010502519245871051\n",
      "Iter 226: Loss 0.010412786297217577\n",
      "Iter 227: Loss 0.010334644013060305\n",
      "Iter 228: Loss 0.010407718117841466\n",
      "Iter 229: Loss 0.010338033268789569\n",
      "Iter 230: Loss 0.010428019388469155\n",
      "Iter 228: Loss 0.010407718117841466\n",
      "Iter 229: Loss 0.010338033268789569\n",
      "Iter 230: Loss 0.010428019388469155\n",
      "Iter 231: Loss 0.010359463339556238\n",
      "Iter 232: Loss 0.010372263704707285\n",
      "Iter 233: Loss 0.010324481004726388\n",
      "Iter 231: Loss 0.010359463339556238\n",
      "Iter 232: Loss 0.010372263704707285\n",
      "Iter 233: Loss 0.010324481004726388\n",
      "Iter 234: Loss 0.010259902405881597\n",
      "Iter 235: Loss 0.010180897817402305\n",
      "Iter 236: Loss 0.010276371847369714\n",
      "Iter 234: Loss 0.010259902405881597\n",
      "Iter 235: Loss 0.010180897817402305\n",
      "Iter 236: Loss 0.010276371847369714\n",
      "Iter 237: Loss 0.010269838892771098\n",
      "Iter 238: Loss 0.010304450036998756\n",
      "Iter 239: Loss 0.010321400122728176\n",
      "Iter 237: Loss 0.010269838892771098\n",
      "Iter 238: Loss 0.010304450036998756\n",
      "Iter 239: Loss 0.010321400122728176\n",
      "Iter 240: Loss 0.010246631865967772\n",
      "Iter 241: Loss 0.010221370917832304\n",
      "Iter 242: Loss 0.010121115191492017\n",
      "Iter 240: Loss 0.010246631865967772\n",
      "Iter 241: Loss 0.010221370917832304\n",
      "Iter 242: Loss 0.010121115191492017\n",
      "Iter 243: Loss 0.010121703385830877\n",
      "Iter 244: Loss 0.010181213805299558\n",
      "Iter 245: Loss 0.010139850799195067\n",
      "Iter 243: Loss 0.010121703385830877\n",
      "Iter 244: Loss 0.010181213805299558\n",
      "Iter 245: Loss 0.010139850799195067\n",
      "Iter 246: Loss 0.010083624940670418\n",
      "Iter 247: Loss 0.010211861776020712\n",
      "Iter 248: Loss 0.010050634662072339\n",
      "Iter 246: Loss 0.010083624940670418\n",
      "Iter 247: Loss 0.010211861776020712\n",
      "Iter 248: Loss 0.010050634662072339\n",
      "Iter 249: Loss 0.01012453966273995\n",
      "Iter 250: Loss 0.010045753981538875\n",
      "Iter 251: Loss 0.010072481608438396\n",
      "Iter 249: Loss 0.01012453966273995\n",
      "Iter 250: Loss 0.010045753981538875\n",
      "Iter 251: Loss 0.010072481608438396\n",
      "Iter 252: Loss 0.01014264805350237\n",
      "Iter 253: Loss 0.009930781975477755\n",
      "Iter 254: Loss 0.010063213264632843\n",
      "Iter 252: Loss 0.01014264805350237\n",
      "Iter 253: Loss 0.009930781975477755\n",
      "Iter 254: Loss 0.010063213264632843\n",
      "Iter 255: Loss 0.009946340572334336\n",
      "Iter 256: Loss 0.010019490818777484\n",
      "Iter 257: Loss 0.009951085149646995\n",
      "Iter 255: Loss 0.009946340572334336\n",
      "Iter 256: Loss 0.010019490818777484\n",
      "Iter 257: Loss 0.009951085149646995\n",
      "Iter 258: Loss 0.010013865853498082\n",
      "Iter 259: Loss 0.00991801872938693\n",
      "Iter 260: Loss 0.009877731224258026\n",
      "Iter 258: Loss 0.010013865853498082\n",
      "Iter 259: Loss 0.00991801872938693\n",
      "Iter 260: Loss 0.009877731224258026\n",
      "Iter 261: Loss 0.00988228830272804\n",
      "Iter 262: Loss 0.009885905031672495\n",
      "Iter 263: Loss 0.009907977547712193\n",
      "Iter 261: Loss 0.00988228830272804\n",
      "Iter 262: Loss 0.009885905031672495\n",
      "Iter 263: Loss 0.009907977547712193\n",
      "Iter 264: Loss 0.009825665555790276\n",
      "Iter 265: Loss 0.009863084423803759\n",
      "Iter 266: Loss 0.009899664781764596\n",
      "Iter 264: Loss 0.009825665555790276\n",
      "Iter 265: Loss 0.009863084423803759\n",
      "Iter 266: Loss 0.009899664781764596\n",
      "Iter 267: Loss 0.00981175828122807\n",
      "Iter 268: Loss 0.00986791561225693\n",
      "Iter 269: Loss 0.009800399848800934\n",
      "Iter 267: Loss 0.00981175828122807\n",
      "Iter 268: Loss 0.00986791561225693\n",
      "Iter 269: Loss 0.009800399848800934\n",
      "Iter 270: Loss 0.009759721165883565\n",
      "Iter 271: Loss 0.00979102966552247\n",
      "Iter 272: Loss 0.009695221564013087\n",
      "Iter 270: Loss 0.009759721165883565\n",
      "Iter 271: Loss 0.00979102966552247\n",
      "Iter 272: Loss 0.009695221564013087\n",
      "Iter 273: Loss 0.009681478945794934\n",
      "Iter 274: Loss 0.009736688313132036\n",
      "Iter 275: Loss 0.00975308637181204\n",
      "Iter 273: Loss 0.009681478945794934\n",
      "Iter 274: Loss 0.009736688313132036\n",
      "Iter 275: Loss 0.00975308637181204\n",
      "Iter 276: Loss 0.009679889488600922\n",
      "Iter 277: Loss 0.00972363096987178\n",
      "Iter 278: Loss 0.009679453577586039\n",
      "Iter 276: Loss 0.009679889488600922\n",
      "Iter 277: Loss 0.00972363096987178\n",
      "Iter 278: Loss 0.009679453577586039\n",
      "Iter 279: Loss 0.009622084642360786\n",
      "Iter 280: Loss 0.009587400211783465\n",
      "Iter 281: Loss 0.009554301431317053\n",
      "Iter 279: Loss 0.009622084642360786\n",
      "Iter 280: Loss 0.009587400211783465\n",
      "Iter 281: Loss 0.009554301431317053\n",
      "Iter 282: Loss 0.009665648142496744\n",
      "Iter 283: Loss 0.009618913342138964\n",
      "Iter 284: Loss 0.009559976840447524\n",
      "Iter 282: Loss 0.009665648142496744\n",
      "Iter 283: Loss 0.009618913342138964\n",
      "Iter 284: Loss 0.009559976840447524\n",
      "Iter 285: Loss 0.00962593455514508\n",
      "Iter 286: Loss 0.009505695449615905\n",
      "Iter 287: Loss 0.00945961832286355\n",
      "Iter 285: Loss 0.00962593455514508\n",
      "Iter 286: Loss 0.009505695449615905\n",
      "Iter 287: Loss 0.00945961832286355\n",
      "Iter 288: Loss 0.009465581166768027\n",
      "Iter 289: Loss 0.009545953449850786\n",
      "Iter 290: Loss 0.00946456467558048\n",
      "Iter 288: Loss 0.009465581166768027\n",
      "Iter 289: Loss 0.009545953449850786\n",
      "Iter 290: Loss 0.00946456467558048\n",
      "Iter 291: Loss 0.009511544081027399\n",
      "Iter 292: Loss 0.009518813706205753\n",
      "Iter 293: Loss 0.009388411592342659\n",
      "Iter 291: Loss 0.009511544081027399\n",
      "Iter 292: Loss 0.009518813706205753\n",
      "Iter 293: Loss 0.009388411592342659\n",
      "Iter 294: Loss 0.009402874701037378\n",
      "Iter 295: Loss 0.009313018022183173\n",
      "Iter 296: Loss 0.009383121650376006\n",
      "Iter 294: Loss 0.009402874701037378\n",
      "Iter 295: Loss 0.009313018022183173\n",
      "Iter 296: Loss 0.009383121650376006\n",
      "Iter 297: Loss 0.009287409677714882\n",
      "Iter 298: Loss 0.009328602316850674\n",
      "Iter 299: Loss 0.009397346816377012\n",
      "Iter 297: Loss 0.009287409677714882\n",
      "Iter 298: Loss 0.009328602316850674\n",
      "Iter 299: Loss 0.009397346816377012\n",
      "Iter 300: Loss 0.00929296326018617\n",
      "Iter 301: Loss 0.009395601268775925\n",
      "Iter 302: Loss 0.009359127509141873\n",
      "Iter 300: Loss 0.00929296326018617\n",
      "Iter 301: Loss 0.009395601268775925\n",
      "Iter 302: Loss 0.009359127509141873\n",
      "Iter 303: Loss 0.009308086897798641\n",
      "Iter 304: Loss 0.009313800377760105\n",
      "Iter 305: Loss 0.00926194980948747\n",
      "Iter 303: Loss 0.009308086897798641\n",
      "Iter 304: Loss 0.009313800377760105\n",
      "Iter 305: Loss 0.00926194980948747\n",
      "Iter 306: Loss 0.0092504714539427\n",
      "Iter 307: Loss 0.009262056407814254\n",
      "Iter 308: Loss 0.009258405415121905\n",
      "Iter 306: Loss 0.0092504714539427\n",
      "Iter 307: Loss 0.009262056407814254\n",
      "Iter 308: Loss 0.009258405415121905\n",
      "Iter 309: Loss 0.009195502884611637\n",
      "Iter 310: Loss 0.009214816217175025\n",
      "Iter 311: Loss 0.009195864557506081\n",
      "Iter 309: Loss 0.009195502884611637\n",
      "Iter 310: Loss 0.009214816217175025\n",
      "Iter 311: Loss 0.009195864557506081\n",
      "Iter 312: Loss 0.009266279414742293\n",
      "Iter 313: Loss 0.009156590688252402\n",
      "Iter 314: Loss 0.009075068666073615\n",
      "Iter 312: Loss 0.009266279414742293\n",
      "Iter 313: Loss 0.009156590688252402\n",
      "Iter 314: Loss 0.009075068666073615\n",
      "Iter 315: Loss 0.009083650783150496\n",
      "Iter 316: Loss 0.009024038524208907\n",
      "Iter 317: Loss 0.00900109228259789\n",
      "Iter 315: Loss 0.009083650783150496\n",
      "Iter 316: Loss 0.009024038524208907\n",
      "Iter 317: Loss 0.00900109228259789\n",
      "Iter 318: Loss 0.009029998512801058\n",
      "Iter 319: Loss 0.00912098113648192\n",
      "Iter 320: Loss 0.009105202680576347\n",
      "Iter 318: Loss 0.009029998512801058\n",
      "Iter 319: Loss 0.00912098113648192\n",
      "Iter 320: Loss 0.009105202680576347\n",
      "Iter 321: Loss 0.008927050226938701\n",
      "Iter 322: Loss 0.009121034435645312\n",
      "Iter 323: Loss 0.008854850799499634\n",
      "Iter 321: Loss 0.008927050226938701\n",
      "Iter 322: Loss 0.009121034435645312\n",
      "Iter 323: Loss 0.008854850799499634\n",
      "Iter 324: Loss 0.008967420536125016\n",
      "Iter 325: Loss 0.008892292510011715\n",
      "Iter 326: Loss 0.008975077532008737\n",
      "Iter 324: Loss 0.008967420536125016\n",
      "Iter 325: Loss 0.008892292510011715\n",
      "Iter 326: Loss 0.008975077532008737\n",
      "Iter 327: Loss 0.008942724939829814\n",
      "Iter 328: Loss 0.008836218934811043\n",
      "Iter 329: Loss 0.008891374051213978\n",
      "Iter 327: Loss 0.008942724939829814\n",
      "Iter 328: Loss 0.008836218934811043\n",
      "Iter 329: Loss 0.008891374051213978\n",
      "Iter 330: Loss 0.008810073791625734\n",
      "Iter 331: Loss 0.008910017337151868\n",
      "Iter 332: Loss 0.00892977609843789\n",
      "Iter 330: Loss 0.008810073791625734\n",
      "Iter 331: Loss 0.008910017337151868\n",
      "Iter 332: Loss 0.00892977609843789\n",
      "Iter 333: Loss 0.008882810017781819\n",
      "Iter 334: Loss 0.008859619171081665\n",
      "Iter 335: Loss 0.008832902965431442\n",
      "Iter 333: Loss 0.008882810017781819\n",
      "Iter 334: Loss 0.008859619171081665\n",
      "Iter 335: Loss 0.008832902965431442\n",
      "Iter 336: Loss 0.008846321981586145\n",
      "Iter 337: Loss 0.008817014104115986\n",
      "Iter 338: Loss 0.008783850603236885\n",
      "Iter 336: Loss 0.008846321981586145\n",
      "Iter 337: Loss 0.008817014104115986\n",
      "Iter 338: Loss 0.008783850603236885\n",
      "Iter 339: Loss 0.008800761666364537\n",
      "Iter 340: Loss 0.008775313219386422\n",
      "Iter 341: Loss 0.008729950872485985\n",
      "Iter 339: Loss 0.008800761666364537\n",
      "Iter 340: Loss 0.008775313219386422\n",
      "Iter 341: Loss 0.008729950872485985\n",
      "Iter 342: Loss 0.008745969174626821\n",
      "Iter 343: Loss 0.00871960797947562\n",
      "Iter 344: Loss 0.008668714892602491\n",
      "Iter 342: Loss 0.008745969174626821\n",
      "Iter 343: Loss 0.00871960797947562\n",
      "Iter 344: Loss 0.008668714892602491\n",
      "Iter 345: Loss 0.00861212831295417\n",
      "Iter 346: Loss 0.008749373658688482\n",
      "Iter 347: Loss 0.00864204817903256\n",
      "Iter 345: Loss 0.00861212831295417\n",
      "Iter 346: Loss 0.008749373658688482\n",
      "Iter 347: Loss 0.00864204817903256\n",
      "Iter 348: Loss 0.008598275289326133\n",
      "Iter 349: Loss 0.008674324629549495\n",
      "Iter 350: Loss 0.00857921607955963\n",
      "Iter 348: Loss 0.008598275289326133\n",
      "Iter 349: Loss 0.008674324629549495\n",
      "Iter 350: Loss 0.00857921607955963\n",
      "Iter 351: Loss 0.00857773417246318\n",
      "Iter 352: Loss 0.008572226274989084\n",
      "Iter 353: Loss 0.008566331958580397\n",
      "Iter 351: Loss 0.00857773417246318\n",
      "Iter 352: Loss 0.008572226274989084\n",
      "Iter 353: Loss 0.008566331958580397\n",
      "Iter 354: Loss 0.008545743253178702\n",
      "Iter 355: Loss 0.008423770973068512\n",
      "Iter 356: Loss 0.00857526718261475\n",
      "Iter 354: Loss 0.008545743253178702\n",
      "Iter 355: Loss 0.008423770973068512\n",
      "Iter 356: Loss 0.00857526718261475\n",
      "Iter 357: Loss 0.008511914464527976\n",
      "Iter 358: Loss 0.008394835237971323\n",
      "Iter 359: Loss 0.008537228711826834\n",
      "Iter 357: Loss 0.008511914464527976\n",
      "Iter 358: Loss 0.008394835237971323\n",
      "Iter 359: Loss 0.008537228711826834\n",
      "Iter 360: Loss 0.008486059611428997\n",
      "Iter 361: Loss 0.008411949980044794\n",
      "Iter 362: Loss 0.008557693687027799\n",
      "Iter 360: Loss 0.008486059611428997\n",
      "Iter 361: Loss 0.008411949980044794\n",
      "Iter 362: Loss 0.008557693687027799\n",
      "Iter 363: Loss 0.008408742512533526\n",
      "Iter 364: Loss 0.00838510052648609\n",
      "Iter 365: Loss 0.008405175275669365\n",
      "Iter 363: Loss 0.008408742512533526\n",
      "Iter 364: Loss 0.00838510052648609\n",
      "Iter 365: Loss 0.008405175275669365\n",
      "Iter 366: Loss 0.008408698731077883\n",
      "Iter 367: Loss 0.00835989002220169\n",
      "Iter 368: Loss 0.00836535984884479\n",
      "Iter 366: Loss 0.008408698731077883\n",
      "Iter 367: Loss 0.00835989002220169\n",
      "Iter 368: Loss 0.00836535984884479\n",
      "Iter 369: Loss 0.00834900937870353\n",
      "Iter 370: Loss 0.008348082353968821\n",
      "Iter 371: Loss 0.008357695238794871\n",
      "Iter 369: Loss 0.00834900937870353\n",
      "Iter 370: Loss 0.008348082353968821\n",
      "Iter 371: Loss 0.008357695238794871\n",
      "Iter 372: Loss 0.008344849188646633\n",
      "Iter 373: Loss 0.008289862535670846\n",
      "Iter 374: Loss 0.008322094252961362\n",
      "Iter 372: Loss 0.008344849188646633\n",
      "Iter 373: Loss 0.008289862535670846\n",
      "Iter 374: Loss 0.008322094252961362\n",
      "Iter 375: Loss 0.008137157577240539\n",
      "Iter 376: Loss 0.008221608198093559\n",
      "Iter 377: Loss 0.008253232685629717\n",
      "Iter 375: Loss 0.008137157577240539\n",
      "Iter 376: Loss 0.008221608198093559\n",
      "Iter 377: Loss 0.008253232685629717\n",
      "Iter 378: Loss 0.00816511965083505\n",
      "Iter 379: Loss 0.008158014682000744\n",
      "Iter 380: Loss 0.008256679047605473\n",
      "Iter 378: Loss 0.00816511965083505\n",
      "Iter 379: Loss 0.008158014682000744\n",
      "Iter 380: Loss 0.008256679047605473\n",
      "Iter 381: Loss 0.008121155455202875\n",
      "Iter 382: Loss 0.008171696386889308\n",
      "Iter 383: Loss 0.008146763799671166\n",
      "Iter 381: Loss 0.008121155455202875\n",
      "Iter 382: Loss 0.008171696386889308\n",
      "Iter 383: Loss 0.008146763799671166\n",
      "Iter 384: Loss 0.008104063555628002\n",
      "Iter 385: Loss 0.008066661819488464\n",
      "Iter 386: Loss 0.008007732931963222\n",
      "Iter 384: Loss 0.008104063555628002\n",
      "Iter 385: Loss 0.008066661819488464\n",
      "Iter 386: Loss 0.008007732931963222\n",
      "Iter 387: Loss 0.008035934851792995\n",
      "Iter 388: Loss 0.008015856295526622\n",
      "Iter 389: Loss 0.008113829675548804\n",
      "Iter 387: Loss 0.008035934851792995\n",
      "Iter 388: Loss 0.008015856295526622\n",
      "Iter 389: Loss 0.008113829675548804\n",
      "Iter 390: Loss 0.007960403274871157\n",
      "Iter 391: Loss 0.007974439990258741\n",
      "Iter 392: Loss 0.00801948159040805\n",
      "Iter 390: Loss 0.007960403274871157\n",
      "Iter 391: Loss 0.007974439990258741\n",
      "Iter 392: Loss 0.00801948159040805\n",
      "Iter 393: Loss 0.007964504455140013\n",
      "Iter 394: Loss 0.007971774556203754\n",
      "Iter 395: Loss 0.00801609899707421\n",
      "Iter 393: Loss 0.007964504455140013\n",
      "Iter 394: Loss 0.007971774556203754\n",
      "Iter 395: Loss 0.00801609899707421\n",
      "Iter 396: Loss 0.007920572143828797\n",
      "Iter 397: Loss 0.00788226574956776\n",
      "Iter 398: Loss 0.007884550475312802\n",
      "Iter 396: Loss 0.007920572143828797\n",
      "Iter 397: Loss 0.00788226574956776\n",
      "Iter 398: Loss 0.007884550475312802\n",
      "Iter 399: Loss 0.0078194764797797\n",
      "Iter 400: Loss 0.008023745523479408\n",
      "Iter 401: Loss 0.007917409885429336\n",
      "Iter 399: Loss 0.0078194764797797\n",
      "Iter 400: Loss 0.008023745523479408\n",
      "Iter 401: Loss 0.007917409885429336\n",
      "Iter 402: Loss 0.007967191304037433\n",
      "Iter 403: Loss 0.007891633553419285\n",
      "Iter 404: Loss 0.007867943502947719\n",
      "Iter 402: Loss 0.007967191304037433\n",
      "Iter 403: Loss 0.007891633553419285\n",
      "Iter 404: Loss 0.007867943502947719\n",
      "Iter 405: Loss 0.007708744612520565\n",
      "Iter 406: Loss 0.007769556102638473\n",
      "Iter 407: Loss 0.007859658338352591\n",
      "Iter 405: Loss 0.007708744612520565\n",
      "Iter 406: Loss 0.007769556102638473\n",
      "Iter 407: Loss 0.007859658338352591\n",
      "Iter 408: Loss 0.007822245180963756\n",
      "Iter 409: Loss 0.00784731434728809\n",
      "Iter 410: Loss 0.007892855151208814\n",
      "Iter 408: Loss 0.007822245180963756\n",
      "Iter 409: Loss 0.00784731434728809\n",
      "Iter 410: Loss 0.007892855151208814\n",
      "Iter 411: Loss 0.007806182145596502\n",
      "Iter 412: Loss 0.0077366467245562585\n",
      "Iter 413: Loss 0.007688174466648977\n",
      "Iter 411: Loss 0.007806182145596502\n",
      "Iter 412: Loss 0.0077366467245562585\n",
      "Iter 413: Loss 0.007688174466648977\n",
      "Iter 414: Loss 0.007715468873044926\n",
      "Iter 415: Loss 0.007600193727992014\n",
      "Iter 416: Loss 0.007779031932472945\n",
      "Iter 414: Loss 0.007715468873044926\n",
      "Iter 415: Loss 0.007600193727992014\n",
      "Iter 416: Loss 0.007779031932472945\n",
      "Iter 417: Loss 0.007687496329971892\n",
      "Iter 418: Loss 0.007672596358491513\n",
      "Iter 419: Loss 0.007692893822036103\n",
      "Iter 417: Loss 0.007687496329971892\n",
      "Iter 418: Loss 0.007672596358491513\n",
      "Iter 419: Loss 0.007692893822036103\n",
      "Iter 420: Loss 0.0077307324209613\n",
      "Iter 421: Loss 0.007543174568526522\n",
      "Iter 422: Loss 0.007645505155155997\n",
      "Iter 420: Loss 0.0077307324209613\n",
      "Iter 421: Loss 0.007543174568526522\n",
      "Iter 422: Loss 0.007645505155155997\n",
      "Iter 423: Loss 0.007545376966099539\n",
      "Iter 424: Loss 0.007466149663258931\n",
      "Iter 425: Loss 0.007443419473613808\n",
      "Iter 423: Loss 0.007545376966099539\n",
      "Iter 424: Loss 0.007466149663258931\n",
      "Iter 425: Loss 0.007443419473613808\n",
      "Iter 426: Loss 0.007521693102138009\n",
      "Iter 427: Loss 0.007560626713339678\n",
      "Iter 428: Loss 0.0074854068413466035\n",
      "Iter 426: Loss 0.007521693102138009\n",
      "Iter 427: Loss 0.007560626713339678\n",
      "Iter 428: Loss 0.0074854068413466035\n",
      "Iter 429: Loss 0.007661552010420077\n",
      "Iter 430: Loss 0.007566580039536406\n",
      "Iter 431: Loss 0.0075051870174750596\n",
      "Iter 429: Loss 0.007661552010420077\n",
      "Iter 430: Loss 0.007566580039536406\n",
      "Iter 431: Loss 0.0075051870174750596\n",
      "Iter 432: Loss 0.007491707563876154\n",
      "Iter 433: Loss 0.007334960434965031\n",
      "Iter 434: Loss 0.007481604517101051\n",
      "Iter 432: Loss 0.007491707563876154\n",
      "Iter 433: Loss 0.007334960434965031\n",
      "Iter 434: Loss 0.007481604517101051\n",
      "Iter 435: Loss 0.007466386178296483\n",
      "Iter 436: Loss 0.007526469563771627\n",
      "Iter 437: Loss 0.007370504314552048\n",
      "Iter 435: Loss 0.007466386178296483\n",
      "Iter 436: Loss 0.007526469563771627\n",
      "Iter 437: Loss 0.007370504314552048\n",
      "Iter 438: Loss 0.007444709123013738\n",
      "Iter 439: Loss 0.007372610583276806\n",
      "Iter 440: Loss 0.007434962038508432\n",
      "Iter 438: Loss 0.007444709123013738\n",
      "Iter 439: Loss 0.007372610583276806\n",
      "Iter 440: Loss 0.007434962038508432\n",
      "Iter 441: Loss 0.007324049334802076\n",
      "Iter 442: Loss 0.007435692998463522\n",
      "Iter 443: Loss 0.007427718111140999\n",
      "Iter 441: Loss 0.007324049334802076\n",
      "Iter 442: Loss 0.007435692998463522\n",
      "Iter 443: Loss 0.007427718111140999\n",
      "Iter 444: Loss 0.007318361552651533\n",
      "Iter 445: Loss 0.007301262990681235\n",
      "Iter 446: Loss 0.0073875657574621265\n",
      "Iter 444: Loss 0.007318361552651533\n",
      "Iter 445: Loss 0.007301262990681235\n",
      "Iter 446: Loss 0.0073875657574621265\n",
      "Iter 447: Loss 0.007226884008167746\n",
      "Iter 448: Loss 0.007236270371549382\n",
      "Iter 449: Loss 0.007271248899296134\n",
      "Iter 447: Loss 0.007226884008167746\n",
      "Iter 448: Loss 0.007236270371549382\n",
      "Iter 449: Loss 0.007271248899296134\n",
      "Iter 450: Loss 0.007339363326569517\n",
      "Iter 451: Loss 0.0072065151618150415\n",
      "Iter 452: Loss 0.007131449000802106\n",
      "Iter 450: Loss 0.007339363326569517\n",
      "Iter 451: Loss 0.0072065151618150415\n",
      "Iter 452: Loss 0.007131449000802106\n",
      "Iter 453: Loss 0.007074702047778223\n",
      "Iter 454: Loss 0.007132740553743587\n",
      "Iter 455: Loss 0.007166148183826439\n",
      "Iter 453: Loss 0.007074702047778223\n",
      "Iter 454: Loss 0.007132740553743587\n",
      "Iter 455: Loss 0.007166148183826439\n",
      "Iter 456: Loss 0.007009942612486209\n",
      "Iter 457: Loss 0.007126565940841705\n",
      "Iter 458: Loss 0.007171239681586534\n",
      "Iter 456: Loss 0.007009942612486209\n",
      "Iter 457: Loss 0.007126565940841705\n",
      "Iter 458: Loss 0.007171239681586534\n",
      "Iter 459: Loss 0.0071941488041373305\n",
      "Iter 460: Loss 0.007089217027980172\n",
      "Iter 461: Loss 0.007054913305712793\n",
      "Iter 459: Loss 0.0071941488041373305\n",
      "Iter 460: Loss 0.007089217027980172\n",
      "Iter 461: Loss 0.007054913305712793\n",
      "Iter 462: Loss 0.007157635070130734\n",
      "Iter 463: Loss 0.0071443473983429625\n",
      "Iter 464: Loss 0.0070597692402061115\n",
      "Iter 462: Loss 0.007157635070130734\n",
      "Iter 463: Loss 0.0071443473983429625\n",
      "Iter 464: Loss 0.0070597692402061115\n",
      "Iter 465: Loss 0.007059298113672557\n",
      "Iter 466: Loss 0.0070051747167895655\n",
      "Iter 467: Loss 0.006957668506218763\n",
      "Iter 465: Loss 0.007059298113672557\n",
      "Iter 466: Loss 0.0070051747167895655\n",
      "Iter 467: Loss 0.006957668506218763\n",
      "Iter 468: Loss 0.006964136740404689\n",
      "Iter 469: Loss 0.007008677233241037\n",
      "Iter 470: Loss 0.007018752202778282\n",
      "Iter 468: Loss 0.006964136740404689\n",
      "Iter 469: Loss 0.007008677233241037\n",
      "Iter 470: Loss 0.007018752202778282\n",
      "Iter 471: Loss 0.0068616971760214925\n",
      "Iter 472: Loss 0.006967233802506072\n",
      "Iter 473: Loss 0.0068918021615155915\n",
      "Iter 471: Loss 0.0068616971760214925\n",
      "Iter 472: Loss 0.006967233802506072\n",
      "Iter 473: Loss 0.0068918021615155915\n",
      "Iter 474: Loss 0.006822559409988617\n",
      "Iter 475: Loss 0.006929277184004793\n",
      "Iter 476: Loss 0.0069663281926137955\n",
      "Iter 474: Loss 0.006822559409988617\n",
      "Iter 475: Loss 0.006929277184004793\n",
      "Iter 476: Loss 0.0069663281926137955\n",
      "Iter 477: Loss 0.006924928067449087\n",
      "Iter 478: Loss 0.006938270466056413\n",
      "Iter 479: Loss 0.00692934618738597\n",
      "Iter 477: Loss 0.006924928067449087\n",
      "Iter 478: Loss 0.006938270466056413\n",
      "Iter 479: Loss 0.00692934618738597\n",
      "Iter 480: Loss 0.0068031009323820625\n",
      "Iter 481: Loss 0.006865336747464544\n",
      "Iter 482: Loss 0.006814794864007337\n",
      "Iter 480: Loss 0.0068031009323820625\n",
      "Iter 481: Loss 0.006865336747464544\n",
      "Iter 482: Loss 0.006814794864007337\n",
      "Iter 483: Loss 0.006729380575244774\n",
      "Iter 484: Loss 0.006839873072154032\n",
      "Iter 485: Loss 0.006758871668589092\n",
      "Iter 483: Loss 0.006729380575244774\n",
      "Iter 484: Loss 0.006839873072154032\n",
      "Iter 485: Loss 0.006758871668589092\n",
      "Iter 486: Loss 0.006787669396923926\n",
      "Iter 487: Loss 0.006771643480616891\n",
      "Iter 488: Loss 0.006760978889084624\n",
      "Iter 486: Loss 0.006787669396923926\n",
      "Iter 487: Loss 0.006771643480616891\n",
      "Iter 488: Loss 0.006760978889084624\n",
      "Iter 489: Loss 0.006851951995057736\n",
      "Iter 490: Loss 0.006696299402537698\n",
      "Iter 491: Loss 0.006732921638412628\n",
      "Iter 489: Loss 0.006851951995057736\n",
      "Iter 490: Loss 0.006696299402537698\n",
      "Iter 491: Loss 0.006732921638412628\n",
      "Iter 492: Loss 0.006708953194989415\n",
      "Iter 493: Loss 0.006764803104057997\n",
      "Iter 494: Loss 0.006611222040629435\n",
      "Iter 492: Loss 0.006708953194989415\n",
      "Iter 493: Loss 0.006764803104057997\n",
      "Iter 494: Loss 0.006611222040629435\n",
      "Iter 495: Loss 0.006688156051788026\n",
      "Iter 496: Loss 0.00668215228174023\n",
      "Iter 497: Loss 0.006669595569907548\n",
      "Iter 495: Loss 0.006688156051788026\n",
      "Iter 496: Loss 0.00668215228174023\n",
      "Iter 497: Loss 0.006669595569907548\n",
      "Iter 498: Loss 0.0066983085906434205\n",
      "Iter 499: Loss 0.006496533186373834\n",
      "Iter 500: Loss 0.006597186752898013\n",
      "Iter 498: Loss 0.0066983085906434205\n",
      "Iter 499: Loss 0.006496533186373834\n",
      "Iter 500: Loss 0.006597186752898013\n",
      "Iter 501: Loss 0.0066318930742031565\n",
      "Iter 502: Loss 0.006586337517835423\n",
      "Iter 503: Loss 0.006570576193803798\n",
      "Iter 501: Loss 0.0066318930742031565\n",
      "Iter 502: Loss 0.006586337517835423\n",
      "Iter 503: Loss 0.006570576193803798\n",
      "Iter 504: Loss 0.006622574286546536\n",
      "Iter 505: Loss 0.006651193081975697\n",
      "Iter 506: Loss 0.006548617890257084\n",
      "Iter 504: Loss 0.006622574286546536\n",
      "Iter 505: Loss 0.006651193081975697\n",
      "Iter 506: Loss 0.006548617890257084\n",
      "Iter 507: Loss 0.00643005485306243\n",
      "Iter 508: Loss 0.006545374255456373\n",
      "Iter 509: Loss 0.006591080191606533\n",
      "Iter 507: Loss 0.00643005485306243\n",
      "Iter 508: Loss 0.006545374255456373\n",
      "Iter 509: Loss 0.006591080191606533\n",
      "Iter 510: Loss 0.006590388254253212\n",
      "Iter 511: Loss 0.006515264986993786\n",
      "Iter 512: Loss 0.006533963475636617\n",
      "Iter 510: Loss 0.006590388254253212\n",
      "Iter 511: Loss 0.006515264986993786\n",
      "Iter 512: Loss 0.006533963475636617\n",
      "Iter 513: Loss 0.006389728325331758\n",
      "Iter 514: Loss 0.006558129887381\n",
      "Iter 515: Loss 0.00645282644473626\n",
      "Iter 513: Loss 0.006389728325331758\n",
      "Iter 514: Loss 0.006558129887381\n",
      "Iter 515: Loss 0.00645282644473626\n",
      "Iter 516: Loss 0.006589472650767801\n",
      "Iter 517: Loss 0.006523296028792026\n",
      "Iter 518: Loss 0.006324385930440145\n",
      "Iter 516: Loss 0.006589472650767801\n",
      "Iter 517: Loss 0.006523296028792026\n",
      "Iter 518: Loss 0.006324385930440145\n",
      "Iter 519: Loss 0.006555398305257161\n",
      "Iter 520: Loss 0.006452266803520644\n",
      "Iter 521: Loss 0.006381762003946209\n",
      "Iter 519: Loss 0.006555398305257161\n",
      "Iter 520: Loss 0.006452266803520644\n",
      "Iter 521: Loss 0.006381762003946209\n",
      "Iter 522: Loss 0.00646770214606188\n",
      "Iter 523: Loss 0.006356368283787649\n",
      "Iter 524: Loss 0.006446418647994538\n",
      "Iter 522: Loss 0.00646770214606188\n",
      "Iter 523: Loss 0.006356368283787649\n",
      "Iter 524: Loss 0.006446418647994538\n",
      "Iter 525: Loss 0.006312345554252823\n",
      "Iter 526: Loss 0.006452529492254505\n",
      "Iter 527: Loss 0.006388738959611295\n",
      "Iter 525: Loss 0.006312345554252823\n",
      "Iter 526: Loss 0.006452529492254505\n",
      "Iter 527: Loss 0.006388738959611295\n",
      "Iter 528: Loss 0.00649410617089795\n",
      "Iter 529: Loss 0.006249239344796735\n",
      "Iter 530: Loss 0.006218873098224937\n",
      "Iter 528: Loss 0.00649410617089795\n",
      "Iter 529: Loss 0.006249239344796735\n",
      "Iter 530: Loss 0.006218873098224937\n",
      "Iter 531: Loss 0.006322884036157422\n",
      "Iter 532: Loss 0.006243689093523159\n",
      "Iter 533: Loss 0.0063745547197536084\n",
      "Iter 531: Loss 0.006322884036157422\n",
      "Iter 532: Loss 0.006243689093523159\n",
      "Iter 533: Loss 0.0063745547197536084\n",
      "Iter 534: Loss 0.006176192365482658\n",
      "Iter 535: Loss 0.006284959777862488\n",
      "Iter 536: Loss 0.006302547550011062\n",
      "Iter 534: Loss 0.006176192365482658\n",
      "Iter 535: Loss 0.006284959777862488\n",
      "Iter 536: Loss 0.006302547550011062\n",
      "Iter 537: Loss 0.006201065942912759\n",
      "Iter 538: Loss 0.006268907687859145\n",
      "Iter 539: Loss 0.006175676505722686\n",
      "Iter 537: Loss 0.006201065942912759\n",
      "Iter 538: Loss 0.006268907687859145\n",
      "Iter 539: Loss 0.006175676505722686\n",
      "Iter 540: Loss 0.006371414827967357\n",
      "Iter 541: Loss 0.006258871265038283\n",
      "Iter 542: Loss 0.0061542797469331355\n",
      "Iter 540: Loss 0.006371414827967357\n",
      "Iter 541: Loss 0.006258871265038283\n",
      "Iter 542: Loss 0.0061542797469331355\n",
      "Iter 543: Loss 0.006154504840721389\n",
      "Iter 544: Loss 0.006195679396212458\n",
      "Iter 545: Loss 0.0061455677131454865\n",
      "Iter 543: Loss 0.006154504840721389\n",
      "Iter 544: Loss 0.006195679396212458\n",
      "Iter 545: Loss 0.0061455677131454865\n",
      "Iter 546: Loss 0.006135075392123468\n",
      "Iter 547: Loss 0.006074340995438322\n",
      "Iter 548: Loss 0.006165027142522816\n",
      "Iter 546: Loss 0.006135075392123468\n",
      "Iter 547: Loss 0.006074340995438322\n",
      "Iter 548: Loss 0.006165027142522816\n",
      "Iter 549: Loss 0.006158970073311629\n",
      "Iter 550: Loss 0.0061039696196596065\n",
      "Iter 551: Loss 0.006112902464267022\n",
      "Iter 549: Loss 0.006158970073311629\n",
      "Iter 550: Loss 0.0061039696196596065\n",
      "Iter 551: Loss 0.006112902464267022\n",
      "Iter 552: Loss 0.006182635377743049\n",
      "Iter 553: Loss 0.006052819554677267\n",
      "Iter 554: Loss 0.0060142747418371265\n",
      "Iter 552: Loss 0.006182635377743049\n",
      "Iter 553: Loss 0.006052819554677267\n",
      "Iter 554: Loss 0.0060142747418371265\n",
      "Iter 555: Loss 0.0060016304670931575\n",
      "Iter 556: Loss 0.006126981533454088\n",
      "Iter 557: Loss 0.00611406505226851\n",
      "Iter 555: Loss 0.0060016304670931575\n",
      "Iter 556: Loss 0.006126981533454088\n",
      "Iter 557: Loss 0.00611406505226851\n",
      "Iter 558: Loss 0.005957553486624164\n",
      "Iter 559: Loss 0.005946432521005352\n",
      "Iter 560: Loss 0.005970285800164807\n",
      "Iter 558: Loss 0.005957553486624164\n",
      "Iter 559: Loss 0.005946432521005352\n",
      "Iter 560: Loss 0.005970285800164807\n",
      "Iter 561: Loss 0.006006915650205936\n",
      "Iter 562: Loss 0.0060543295390115765\n",
      "Iter 563: Loss 0.0059648583273211875\n",
      "Iter 561: Loss 0.006006915650205936\n",
      "Iter 562: Loss 0.0060543295390115765\n",
      "Iter 563: Loss 0.0059648583273211875\n",
      "Iter 564: Loss 0.006076775625080405\n",
      "Iter 565: Loss 0.005986184179188011\n",
      "Iter 566: Loss 0.005879012410512228\n",
      "Iter 564: Loss 0.006076775625080405\n",
      "Iter 565: Loss 0.005986184179188011\n",
      "Iter 566: Loss 0.005879012410512228\n",
      "Iter 567: Loss 0.00586990777127995\n",
      "Iter 568: Loss 0.005942299456415538\n",
      "Iter 569: Loss 0.005936855803468746\n",
      "Iter 567: Loss 0.00586990777127995\n",
      "Iter 568: Loss 0.005942299456415538\n",
      "Iter 569: Loss 0.005936855803468746\n",
      "Iter 570: Loss 0.005967254886132277\n",
      "Iter 571: Loss 0.005949204077501735\n",
      "Iter 572: Loss 0.005887905756632487\n",
      "Iter 570: Loss 0.005967254886132277\n",
      "Iter 571: Loss 0.005949204077501735\n",
      "Iter 572: Loss 0.005887905756632487\n",
      "Iter 573: Loss 0.005894261205981592\n",
      "Iter 574: Loss 0.00581120968816761\n",
      "Iter 575: Loss 0.005818806722492515\n",
      "Iter 573: Loss 0.005894261205981592\n",
      "Iter 574: Loss 0.00581120968816761\n",
      "Iter 575: Loss 0.005818806722492515\n",
      "Iter 576: Loss 0.00582935519799025\n",
      "Iter 577: Loss 0.00581665000991669\n",
      "Iter 578: Loss 0.005810975076671608\n",
      "Iter 576: Loss 0.00582935519799025\n",
      "Iter 577: Loss 0.00581665000991669\n",
      "Iter 578: Loss 0.005810975076671608\n",
      "Iter 579: Loss 0.005851223083313354\n",
      "Iter 580: Loss 0.005846683136717288\n",
      "Iter 581: Loss 0.005778104245305775\n",
      "Iter 579: Loss 0.005851223083313354\n",
      "Iter 580: Loss 0.005846683136717288\n",
      "Iter 581: Loss 0.005778104245305775\n",
      "Iter 582: Loss 0.005774596018229654\n",
      "Iter 583: Loss 0.0057684846980843\n",
      "Iter 584: Loss 0.005853271769906233\n",
      "Iter 582: Loss 0.005774596018229654\n",
      "Iter 583: Loss 0.0057684846980843\n",
      "Iter 584: Loss 0.005853271769906233\n",
      "Iter 585: Loss 0.005823109202280254\n",
      "Iter 586: Loss 0.005836038532371293\n",
      "Iter 587: Loss 0.00568715207828971\n",
      "Iter 585: Loss 0.005823109202280254\n",
      "Iter 586: Loss 0.005836038532371293\n",
      "Iter 587: Loss 0.00568715207828971\n",
      "Iter 588: Loss 0.0057070331421203\n",
      "Iter 589: Loss 0.005729022378217198\n",
      "Iter 590: Loss 0.005789124323222452\n",
      "Iter 588: Loss 0.0057070331421203\n",
      "Iter 589: Loss 0.005729022378217198\n",
      "Iter 590: Loss 0.005789124323222452\n",
      "Iter 591: Loss 0.005700811416565063\n",
      "Iter 592: Loss 0.0056985504850893915\n",
      "Iter 593: Loss 0.005830481142816905\n",
      "Iter 591: Loss 0.005700811416565063\n",
      "Iter 592: Loss 0.0056985504850893915\n",
      "Iter 593: Loss 0.005830481142816905\n",
      "Iter 594: Loss 0.005699561265652289\n",
      "Iter 595: Loss 0.00567791085995124\n",
      "Iter 596: Loss 0.005614482476087863\n",
      "Iter 594: Loss 0.005699561265652289\n",
      "Iter 595: Loss 0.00567791085995124\n",
      "Iter 596: Loss 0.005614482476087863\n",
      "Iter 597: Loss 0.0057631776242437\n",
      "Iter 598: Loss 0.005561736767401476\n",
      "Iter 599: Loss 0.005689988831084169\n",
      "Iter 597: Loss 0.0057631776242437\n",
      "Iter 598: Loss 0.005561736767401476\n",
      "Iter 599: Loss 0.005689988831084169\n",
      "Iter 600: Loss 0.005660683808926337\n",
      "Iter 601: Loss 0.005528154011496051\n",
      "Iter 602: Loss 0.005725235282304044\n",
      "Iter 600: Loss 0.005660683808926337\n",
      "Iter 601: Loss 0.005528154011496051\n",
      "Iter 602: Loss 0.005725235282304044\n",
      "Iter 603: Loss 0.005639160464623731\n",
      "Iter 604: Loss 0.00563391097291501\n",
      "Iter 605: Loss 0.0056643704930227435\n",
      "Iter 603: Loss 0.005639160464623731\n",
      "Iter 604: Loss 0.00563391097291501\n",
      "Iter 605: Loss 0.0056643704930227435\n",
      "Iter 606: Loss 0.005579202237005481\n",
      "Iter 607: Loss 0.005461778945313718\n",
      "Iter 608: Loss 0.005569033042161526\n",
      "Iter 606: Loss 0.005579202237005481\n",
      "Iter 607: Loss 0.005461778945313718\n",
      "Iter 608: Loss 0.005569033042161526\n",
      "Iter 609: Loss 0.005664763098467372\n",
      "Iter 610: Loss 0.0056691360092924505\n",
      "Iter 611: Loss 0.005617370148618778\n",
      "Iter 609: Loss 0.005664763098467372\n",
      "Iter 610: Loss 0.0056691360092924505\n",
      "Iter 611: Loss 0.005617370148618778\n",
      "Iter 612: Loss 0.005375686995759458\n",
      "Iter 613: Loss 0.005540867765506585\n",
      "Iter 614: Loss 0.00545253011280905\n",
      "Iter 612: Loss 0.005375686995759458\n",
      "Iter 613: Loss 0.005540867765506585\n",
      "Iter 614: Loss 0.00545253011280905\n",
      "Iter 615: Loss 0.005626256832343614\n",
      "Iter 616: Loss 0.005411674400527558\n",
      "Iter 617: Loss 0.005454699674290336\n",
      "Iter 615: Loss 0.005626256832343614\n",
      "Iter 616: Loss 0.005411674400527558\n",
      "Iter 617: Loss 0.005454699674290336\n",
      "Iter 618: Loss 0.005493101721514247\n",
      "Iter 619: Loss 0.005590214224870571\n",
      "Iter 620: Loss 0.005502094527680479\n",
      "Iter 618: Loss 0.005493101721514247\n",
      "Iter 619: Loss 0.005590214224870571\n",
      "Iter 620: Loss 0.005502094527680479\n",
      "Iter 621: Loss 0.005478605776727794\n",
      "Iter 622: Loss 0.0055442784360782826\n",
      "Iter 623: Loss 0.005464710875185664\n",
      "Iter 621: Loss 0.005478605776727794\n",
      "Iter 622: Loss 0.0055442784360782826\n",
      "Iter 623: Loss 0.005464710875185664\n",
      "Iter 624: Loss 0.005354767074128111\n",
      "Iter 625: Loss 0.005402304217249096\n",
      "Iter 626: Loss 0.005360742766938048\n",
      "Iter 624: Loss 0.005354767074128111\n",
      "Iter 625: Loss 0.005402304217249096\n",
      "Iter 626: Loss 0.005360742766938048\n",
      "Iter 627: Loss 0.005507782785716409\n",
      "Iter 628: Loss 0.0054292022111173165\n",
      "Iter 629: Loss 0.005373716354370117\n",
      "Iter 627: Loss 0.005507782785716409\n",
      "Iter 628: Loss 0.0054292022111173165\n",
      "Iter 629: Loss 0.005373716354370117\n",
      "Iter 630: Loss 0.0053981730562008305\n",
      "Iter 631: Loss 0.005474928134453749\n",
      "Iter 632: Loss 0.005280975334182709\n",
      "Iter 630: Loss 0.0053981730562008305\n",
      "Iter 631: Loss 0.005474928134453749\n",
      "Iter 632: Loss 0.005280975334182709\n",
      "Iter 633: Loss 0.005493709427153993\n",
      "Iter 634: Loss 0.00540270300920376\n",
      "Iter 635: Loss 0.005412718968952963\n",
      "Iter 633: Loss 0.005493709427153993\n",
      "Iter 634: Loss 0.00540270300920376\n",
      "Iter 635: Loss 0.005412718968952963\n",
      "Iter 636: Loss 0.005309019735949244\n",
      "Iter 637: Loss 0.00532768776792728\n",
      "Iter 638: Loss 0.005264716234035835\n",
      "Iter 636: Loss 0.005309019735949244\n",
      "Iter 637: Loss 0.00532768776792728\n",
      "Iter 638: Loss 0.005264716234035835\n",
      "Iter 639: Loss 0.0054136850162894426\n",
      "Iter 640: Loss 0.005394711465892678\n",
      "Iter 641: Loss 0.005240042528468453\n",
      "Iter 639: Loss 0.0054136850162894426\n",
      "Iter 640: Loss 0.005394711465892678\n",
      "Iter 641: Loss 0.005240042528468453\n",
      "Iter 642: Loss 0.005250954580402184\n",
      "Iter 643: Loss 0.005269494123325614\n",
      "Iter 644: Loss 0.005265953536043148\n",
      "Iter 642: Loss 0.005250954580402184\n",
      "Iter 643: Loss 0.005269494123325614\n",
      "Iter 644: Loss 0.005265953536043148\n",
      "Iter 645: Loss 0.00527231136481919\n",
      "Iter 646: Loss 0.005297609907900264\n",
      "Iter 647: Loss 0.005343135959373977\n",
      "Iter 645: Loss 0.00527231136481919\n",
      "Iter 646: Loss 0.005297609907900264\n",
      "Iter 647: Loss 0.005343135959373977\n",
      "Iter 648: Loss 0.005243581688094758\n",
      "Iter 649: Loss 0.005252695845034784\n",
      "Iter 650: Loss 0.005260065406144498\n",
      "Iter 648: Loss 0.005243581688094758\n",
      "Iter 649: Loss 0.005252695845034784\n",
      "Iter 650: Loss 0.005260065406144498\n",
      "Iter 651: Loss 0.005378447131006541\n",
      "Iter 652: Loss 0.005336505448270939\n",
      "Iter 653: Loss 0.005356028170404796\n",
      "Iter 651: Loss 0.005378447131006541\n",
      "Iter 652: Loss 0.005336505448270939\n",
      "Iter 653: Loss 0.005356028170404796\n",
      "Iter 654: Loss 0.005126882693962661\n",
      "Iter 655: Loss 0.005285672322956626\n",
      "Iter 656: Loss 0.00522603531797489\n",
      "Iter 654: Loss 0.005126882693962661\n",
      "Iter 655: Loss 0.005285672322956626\n",
      "Iter 656: Loss 0.00522603531797489\n",
      "Iter 657: Loss 0.005239810296399389\n",
      "Iter 658: Loss 0.005340492416046813\n",
      "Iter 659: Loss 0.00527493158976237\n",
      "Iter 657: Loss 0.005239810296399389\n",
      "Iter 658: Loss 0.005340492416046813\n",
      "Iter 659: Loss 0.00527493158976237\n",
      "Iter 660: Loss 0.005218375466778845\n",
      "Iter 661: Loss 0.005169953652722631\n",
      "Iter 662: Loss 0.0051511195367443825\n",
      "Iter 660: Loss 0.005218375466778845\n",
      "Iter 661: Loss 0.005169953652722631\n",
      "Iter 662: Loss 0.0051511195367443825\n",
      "Iter 663: Loss 0.005272951906550668\n",
      "Iter 664: Loss 0.005106766066865293\n",
      "Iter 665: Loss 0.005354699498403096\n",
      "Iter 663: Loss 0.005272951906550668\n",
      "Iter 664: Loss 0.005106766066865293\n",
      "Iter 665: Loss 0.005354699498403096\n",
      "Iter 666: Loss 0.005402440796355288\n",
      "Iter 667: Loss 0.005018487899841187\n",
      "Iter 668: Loss 0.005041874811320962\n",
      "Iter 666: Loss 0.005402440796355288\n",
      "Iter 667: Loss 0.005018487899841187\n",
      "Iter 668: Loss 0.005041874811320962\n",
      "Iter 669: Loss 0.005194111022644652\n",
      "Iter 670: Loss 0.005107052549868525\n",
      "Iter 671: Loss 0.005275482665041012\n",
      "Iter 669: Loss 0.005194111022644652\n",
      "Iter 670: Loss 0.005107052549868525\n",
      "Iter 671: Loss 0.005275482665041012\n",
      "Iter 672: Loss 0.005178802265616472\n",
      "Iter 673: Loss 0.00523004893533246\n",
      "Iter 674: Loss 0.005149361140237835\n",
      "Iter 672: Loss 0.005178802265616472\n",
      "Iter 673: Loss 0.00523004893533246\n",
      "Iter 674: Loss 0.005149361140237835\n",
      "Iter 675: Loss 0.005185753523470637\n",
      "Iter 676: Loss 0.005049074481347364\n",
      "Iter 677: Loss 0.004891223298337407\n",
      "Iter 675: Loss 0.005185753523470637\n",
      "Iter 676: Loss 0.005049074481347364\n",
      "Iter 677: Loss 0.004891223298337407\n",
      "Iter 678: Loss 0.005056894705918973\n",
      "Iter 679: Loss 0.005046100197675937\n",
      "Iter 680: Loss 0.004999156483633076\n",
      "Iter 678: Loss 0.005056894705918973\n",
      "Iter 679: Loss 0.005046100197675937\n",
      "Iter 680: Loss 0.004999156483633076\n",
      "Iter 681: Loss 0.00507838616590062\n",
      "Iter 682: Loss 0.0050212941959708515\n",
      "Iter 683: Loss 0.004897129035995392\n",
      "Iter 681: Loss 0.00507838616590062\n",
      "Iter 682: Loss 0.0050212941959708515\n",
      "Iter 683: Loss 0.004897129035995392\n",
      "Iter 684: Loss 0.005103283537600093\n",
      "Iter 685: Loss 0.00501805056117014\n",
      "Iter 686: Loss 0.004901301599072363\n",
      "Iter 684: Loss 0.005103283537600093\n",
      "Iter 685: Loss 0.00501805056117014\n",
      "Iter 686: Loss 0.004901301599072363\n",
      "Iter 687: Loss 0.005022431562046805\n",
      "Iter 688: Loss 0.0049637672667969725\n",
      "Iter 689: Loss 0.0050538085891815\n",
      "Iter 687: Loss 0.005022431562046805\n",
      "Iter 688: Loss 0.0049637672667969725\n",
      "Iter 689: Loss 0.0050538085891815\n",
      "Iter 690: Loss 0.005037942094479254\n",
      "Iter 691: Loss 0.0050363959428555\n",
      "Iter 692: Loss 0.004844164895916175\n",
      "Iter 690: Loss 0.005037942094479254\n",
      "Iter 691: Loss 0.0050363959428555\n",
      "Iter 692: Loss 0.004844164895916175\n",
      "Iter 693: Loss 0.0050051140927982905\n",
      "Iter 694: Loss 0.005059234634368958\n",
      "Iter 695: Loss 0.004912224120484617\n",
      "Iter 693: Loss 0.0050051140927982905\n",
      "Iter 694: Loss 0.005059234634368958\n",
      "Iter 695: Loss 0.004912224120484617\n",
      "Iter 696: Loss 0.005144896383532983\n",
      "Iter 697: Loss 0.005114726677626193\n",
      "Iter 698: Loss 0.004976707542251922\n",
      "Iter 696: Loss 0.005144896383532983\n",
      "Iter 697: Loss 0.005114726677626193\n",
      "Iter 698: Loss 0.004976707542251922\n",
      "Iter 699: Loss 0.0049072311309997195\n",
      "Iter 700: Loss 0.005031944510941496\n",
      "Iter 701: Loss 0.00503052446894541\n",
      "Iter 699: Loss 0.0049072311309997195\n",
      "Iter 700: Loss 0.005031944510941496\n",
      "Iter 701: Loss 0.00503052446894541\n",
      "Iter 702: Loss 0.004833902903421673\n",
      "Iter 703: Loss 0.004842113829896359\n",
      "Iter 704: Loss 0.004943537378977397\n",
      "Iter 702: Loss 0.004833902903421673\n",
      "Iter 703: Loss 0.004842113829896359\n",
      "Iter 704: Loss 0.004943537378977397\n",
      "Iter 705: Loss 0.004966258526800159\n",
      "Iter 706: Loss 0.00488626171728808\n",
      "Iter 707: Loss 0.0048811716471841475\n",
      "Iter 705: Loss 0.004966258526800159\n",
      "Iter 706: Loss 0.00488626171728808\n",
      "Iter 707: Loss 0.0048811716471841475\n",
      "Iter 708: Loss 0.004904972578950985\n",
      "Iter 709: Loss 0.005019624790031753\n",
      "Iter 710: Loss 0.004808459215297433\n",
      "Iter 708: Loss 0.004904972578950985\n",
      "Iter 709: Loss 0.005019624790031753\n",
      "Iter 710: Loss 0.004808459215297433\n",
      "Iter 711: Loss 0.0049278345888484265\n",
      "Iter 712: Loss 0.0048714792894983955\n",
      "Iter 713: Loss 0.005006053966438461\n",
      "Iter 711: Loss 0.0049278345888484265\n",
      "Iter 712: Loss 0.0048714792894983955\n",
      "Iter 713: Loss 0.005006053966438461\n",
      "Iter 714: Loss 0.00491939761681471\n",
      "Iter 715: Loss 0.004958754290125803\n",
      "Iter 716: Loss 0.004767370795061488\n",
      "Iter 714: Loss 0.00491939761681471\n",
      "Iter 715: Loss 0.004958754290125803\n",
      "Iter 716: Loss 0.004767370795061488\n",
      "Iter 717: Loss 0.004865619712722992\n",
      "Iter 718: Loss 0.005006114403882664\n",
      "Iter 719: Loss 0.004810792005466606\n",
      "Iter 717: Loss 0.004865619712722992\n",
      "Iter 718: Loss 0.005006114403882664\n",
      "Iter 719: Loss 0.004810792005466606\n",
      "Iter 720: Loss 0.004781889582346537\n",
      "Iter 721: Loss 0.004815151115615449\n",
      "Iter 722: Loss 0.004856386584436108\n",
      "Iter 720: Loss 0.004781889582346537\n",
      "Iter 721: Loss 0.004815151115615449\n",
      "Iter 722: Loss 0.004856386584436108\n",
      "Iter 723: Loss 0.004735209508808311\n",
      "Iter 724: Loss 0.004820607141582314\n",
      "Iter 725: Loss 0.004840121297779197\n",
      "Iter 723: Loss 0.004735209508808311\n",
      "Iter 724: Loss 0.004820607141582314\n",
      "Iter 725: Loss 0.004840121297779197\n",
      "Iter 726: Loss 0.004692924712708373\n",
      "Iter 727: Loss 0.0049030528572980995\n",
      "Iter 728: Loss 0.004757064069340567\n",
      "Iter 726: Loss 0.004692924712708373\n",
      "Iter 727: Loss 0.0049030528572980995\n",
      "Iter 728: Loss 0.004757064069340567\n",
      "Iter 729: Loss 0.004799009559159269\n",
      "Iter 730: Loss 0.0047743377571334384\n",
      "Iter 731: Loss 0.004807828191273703\n",
      "Iter 729: Loss 0.004799009559159269\n",
      "Iter 730: Loss 0.0047743377571334384\n",
      "Iter 731: Loss 0.004807828191273703\n",
      "Iter 732: Loss 0.004800591402187081\n",
      "Iter 733: Loss 0.004826667541991213\n",
      "Iter 734: Loss 0.005025530527689738\n",
      "Iter 732: Loss 0.004800591402187081\n",
      "Iter 733: Loss 0.004826667541991213\n",
      "Iter 734: Loss 0.005025530527689738\n",
      "Iter 735: Loss 0.004867639370307237\n",
      "Iter 736: Loss 0.004898219765303378\n",
      "Iter 737: Loss 0.004723395654065404\n",
      "Iter 735: Loss 0.004867639370307237\n",
      "Iter 736: Loss 0.004898219765303378\n",
      "Iter 737: Loss 0.004723395654065404\n",
      "Iter 738: Loss 0.004697456093367464\n",
      "Iter 739: Loss 0.0048105288408473584\n",
      "Iter 740: Loss 0.004665308607790523\n",
      "Iter 738: Loss 0.004697456093367464\n",
      "Iter 739: Loss 0.0048105288408473584\n",
      "Iter 740: Loss 0.004665308607790523\n",
      "Iter 741: Loss 0.004734689366079852\n",
      "Iter 742: Loss 0.004600077570079567\n",
      "Iter 743: Loss 0.004725421498159687\n",
      "Iter 741: Loss 0.004734689366079852\n",
      "Iter 742: Loss 0.004600077570079567\n",
      "Iter 743: Loss 0.004725421498159687\n",
      "Iter 744: Loss 0.004622165314451663\n",
      "Iter 745: Loss 0.004762046589346941\n",
      "Iter 746: Loss 0.004578193504653291\n",
      "Iter 744: Loss 0.004622165314451663\n",
      "Iter 745: Loss 0.004762046589346941\n",
      "Iter 746: Loss 0.004578193504653291\n",
      "Iter 747: Loss 0.004637488823926853\n",
      "Iter 748: Loss 0.0046029071845932155\n",
      "Iter 749: Loss 0.004565901385096019\n",
      "Iter 747: Loss 0.004637488823926853\n",
      "Iter 748: Loss 0.0046029071845932155\n",
      "Iter 749: Loss 0.004565901385096019\n",
      "Iter 750: Loss 0.004783933034200155\n",
      "Iter 751: Loss 0.004748936898694067\n",
      "Iter 752: Loss 0.004624880716472329\n",
      "Iter 750: Loss 0.004783933034200155\n",
      "Iter 751: Loss 0.004748936898694067\n",
      "Iter 752: Loss 0.004624880716472329\n",
      "Iter 753: Loss 0.004676786011564517\n",
      "Iter 754: Loss 0.00457087153208232\n",
      "Iter 755: Loss 0.004624296805101955\n",
      "Iter 753: Loss 0.004676786011564517\n",
      "Iter 754: Loss 0.00457087153208232\n",
      "Iter 755: Loss 0.004624296805101955\n",
      "Iter 756: Loss 0.0045382043796623065\n",
      "Iter 757: Loss 0.004458238978585797\n",
      "Iter 758: Loss 0.004698075220256509\n",
      "Iter 756: Loss 0.0045382043796623065\n",
      "Iter 757: Loss 0.004458238978585797\n",
      "Iter 758: Loss 0.004698075220256509\n",
      "Iter 759: Loss 0.004629255054953569\n",
      "Iter 760: Loss 0.004653396720657806\n",
      "Iter 761: Loss 0.004803197350568638\n",
      "Iter 759: Loss 0.004629255054953569\n",
      "Iter 760: Loss 0.004653396720657806\n",
      "Iter 761: Loss 0.004803197350568638\n",
      "Iter 762: Loss 0.0045127383249248575\n",
      "Iter 763: Loss 0.004718146638242071\n",
      "Iter 764: Loss 0.004707926999547049\n",
      "Iter 762: Loss 0.0045127383249248575\n",
      "Iter 763: Loss 0.004718146638242071\n",
      "Iter 764: Loss 0.004707926999547049\n",
      "Iter 765: Loss 0.004548061393692108\n",
      "Iter 766: Loss 0.004676963040928641\n",
      "Iter 767: Loss 0.004692219450564203\n",
      "Iter 765: Loss 0.004548061393692108\n",
      "Iter 766: Loss 0.004676963040928641\n",
      "Iter 767: Loss 0.004692219450564203\n",
      "Iter 768: Loss 0.0045731310359018295\n",
      "Iter 769: Loss 0.0045845884524895525\n",
      "Iter 770: Loss 0.004749742572654983\n",
      "Iter 768: Loss 0.0045731310359018295\n",
      "Iter 769: Loss 0.0045845884524895525\n",
      "Iter 770: Loss 0.004749742572654983\n",
      "Iter 771: Loss 0.004594745274313434\n",
      "Iter 772: Loss 0.004433625710462619\n",
      "Iter 773: Loss 0.004561198685697453\n",
      "Iter 771: Loss 0.004594745274313434\n",
      "Iter 772: Loss 0.004433625710462619\n",
      "Iter 773: Loss 0.004561198685697453\n",
      "Iter 774: Loss 0.004716710892028199\n",
      "Iter 775: Loss 0.0046653742799739875\n",
      "Iter 776: Loss 0.004512064471216259\n",
      "Iter 774: Loss 0.004716710892028199\n",
      "Iter 775: Loss 0.0046653742799739875\n",
      "Iter 776: Loss 0.004512064471216259\n",
      "Iter 777: Loss 0.004592199287490693\n",
      "Iter 778: Loss 0.004716099379305354\n",
      "Iter 779: Loss 0.004653367215763785\n",
      "Iter 777: Loss 0.004592199287490693\n",
      "Iter 778: Loss 0.004716099379305354\n",
      "Iter 779: Loss 0.004653367215763785\n",
      "Iter 780: Loss 0.004674088693188574\n",
      "Iter 781: Loss 0.004606306909801004\n",
      "Iter 782: Loss 0.004606551039004754\n",
      "Iter 780: Loss 0.004674088693188574\n",
      "Iter 781: Loss 0.004606306909801004\n",
      "Iter 782: Loss 0.004606551039004754\n",
      "Iter 783: Loss 0.004506166347724473\n",
      "Iter 784: Loss 0.004463456585973561\n",
      "Iter 785: Loss 0.004515556994074595\n",
      "Iter 783: Loss 0.004506166347724473\n",
      "Iter 784: Loss 0.004463456585973561\n",
      "Iter 785: Loss 0.004515556994074595\n",
      "Iter 786: Loss 0.004556552140774603\n",
      "Iter 787: Loss 0.004526635130008538\n",
      "Iter 788: Loss 0.004513447870037513\n",
      "Iter 786: Loss 0.004556552140774603\n",
      "Iter 787: Loss 0.004526635130008538\n",
      "Iter 788: Loss 0.004513447870037513\n",
      "Iter 789: Loss 0.00461657937177403\n",
      "Iter 790: Loss 0.0046117814952979785\n",
      "Iter 791: Loss 0.004512070657726295\n",
      "Iter 789: Loss 0.00461657937177403\n",
      "Iter 790: Loss 0.0046117814952979785\n",
      "Iter 791: Loss 0.004512070657726295\n",
      "Iter 792: Loss 0.004521463207617967\n",
      "Iter 793: Loss 0.004565647262299132\n",
      "Iter 794: Loss 0.004462865536322374\n",
      "Iter 792: Loss 0.004521463207617967\n",
      "Iter 793: Loss 0.004565647262299132\n",
      "Iter 794: Loss 0.004462865536322374\n",
      "Iter 795: Loss 0.004493898022436572\n",
      "Iter 796: Loss 0.004203790200208713\n",
      "Iter 797: Loss 0.004414204351916285\n",
      "Iter 795: Loss 0.004493898022436572\n",
      "Iter 796: Loss 0.004203790200208713\n",
      "Iter 797: Loss 0.004414204351916285\n",
      "Iter 798: Loss 0.004402177776405197\n",
      "Iter 799: Loss 0.004321832142904133\n",
      "Iter 800: Loss 0.004322741559879509\n",
      "Iter 798: Loss 0.004402177776405197\n",
      "Iter 799: Loss 0.004321832142904133\n",
      "Iter 800: Loss 0.004322741559879509\n",
      "Iter 801: Loss 0.004367455274996881\n",
      "Iter 802: Loss 0.004543718463646438\n",
      "Iter 803: Loss 0.00447054394704853\n",
      "Iter 801: Loss 0.004367455274996881\n",
      "Iter 802: Loss 0.004543718463646438\n",
      "Iter 803: Loss 0.00447054394704853\n",
      "Iter 804: Loss 0.004486712153086405\n",
      "Iter 805: Loss 0.004329685203567474\n",
      "Iter 806: Loss 0.004313894374641829\n",
      "Iter 804: Loss 0.004486712153086405\n",
      "Iter 805: Loss 0.004329685203567474\n",
      "Iter 806: Loss 0.004313894374641829\n",
      "Iter 807: Loss 0.0044620555793929715\n",
      "Iter 808: Loss 0.004437344754765372\n",
      "Iter 809: Loss 0.004385631717369704\n",
      "Iter 807: Loss 0.0044620555793929715\n",
      "Iter 808: Loss 0.004437344754765372\n",
      "Iter 809: Loss 0.004385631717369704\n",
      "Iter 810: Loss 0.004462548596654348\n",
      "Iter 811: Loss 0.0044361921603570205\n",
      "Iter 812: Loss 0.004530352746655127\n",
      "Iter 810: Loss 0.004462548596654348\n",
      "Iter 811: Loss 0.0044361921603570205\n",
      "Iter 812: Loss 0.004530352746655127\n",
      "Iter 813: Loss 0.00447922076531751\n",
      "Iter 814: Loss 0.004377225677886171\n",
      "Iter 815: Loss 0.004358238326813171\n",
      "Iter 813: Loss 0.00447922076531751\n",
      "Iter 814: Loss 0.004377225677886171\n",
      "Iter 815: Loss 0.004358238326813171\n",
      "Iter 816: Loss 0.004397878151929783\n",
      "Iter 817: Loss 0.004350865910391132\n",
      "Iter 818: Loss 0.004524312809317888\n",
      "Iter 816: Loss 0.004397878151929783\n",
      "Iter 817: Loss 0.004350865910391132\n",
      "Iter 818: Loss 0.004524312809317888\n",
      "Iter 819: Loss 0.004326885093947847\n",
      "Iter 820: Loss 0.00441960469929282\n",
      "Iter 821: Loss 0.004464408356748417\n",
      "Iter 819: Loss 0.004326885093947847\n",
      "Iter 820: Loss 0.00441960469929282\n",
      "Iter 821: Loss 0.004464408356748417\n",
      "Iter 822: Loss 0.004283662803634674\n",
      "Iter 823: Loss 0.004277369219385935\n",
      "Iter 824: Loss 0.0044850375124080456\n",
      "Iter 822: Loss 0.004283662803634674\n",
      "Iter 823: Loss 0.004277369219385935\n",
      "Iter 824: Loss 0.0044850375124080456\n",
      "Iter 825: Loss 0.004399709358900607\n",
      "Iter 826: Loss 0.004506884458774102\n",
      "Iter 827: Loss 0.004338137403933588\n",
      "Iter 825: Loss 0.004399709358900607\n",
      "Iter 826: Loss 0.004506884458774102\n",
      "Iter 827: Loss 0.004338137403933588\n",
      "Iter 828: Loss 0.004284459911658617\n",
      "Iter 829: Loss 0.004209648825213343\n",
      "Iter 830: Loss 0.004341316794206996\n",
      "Iter 828: Loss 0.004284459911658617\n",
      "Iter 829: Loss 0.004209648825213343\n",
      "Iter 830: Loss 0.004341316794206996\n",
      "Iter 831: Loss 0.004325826248960819\n",
      "Iter 832: Loss 0.004358854122504503\n",
      "Iter 833: Loss 0.004381858422132785\n",
      "Iter 831: Loss 0.004325826248960819\n",
      "Iter 832: Loss 0.004358854122504503\n",
      "Iter 833: Loss 0.004381858422132785\n",
      "Iter 834: Loss 0.004228057975540618\n",
      "Iter 835: Loss 0.004232475619592115\n",
      "Iter 836: Loss 0.004317857548148332\n",
      "Iter 834: Loss 0.004228057975540618\n",
      "Iter 835: Loss 0.004232475619592115\n",
      "Iter 836: Loss 0.004317857548148332\n",
      "Iter 837: Loss 0.004395532036969762\n",
      "Iter 838: Loss 0.004290917676366018\n",
      "Iter 839: Loss 0.004355787517067915\n",
      "Iter 837: Loss 0.004395532036969762\n",
      "Iter 838: Loss 0.004290917676366018\n",
      "Iter 839: Loss 0.004355787517067915\n",
      "Iter 840: Loss 0.004368453206654318\n",
      "Iter 841: Loss 0.0042776047826527126\n",
      "Iter 842: Loss 0.004295192078915899\n",
      "Iter 840: Loss 0.004368453206654318\n",
      "Iter 841: Loss 0.0042776047826527126\n",
      "Iter 842: Loss 0.004295192078915899\n",
      "Iter 843: Loss 0.004233259402825209\n",
      "Iter 844: Loss 0.00424562385696137\n",
      "Iter 845: Loss 0.0043472529885297765\n",
      "Iter 843: Loss 0.004233259402825209\n",
      "Iter 844: Loss 0.00424562385696137\n",
      "Iter 845: Loss 0.0043472529885297765\n",
      "Iter 846: Loss 0.0043486930177121345\n",
      "Iter 847: Loss 0.00431991004182431\n",
      "Iter 848: Loss 0.004302543556380891\n",
      "Iter 846: Loss 0.0043486930177121345\n",
      "Iter 847: Loss 0.00431991004182431\n",
      "Iter 848: Loss 0.004302543556380891\n",
      "Iter 849: Loss 0.0042945029968749025\n",
      "Iter 850: Loss 0.004286340610709733\n",
      "Iter 851: Loss 0.004128625055035193\n",
      "Iter 849: Loss 0.0042945029968749025\n",
      "Iter 850: Loss 0.004286340610709733\n",
      "Iter 851: Loss 0.004128625055035193\n",
      "Iter 852: Loss 0.004334722450393402\n",
      "Iter 853: Loss 0.004317241276571612\n",
      "Iter 854: Loss 0.004283398687244652\n",
      "Iter 852: Loss 0.004334722450393402\n",
      "Iter 853: Loss 0.004317241276571612\n",
      "Iter 854: Loss 0.004283398687244652\n",
      "Iter 855: Loss 0.0043125509501931196\n",
      "Iter 856: Loss 0.004253664892352745\n",
      "Iter 857: Loss 0.00429672776106113\n",
      "Iter 855: Loss 0.0043125509501931196\n",
      "Iter 856: Loss 0.004253664892352745\n",
      "Iter 857: Loss 0.00429672776106113\n",
      "Iter 858: Loss 0.004262682920444512\n",
      "Iter 859: Loss 0.004122218685949634\n",
      "Iter 860: Loss 0.00411957704616402\n",
      "Iter 858: Loss 0.004262682920444512\n",
      "Iter 859: Loss 0.004122218685949634\n",
      "Iter 860: Loss 0.00411957704616402\n",
      "Iter 861: Loss 0.004135681959445367\n",
      "Iter 862: Loss 0.004312421985253127\n",
      "Iter 863: Loss 0.004285672943511171\n",
      "Iter 861: Loss 0.004135681959445367\n",
      "Iter 862: Loss 0.004312421985253127\n",
      "Iter 863: Loss 0.004285672943511171\n",
      "Iter 864: Loss 0.004146269933430259\n",
      "Iter 865: Loss 0.004258120131349849\n",
      "Iter 866: Loss 0.0043440655082047815\n",
      "Iter 864: Loss 0.004146269933430259\n",
      "Iter 865: Loss 0.004258120131349849\n",
      "Iter 866: Loss 0.0043440655082047815\n",
      "Iter 867: Loss 0.004245300730783307\n",
      "Iter 868: Loss 0.004287213860395663\n",
      "Iter 869: Loss 0.004280583349292625\n",
      "Iter 867: Loss 0.004245300730783307\n",
      "Iter 868: Loss 0.004287213860395663\n",
      "Iter 869: Loss 0.004280583349292625\n",
      "Iter 870: Loss 0.004186900076038109\n",
      "Iter 871: Loss 0.004206277653128801\n",
      "Iter 872: Loss 0.004252669816007634\n",
      "Iter 870: Loss 0.004186900076038109\n",
      "Iter 871: Loss 0.004206277653128801\n",
      "Iter 872: Loss 0.004252669816007634\n",
      "Iter 873: Loss 0.004067997256676832\n",
      "Iter 874: Loss 0.004153418683720206\n",
      "Iter 875: Loss 0.004143462685529819\n",
      "Iter 873: Loss 0.004067997256676832\n",
      "Iter 874: Loss 0.004153418683720206\n",
      "Iter 875: Loss 0.004143462685529819\n",
      "Iter 876: Loss 0.004327366689959924\n",
      "Iter 877: Loss 0.004233246078034361\n",
      "Iter 878: Loss 0.00416173382909474\n",
      "Iter 876: Loss 0.004327366689959924\n",
      "Iter 877: Loss 0.004233246078034361\n",
      "Iter 878: Loss 0.00416173382909474\n",
      "Iter 879: Loss 0.004228150297305779\n",
      "Iter 880: Loss 0.004220226329719711\n",
      "Iter 881: Loss 0.004280963581717181\n",
      "Iter 879: Loss 0.004228150297305779\n",
      "Iter 880: Loss 0.004220226329719711\n",
      "Iter 881: Loss 0.004280963581717181\n",
      "Iter 882: Loss 0.004085203844630076\n",
      "Iter 883: Loss 0.00410396657780021\n",
      "Iter 884: Loss 0.0041274734123976165\n",
      "Iter 882: Loss 0.004085203844630076\n",
      "Iter 883: Loss 0.00410396657780021\n",
      "Iter 884: Loss 0.0041274734123976165\n",
      "Iter 885: Loss 0.004007314731498917\n",
      "Iter 886: Loss 0.004274829894958618\n",
      "Iter 887: Loss 0.0042622603342204754\n",
      "Iter 885: Loss 0.004007314731498917\n",
      "Iter 886: Loss 0.004274829894958618\n",
      "Iter 887: Loss 0.0042622603342204754\n",
      "Iter 888: Loss 0.0042757769068796\n",
      "Iter 889: Loss 0.004059302830648518\n",
      "Iter 890: Loss 0.004122101142258939\n",
      "Iter 888: Loss 0.0042757769068796\n",
      "Iter 889: Loss 0.004059302830648518\n",
      "Iter 890: Loss 0.004122101142258939\n",
      "Iter 891: Loss 0.004009417669025962\n",
      "Iter 892: Loss 0.004197377644613118\n",
      "Iter 893: Loss 0.004252077338700285\n",
      "Iter 891: Loss 0.004009417669025962\n",
      "Iter 892: Loss 0.004197377644613118\n",
      "Iter 893: Loss 0.004252077338700285\n",
      "Iter 894: Loss 0.004048349852571468\n",
      "Iter 895: Loss 0.004153546696889424\n",
      "Iter 896: Loss 0.00424267241578854\n",
      "Iter 894: Loss 0.004048349852571468\n",
      "Iter 895: Loss 0.004153546696889424\n",
      "Iter 896: Loss 0.00424267241578854\n",
      "Iter 897: Loss 0.0040968944450576385\n",
      "Iter 898: Loss 0.004050247683496533\n",
      "Iter 899: Loss 0.004068913336047631\n",
      "Iter 897: Loss 0.0040968944450576385\n",
      "Iter 898: Loss 0.004050247683496533\n",
      "Iter 899: Loss 0.004068913336047631\n",
      "Iter 900: Loss 0.0041368012418765985\n",
      "Iter 901: Loss 0.004187651023179471\n",
      "Iter 902: Loss 0.003987947385944054\n",
      "Iter 900: Loss 0.0041368012418765985\n",
      "Iter 901: Loss 0.004187651023179471\n",
      "Iter 902: Loss 0.003987947385944054\n",
      "Iter 903: Loss 0.003990337520302413\n",
      "Iter 904: Loss 0.004107137402136645\n",
      "Iter 905: Loss 0.004122321953078706\n",
      "Iter 903: Loss 0.003990337520302413\n",
      "Iter 904: Loss 0.004107137402136645\n",
      "Iter 905: Loss 0.004122321953078706\n",
      "Iter 906: Loss 0.00409519363068297\n",
      "Iter 907: Loss 0.004095810378145077\n",
      "Iter 908: Loss 0.004107744631891003\n",
      "Iter 906: Loss 0.00409519363068297\n",
      "Iter 907: Loss 0.004095810378145077\n",
      "Iter 908: Loss 0.004107744631891003\n",
      "Iter 909: Loss 0.004084111211780541\n",
      "Iter 910: Loss 0.004002573009498581\n",
      "Iter 911: Loss 0.004059644040471304\n",
      "Iter 909: Loss 0.004084111211780541\n",
      "Iter 910: Loss 0.004002573009498581\n",
      "Iter 911: Loss 0.004059644040471304\n",
      "Iter 912: Loss 0.0041360893173370055\n",
      "Iter 913: Loss 0.004153463892832012\n",
      "Iter 914: Loss 0.0041084713088776065\n",
      "Iter 912: Loss 0.0041360893173370055\n",
      "Iter 913: Loss 0.004153463892832012\n",
      "Iter 914: Loss 0.0041084713088776065\n",
      "Iter 915: Loss 0.004054815707330456\n",
      "Iter 916: Loss 0.0040546096489577\n",
      "Iter 917: Loss 0.004000628065920162\n",
      "Iter 915: Loss 0.004054815707330456\n",
      "Iter 916: Loss 0.0040546096489577\n",
      "Iter 917: Loss 0.004000628065920162\n",
      "Iter 918: Loss 0.004164434002783008\n",
      "Iter 919: Loss 0.004008648638239878\n",
      "Iter 920: Loss 0.004110721770874754\n",
      "Iter 918: Loss 0.004164434002783008\n",
      "Iter 919: Loss 0.004008648638239878\n",
      "Iter 920: Loss 0.004110721770874754\n",
      "Iter 921: Loss 0.004071000093471504\n",
      "Iter 922: Loss 0.0039966082620525555\n",
      "Iter 923: Loss 0.004055222589336708\n",
      "Iter 921: Loss 0.004071000093471504\n",
      "Iter 922: Loss 0.0039966082620525555\n",
      "Iter 923: Loss 0.004055222589336708\n",
      "Iter 924: Loss 0.0039739570693817445\n",
      "Iter 925: Loss 0.003929129617656776\n",
      "Iter 926: Loss 0.0039035460192286325\n",
      "Iter 924: Loss 0.0039739570693817445\n",
      "Iter 925: Loss 0.003929129617656776\n",
      "Iter 926: Loss 0.0039035460192286325\n",
      "Iter 927: Loss 0.004075091280147225\n",
      "Iter 928: Loss 0.004066813729718298\n",
      "Iter 929: Loss 0.004062600716383396\n",
      "Iter 927: Loss 0.004075091280147225\n",
      "Iter 928: Loss 0.004066813729718298\n",
      "Iter 929: Loss 0.004062600716383396\n",
      "Iter 930: Loss 0.00397311879727179\n",
      "Iter 931: Loss 0.004005765724562837\n",
      "Iter 932: Loss 0.003982072818778946\n",
      "Iter 930: Loss 0.00397311879727179\n",
      "Iter 931: Loss 0.004005765724562837\n",
      "Iter 932: Loss 0.003982072818778946\n",
      "Iter 933: Loss 0.004047005952237371\n",
      "Iter 934: Loss 0.004070130650868673\n",
      "Iter 935: Loss 0.004066638603895724\n",
      "Iter 933: Loss 0.004047005952237371\n",
      "Iter 934: Loss 0.004070130650868673\n",
      "Iter 935: Loss 0.004066638603895724\n",
      "Iter 936: Loss 0.004092581019905989\n",
      "Iter 937: Loss 0.0041028315911511935\n",
      "Iter 938: Loss 0.004000935011995053\n",
      "Iter 936: Loss 0.004092581019905989\n",
      "Iter 937: Loss 0.0041028315911511935\n",
      "Iter 938: Loss 0.004000935011995053\n",
      "Iter 939: Loss 0.0038972926472951312\n",
      "Iter 940: Loss 0.004014141307381575\n",
      "Iter 941: Loss 0.00398751337847072\n",
      "Iter 939: Loss 0.0038972926472951312\n",
      "Iter 940: Loss 0.004014141307381575\n",
      "Iter 941: Loss 0.00398751337847072\n",
      "Iter 942: Loss 0.004131342836482796\n",
      "Iter 943: Loss 0.004014261706384594\n",
      "Iter 944: Loss 0.0038996149679857814\n",
      "Iter 942: Loss 0.004131342836482796\n",
      "Iter 943: Loss 0.004014261706384594\n",
      "Iter 944: Loss 0.0038996149679857814\n",
      "Iter 945: Loss 0.004050598411027067\n",
      "Iter 946: Loss 0.003938140031582343\n",
      "Iter 947: Loss 0.004018832109645455\n",
      "Iter 945: Loss 0.004050598411027067\n",
      "Iter 946: Loss 0.003938140031582343\n",
      "Iter 947: Loss 0.004018832109645455\n",
      "Iter 948: Loss 0.004017227424119047\n",
      "Iter 949: Loss 0.003946710489467232\n",
      "Iter 950: Loss 0.004093863055139721\n",
      "Iter 948: Loss 0.004017227424119047\n",
      "Iter 949: Loss 0.003946710489467232\n",
      "Iter 950: Loss 0.004093863055139721\n",
      "Iter 951: Loss 0.0038633375110740434\n",
      "Iter 952: Loss 0.004063026633805144\n",
      "Iter 953: Loss 0.0038448831516349623\n",
      "Iter 951: Loss 0.0038633375110740434\n",
      "Iter 952: Loss 0.004063026633805144\n",
      "Iter 953: Loss 0.0038448831516349623\n",
      "Iter 954: Loss 0.003899179770798978\n",
      "Iter 955: Loss 0.004065190008776393\n",
      "Iter 956: Loss 0.003988655265457853\n",
      "Iter 954: Loss 0.003899179770798978\n",
      "Iter 955: Loss 0.004065190008776393\n",
      "Iter 956: Loss 0.003988655265457853\n",
      "Iter 957: Loss 0.003998442324335704\n",
      "Iter 958: Loss 0.003987791295536978\n",
      "Iter 959: Loss 0.004054621546092385\n",
      "Iter 957: Loss 0.003998442324335704\n",
      "Iter 958: Loss 0.003987791295536978\n",
      "Iter 959: Loss 0.004054621546092385\n",
      "Iter 960: Loss 0.00395811246540732\n",
      "Iter 961: Loss 0.0037972908058090363\n",
      "Iter 962: Loss 0.003926777792072106\n",
      "Iter 960: Loss 0.00395811246540732\n",
      "Iter 961: Loss 0.0037972908058090363\n",
      "Iter 962: Loss 0.003926777792072106\n",
      "Iter 963: Loss 0.003901710053403934\n",
      "Iter 964: Loss 0.003934622762683861\n",
      "Iter 965: Loss 0.003944156174650212\n",
      "Iter 963: Loss 0.003901710053403934\n",
      "Iter 964: Loss 0.003934622762683861\n",
      "Iter 965: Loss 0.003944156174650212\n",
      "Iter 966: Loss 0.003909952388314192\n",
      "Iter 967: Loss 0.003943226294603176\n",
      "Iter 968: Loss 0.003944157126420986\n",
      "Iter 966: Loss 0.003909952388314192\n",
      "Iter 967: Loss 0.003943226294603176\n",
      "Iter 968: Loss 0.003944157126420986\n",
      "Iter 969: Loss 0.00397308358175312\n",
      "Iter 970: Loss 0.003670104249508795\n",
      "Iter 971: Loss 0.003891584640015623\n",
      "Iter 969: Loss 0.00397308358175312\n",
      "Iter 970: Loss 0.003670104249508795\n",
      "Iter 971: Loss 0.003891584640015623\n",
      "Iter 972: Loss 0.003705453016086967\n",
      "Iter 973: Loss 0.004033373263543714\n",
      "Iter 974: Loss 0.0038329505634878925\n",
      "Iter 972: Loss 0.003705453016086967\n",
      "Iter 973: Loss 0.004033373263543714\n",
      "Iter 974: Loss 0.0038329505634878925\n",
      "Iter 975: Loss 0.003967571877195926\n",
      "Iter 976: Loss 0.0038380696626004584\n",
      "Iter 977: Loss 0.0038751918160748813\n",
      "Iter 975: Loss 0.003967571877195926\n",
      "Iter 976: Loss 0.0038380696626004584\n",
      "Iter 977: Loss 0.0038751918160748813\n",
      "Iter 978: Loss 0.003825796816401377\n",
      "Iter 979: Loss 0.003951598546224202\n",
      "Iter 980: Loss 0.0038938103559726252\n",
      "Iter 978: Loss 0.003825796816401377\n",
      "Iter 979: Loss 0.003951598546224202\n",
      "Iter 980: Loss 0.0038938103559726252\n",
      "Iter 981: Loss 0.003824621379494429\n",
      "Iter 982: Loss 0.003968508657581078\n",
      "Iter 983: Loss 0.0038403560539443573\n",
      "Iter 981: Loss 0.003824621379494429\n",
      "Iter 982: Loss 0.003968508657581078\n",
      "Iter 983: Loss 0.0038403560539443573\n",
      "Iter 984: Loss 0.0039761009330521084\n",
      "Iter 985: Loss 0.0038175618577146243\n",
      "Iter 986: Loss 0.003884043522223741\n",
      "Iter 984: Loss 0.0039761009330521084\n",
      "Iter 985: Loss 0.0038175618577146243\n",
      "Iter 986: Loss 0.003884043522223741\n",
      "Iter 987: Loss 0.003872515436655985\n",
      "Iter 988: Loss 0.003900829665437192\n",
      "Iter 989: Loss 0.0036938283257855627\n",
      "Iter 987: Loss 0.003872515436655985\n",
      "Iter 988: Loss 0.003900829665437192\n",
      "Iter 989: Loss 0.0036938283257855627\n",
      "Iter 990: Loss 0.0037223759763493035\n",
      "Iter 991: Loss 0.003915278497570289\n",
      "Iter 992: Loss 0.003894447804449085\n",
      "Iter 990: Loss 0.0037223759763493035\n",
      "Iter 991: Loss 0.003915278497570289\n",
      "Iter 992: Loss 0.003894447804449085\n",
      "Iter 993: Loss 0.0038134392626033332\n",
      "Iter 994: Loss 0.003837902388886777\n",
      "Iter 995: Loss 0.0037033239048636125\n",
      "Iter 993: Loss 0.0038134392626033332\n",
      "Iter 994: Loss 0.003837902388886777\n",
      "Iter 995: Loss 0.0037033239048636125\n",
      "Iter 996: Loss 0.003874461332005179\n",
      "Iter 997: Loss 0.003757308819098863\n",
      "Iter 998: Loss 0.003939514864466623\n",
      "Iter 996: Loss 0.003874461332005179\n",
      "Iter 997: Loss 0.003757308819098863\n",
      "Iter 998: Loss 0.003939514864466623\n",
      "Iter 999: Loss 0.003899885032943147\n",
      "Iter 999: Loss 0.003899885032943147\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # model.eval()\n",
    "    # decoder.eval()\n",
    "    # emb_func.eval()\n",
    "    w = get_batch('train', batch_size)  # already on device\n",
    "    w_emb = emb_func(w)  # (B, T, C)\n",
    "    x0 = w_emb + 1.2 * torch.randn_like(w_emb)  # use randn_like for Gaussian noise\n",
    "    \n",
    "    total_loss = 0\n",
    "    # for t_step in range(T + 1):\n",
    "        # t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "        \n",
    "    t_tensor = torch.randint(1, T + 1, (batch_size,), device=device)\n",
    "    xt = fwd_sample(x0, t_tensor, alphas)\n",
    "    x0_cap = model(xt, t_tensor)\n",
    "    total_loss += F.mse_loss(x0_cap, x0)\n",
    "    \n",
    "    # Final step loss\n",
    "    t_one = torch.tensor([1] * batch_size, device=device)\n",
    "    total_loss += F.mse_loss(model(fwd_sample(x0, t_one, alphas), t_one), w_emb)\n",
    "\n",
    "    # Decoder cross-entropy loss\n",
    "    logits = decoder(x0)  # (B, T, V)\n",
    "    V = config.n_vocab\n",
    "    logits_flat = logits.view(-1, V)  # (B*T, V)\n",
    "    targets_flat = w.view(-1)  # (B*T,)\n",
    "    total_loss += F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "    optimizer_model.zero_grad(set_to_none=True)\n",
    "    optimizer_model_decoder.zero_grad(set_to_none=True)\n",
    "    optimizer_emb.zero_grad(set_to_none=True)\n",
    "    total_loss.backward()\n",
    "    optimizer_model.step()\n",
    "    optimizer_model_decoder.step()\n",
    "    optimizer_emb.step()\n",
    "    \n",
    "    print(f\"Iter {iter}: Loss {total_loss.item() / (T + 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc432a96",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad887e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reverse diffusion inference with clamping...\n",
      "Denoising step 1000/1000\n",
      "Denoising step 999/1000\n",
      "Denoising step 998/1000\n",
      "Denoising step 997/1000\n",
      "Denoising step 996/1000\n",
      "Denoising step 995/1000\n",
      "Denoising step 994/1000\n",
      "Denoising step 993/1000\n",
      "Denoising step 992/1000\n",
      "Denoising step 991/1000\n",
      "Denoising step 990/1000\n",
      "Denoising step 989/1000\n",
      "Denoising step 988/1000\n",
      "Denoising step 987/1000\n",
      "Denoising step 986/1000\n",
      "Denoising step 985/1000\n",
      "Denoising step 984/1000\n",
      "Denoising step 983/1000\n",
      "Denoising step 982/1000\n",
      "Denoising step 981/1000\n",
      "Denoising step 980/1000\n",
      "Denoising step 979/1000\n",
      "Denoising step 978/1000\n",
      "Denoising step 977/1000\n",
      "Denoising step 976/1000\n",
      "Denoising step 975/1000\n",
      "Denoising step 974/1000\n",
      "Denoising step 973/1000\n",
      "Denoising step 972/1000\n",
      "Denoising step 971/1000\n",
      "Denoising step 970/1000\n",
      "Denoising step 969/1000\n",
      "Denoising step 968/1000\n",
      "Denoising step 967/1000\n",
      "Denoising step 966/1000\n",
      "Denoising step 965/1000\n",
      "Denoising step 964/1000\n",
      "Denoising step 963/1000\n",
      "Denoising step 962/1000\n",
      "Denoising step 961/1000\n",
      "Denoising step 960/1000\n",
      "Denoising step 959/1000\n",
      "Denoising step 958/1000\n",
      "Denoising step 957/1000\n",
      "Denoising step 956/1000\n",
      "Denoising step 955/1000\n",
      "Denoising step 954/1000\n",
      "Denoising step 953/1000\n",
      "Denoising step 952/1000\n",
      "Denoising step 951/1000\n",
      "Denoising step 950/1000\n",
      "Denoising step 949/1000\n",
      "Denoising step 948/1000\n",
      "Denoising step 947/1000\n",
      "Denoising step 946/1000\n",
      "Denoising step 945/1000\n",
      "Denoising step 944/1000\n",
      "Denoising step 943/1000\n",
      "Denoising step 942/1000\n",
      "Denoising step 941/1000\n",
      "Denoising step 940/1000\n",
      "Denoising step 939/1000\n",
      "Denoising step 938/1000\n",
      "Denoising step 937/1000\n",
      "Denoising step 936/1000\n",
      "Denoising step 935/1000\n",
      "Denoising step 934/1000\n",
      "Denoising step 933/1000\n",
      "Denoising step 932/1000\n",
      "Denoising step 931/1000\n",
      "Denoising step 930/1000\n",
      "Denoising step 929/1000\n",
      "Denoising step 928/1000\n",
      "Denoising step 927/1000\n",
      "Denoising step 926/1000\n",
      "Denoising step 925/1000\n",
      "Denoising step 924/1000\n",
      "Denoising step 923/1000\n",
      "Denoising step 922/1000\n",
      "Denoising step 921/1000\n",
      "Denoising step 920/1000\n",
      "Denoising step 919/1000\n",
      "Denoising step 918/1000\n",
      "Denoising step 917/1000\n",
      "Denoising step 916/1000\n",
      "Denoising step 915/1000\n",
      "Denoising step 914/1000\n",
      "Denoising step 913/1000\n",
      "Denoising step 912/1000\n",
      "Denoising step 911/1000\n",
      "Denoising step 910/1000\n",
      "Denoising step 909/1000\n",
      "Denoising step 908/1000\n",
      "Denoising step 907/1000\n",
      "Denoising step 906/1000\n",
      "Denoising step 905/1000\n",
      "Denoising step 904/1000\n",
      "Denoising step 903/1000\n",
      "Denoising step 902/1000\n",
      "Denoising step 901/1000\n",
      "Denoising step 900/1000\n",
      "Denoising step 899/1000\n",
      "Denoising step 898/1000\n",
      "Denoising step 897/1000\n",
      "Denoising step 896/1000\n",
      "Denoising step 895/1000\n",
      "Denoising step 894/1000\n",
      "Denoising step 893/1000\n",
      "Denoising step 892/1000\n",
      "Denoising step 891/1000\n",
      "Denoising step 890/1000\n",
      "Denoising step 889/1000\n",
      "Denoising step 888/1000\n",
      "Denoising step 887/1000\n",
      "Denoising step 886/1000\n",
      "Denoising step 885/1000\n",
      "Denoising step 884/1000\n",
      "Denoising step 883/1000\n",
      "Denoising step 882/1000\n",
      "Denoising step 881/1000\n",
      "Denoising step 880/1000\n",
      "Denoising step 879/1000\n",
      "Denoising step 878/1000\n",
      "Denoising step 877/1000\n",
      "Denoising step 876/1000\n",
      "Denoising step 875/1000\n",
      "Denoising step 874/1000\n",
      "Denoising step 873/1000\n",
      "Denoising step 872/1000\n",
      "Denoising step 871/1000\n",
      "Denoising step 870/1000\n",
      "Denoising step 869/1000\n",
      "Denoising step 868/1000\n",
      "Denoising step 867/1000\n",
      "Denoising step 866/1000\n",
      "Denoising step 865/1000\n",
      "Denoising step 864/1000\n",
      "Denoising step 863/1000\n",
      "Denoising step 862/1000\n",
      "Denoising step 861/1000\n",
      "Denoising step 860/1000\n",
      "Denoising step 859/1000\n",
      "Denoising step 858/1000\n",
      "Denoising step 857/1000\n",
      "Denoising step 856/1000\n",
      "Denoising step 855/1000\n",
      "Denoising step 854/1000\n",
      "Denoising step 853/1000\n",
      "Denoising step 852/1000\n",
      "Denoising step 851/1000\n",
      "Denoising step 850/1000\n",
      "Denoising step 849/1000\n",
      "Denoising step 848/1000\n",
      "Denoising step 847/1000\n",
      "Denoising step 846/1000\n",
      "Denoising step 845/1000\n",
      "Denoising step 844/1000\n",
      "Denoising step 843/1000\n",
      "Denoising step 842/1000\n",
      "Denoising step 841/1000\n",
      "Denoising step 840/1000\n",
      "Denoising step 839/1000\n",
      "Denoising step 838/1000\n",
      "Denoising step 837/1000\n",
      "Denoising step 836/1000\n",
      "Denoising step 919/1000\n",
      "Denoising step 918/1000\n",
      "Denoising step 917/1000\n",
      "Denoising step 916/1000\n",
      "Denoising step 915/1000\n",
      "Denoising step 914/1000\n",
      "Denoising step 913/1000\n",
      "Denoising step 912/1000\n",
      "Denoising step 911/1000\n",
      "Denoising step 910/1000\n",
      "Denoising step 909/1000\n",
      "Denoising step 908/1000\n",
      "Denoising step 907/1000\n",
      "Denoising step 906/1000\n",
      "Denoising step 905/1000\n",
      "Denoising step 904/1000\n",
      "Denoising step 903/1000\n",
      "Denoising step 902/1000\n",
      "Denoising step 901/1000\n",
      "Denoising step 900/1000\n",
      "Denoising step 899/1000\n",
      "Denoising step 898/1000\n",
      "Denoising step 897/1000\n",
      "Denoising step 896/1000\n",
      "Denoising step 895/1000\n",
      "Denoising step 894/1000\n",
      "Denoising step 893/1000\n",
      "Denoising step 892/1000\n",
      "Denoising step 891/1000\n",
      "Denoising step 890/1000\n",
      "Denoising step 889/1000\n",
      "Denoising step 888/1000\n",
      "Denoising step 887/1000\n",
      "Denoising step 886/1000\n",
      "Denoising step 885/1000\n",
      "Denoising step 884/1000\n",
      "Denoising step 883/1000\n",
      "Denoising step 882/1000\n",
      "Denoising step 881/1000\n",
      "Denoising step 880/1000\n",
      "Denoising step 879/1000\n",
      "Denoising step 878/1000\n",
      "Denoising step 877/1000\n",
      "Denoising step 876/1000\n",
      "Denoising step 875/1000\n",
      "Denoising step 874/1000\n",
      "Denoising step 873/1000\n",
      "Denoising step 872/1000\n",
      "Denoising step 871/1000\n",
      "Denoising step 870/1000\n",
      "Denoising step 869/1000\n",
      "Denoising step 868/1000\n",
      "Denoising step 867/1000\n",
      "Denoising step 866/1000\n",
      "Denoising step 865/1000\n",
      "Denoising step 864/1000\n",
      "Denoising step 863/1000\n",
      "Denoising step 862/1000\n",
      "Denoising step 861/1000\n",
      "Denoising step 860/1000\n",
      "Denoising step 859/1000\n",
      "Denoising step 858/1000\n",
      "Denoising step 857/1000\n",
      "Denoising step 856/1000\n",
      "Denoising step 855/1000\n",
      "Denoising step 854/1000\n",
      "Denoising step 853/1000\n",
      "Denoising step 852/1000\n",
      "Denoising step 851/1000\n",
      "Denoising step 850/1000\n",
      "Denoising step 849/1000\n",
      "Denoising step 848/1000\n",
      "Denoising step 847/1000\n",
      "Denoising step 846/1000\n",
      "Denoising step 845/1000\n",
      "Denoising step 844/1000\n",
      "Denoising step 843/1000\n",
      "Denoising step 842/1000\n",
      "Denoising step 841/1000\n",
      "Denoising step 840/1000\n",
      "Denoising step 839/1000\n",
      "Denoising step 838/1000\n",
      "Denoising step 837/1000\n",
      "Denoising step 836/1000\n",
      "Denoising step 835/1000\n",
      "Denoising step 834/1000\n",
      "Denoising step 833/1000\n",
      "Denoising step 832/1000\n",
      "Denoising step 831/1000\n",
      "Denoising step 830/1000\n",
      "Denoising step 829/1000\n",
      "Denoising step 828/1000\n",
      "Denoising step 827/1000\n",
      "Denoising step 826/1000\n",
      "Denoising step 825/1000\n",
      "Denoising step 824/1000\n",
      "Denoising step 823/1000\n",
      "Denoising step 822/1000\n",
      "Denoising step 821/1000\n",
      "Denoising step 820/1000\n",
      "Denoising step 819/1000\n",
      "Denoising step 818/1000\n",
      "Denoising step 817/1000\n",
      "Denoising step 816/1000\n",
      "Denoising step 815/1000\n",
      "Denoising step 814/1000\n",
      "Denoising step 813/1000\n",
      "Denoising step 812/1000\n",
      "Denoising step 811/1000\n",
      "Denoising step 810/1000\n",
      "Denoising step 809/1000\n",
      "Denoising step 808/1000\n",
      "Denoising step 807/1000\n",
      "Denoising step 806/1000\n",
      "Denoising step 805/1000\n",
      "Denoising step 804/1000\n",
      "Denoising step 803/1000\n",
      "Denoising step 802/1000\n",
      "Denoising step 801/1000\n",
      "Denoising step 800/1000\n",
      "Denoising step 799/1000\n",
      "Denoising step 798/1000\n",
      "Denoising step 797/1000\n",
      "Denoising step 796/1000\n",
      "Denoising step 795/1000\n",
      "Denoising step 794/1000\n",
      "Denoising step 793/1000\n",
      "Denoising step 792/1000\n",
      "Denoising step 791/1000\n",
      "Denoising step 790/1000\n",
      "Denoising step 789/1000\n",
      "Denoising step 788/1000\n",
      "Denoising step 787/1000\n",
      "Denoising step 786/1000\n",
      "Denoising step 785/1000\n",
      "Denoising step 784/1000\n",
      "Denoising step 783/1000\n",
      "Denoising step 782/1000\n",
      "Denoising step 781/1000\n",
      "Denoising step 780/1000\n",
      "Denoising step 779/1000\n",
      "Denoising step 778/1000\n",
      "Denoising step 777/1000\n",
      "Denoising step 776/1000\n",
      "Denoising step 775/1000\n",
      "Denoising step 774/1000\n",
      "Denoising step 773/1000\n",
      "Denoising step 772/1000\n",
      "Denoising step 771/1000\n",
      "Denoising step 770/1000\n",
      "Denoising step 769/1000\n",
      "Denoising step 768/1000\n",
      "Denoising step 767/1000\n",
      "Denoising step 766/1000\n",
      "Denoising step 765/1000\n",
      "Denoising step 764/1000\n",
      "Denoising step 763/1000\n",
      "Denoising step 762/1000\n",
      "Denoising step 761/1000\n",
      "Denoising step 760/1000\n",
      "Denoising step 759/1000\n",
      "Denoising step 758/1000\n",
      "Denoising step 757/1000\n",
      "Denoising step 756/1000\n",
      "Denoising step 755/1000\n",
      "Denoising step 754/1000\n",
      "Denoising step 753/1000\n",
      "Denoising step 752/1000\n",
      "Denoising step 751/1000\n",
      "Denoising step 750/1000\n",
      "Denoising step 749/1000\n",
      "Denoising step 748/1000\n",
      "Denoising step 747/1000\n",
      "Denoising step 746/1000\n",
      "Denoising step 745/1000\n",
      "Denoising step 744/1000\n",
      "Denoising step 743/1000\n",
      "Denoising step 835/1000\n",
      "Denoising step 834/1000\n",
      "Denoising step 833/1000\n",
      "Denoising step 832/1000\n",
      "Denoising step 831/1000\n",
      "Denoising step 830/1000\n",
      "Denoising step 829/1000\n",
      "Denoising step 828/1000\n",
      "Denoising step 827/1000\n",
      "Denoising step 826/1000\n",
      "Denoising step 825/1000\n",
      "Denoising step 824/1000\n",
      "Denoising step 823/1000\n",
      "Denoising step 822/1000\n",
      "Denoising step 821/1000\n",
      "Denoising step 820/1000\n",
      "Denoising step 819/1000\n",
      "Denoising step 818/1000\n",
      "Denoising step 817/1000\n",
      "Denoising step 816/1000\n",
      "Denoising step 815/1000\n",
      "Denoising step 814/1000\n",
      "Denoising step 813/1000\n",
      "Denoising step 812/1000\n",
      "Denoising step 811/1000\n",
      "Denoising step 810/1000\n",
      "Denoising step 809/1000\n",
      "Denoising step 808/1000\n",
      "Denoising step 807/1000\n",
      "Denoising step 806/1000\n",
      "Denoising step 805/1000\n",
      "Denoising step 804/1000\n",
      "Denoising step 803/1000\n",
      "Denoising step 802/1000\n",
      "Denoising step 801/1000\n",
      "Denoising step 800/1000\n",
      "Denoising step 799/1000\n",
      "Denoising step 798/1000\n",
      "Denoising step 797/1000\n",
      "Denoising step 796/1000\n",
      "Denoising step 795/1000\n",
      "Denoising step 794/1000\n",
      "Denoising step 793/1000\n",
      "Denoising step 792/1000\n",
      "Denoising step 791/1000\n",
      "Denoising step 790/1000\n",
      "Denoising step 789/1000\n",
      "Denoising step 788/1000\n",
      "Denoising step 787/1000\n",
      "Denoising step 786/1000\n",
      "Denoising step 785/1000\n",
      "Denoising step 784/1000\n",
      "Denoising step 783/1000\n",
      "Denoising step 782/1000\n",
      "Denoising step 781/1000\n",
      "Denoising step 780/1000\n",
      "Denoising step 779/1000\n",
      "Denoising step 778/1000\n",
      "Denoising step 777/1000\n",
      "Denoising step 776/1000\n",
      "Denoising step 775/1000\n",
      "Denoising step 774/1000\n",
      "Denoising step 773/1000\n",
      "Denoising step 772/1000\n",
      "Denoising step 771/1000\n",
      "Denoising step 770/1000\n",
      "Denoising step 769/1000\n",
      "Denoising step 768/1000\n",
      "Denoising step 767/1000\n",
      "Denoising step 766/1000\n",
      "Denoising step 765/1000\n",
      "Denoising step 764/1000\n",
      "Denoising step 763/1000\n",
      "Denoising step 762/1000\n",
      "Denoising step 761/1000\n",
      "Denoising step 760/1000\n",
      "Denoising step 759/1000\n",
      "Denoising step 758/1000\n",
      "Denoising step 757/1000\n",
      "Denoising step 756/1000\n",
      "Denoising step 755/1000\n",
      "Denoising step 754/1000\n",
      "Denoising step 753/1000\n",
      "Denoising step 752/1000\n",
      "Denoising step 751/1000\n",
      "Denoising step 750/1000\n",
      "Denoising step 749/1000\n",
      "Denoising step 748/1000\n",
      "Denoising step 747/1000\n",
      "Denoising step 746/1000\n",
      "Denoising step 745/1000\n",
      "Denoising step 744/1000\n",
      "Denoising step 743/1000\n",
      "Denoising step 742/1000\n",
      "Denoising step 741/1000\n",
      "Denoising step 740/1000\n",
      "Denoising step 739/1000\n",
      "Denoising step 738/1000\n",
      "Denoising step 737/1000\n",
      "Denoising step 736/1000\n",
      "Denoising step 735/1000\n",
      "Denoising step 734/1000\n",
      "Denoising step 733/1000\n",
      "Denoising step 732/1000\n",
      "Denoising step 731/1000\n",
      "Denoising step 730/1000\n",
      "Denoising step 729/1000\n",
      "Denoising step 728/1000\n",
      "Denoising step 727/1000\n",
      "Denoising step 726/1000\n",
      "Denoising step 725/1000\n",
      "Denoising step 724/1000\n",
      "Denoising step 723/1000\n",
      "Denoising step 722/1000\n",
      "Denoising step 721/1000\n",
      "Denoising step 720/1000\n",
      "Denoising step 719/1000\n",
      "Denoising step 718/1000\n",
      "Denoising step 717/1000\n",
      "Denoising step 716/1000\n",
      "Denoising step 715/1000\n",
      "Denoising step 714/1000\n",
      "Denoising step 713/1000\n",
      "Denoising step 712/1000\n",
      "Denoising step 711/1000\n",
      "Denoising step 710/1000\n",
      "Denoising step 709/1000\n",
      "Denoising step 708/1000\n",
      "Denoising step 707/1000\n",
      "Denoising step 706/1000\n",
      "Denoising step 705/1000\n",
      "Denoising step 704/1000\n",
      "Denoising step 703/1000\n",
      "Denoising step 702/1000\n",
      "Denoising step 701/1000\n",
      "Denoising step 700/1000\n",
      "Denoising step 699/1000\n",
      "Denoising step 698/1000\n",
      "Denoising step 697/1000\n",
      "Denoising step 696/1000\n",
      "Denoising step 695/1000\n",
      "Denoising step 694/1000\n",
      "Denoising step 693/1000\n",
      "Denoising step 692/1000\n",
      "Denoising step 691/1000\n",
      "Denoising step 690/1000\n",
      "Denoising step 689/1000\n",
      "Denoising step 688/1000\n",
      "Denoising step 687/1000\n",
      "Denoising step 686/1000\n",
      "Denoising step 685/1000\n",
      "Denoising step 684/1000\n",
      "Denoising step 683/1000\n",
      "Denoising step 682/1000\n",
      "Denoising step 681/1000\n",
      "Denoising step 680/1000\n",
      "Denoising step 679/1000\n",
      "Denoising step 678/1000\n",
      "Denoising step 677/1000\n",
      "Denoising step 676/1000\n",
      "Denoising step 675/1000\n",
      "Denoising step 674/1000\n",
      "Denoising step 673/1000\n",
      "Denoising step 672/1000\n",
      "Denoising step 671/1000\n",
      "Denoising step 670/1000\n",
      "Denoising step 669/1000\n",
      "Denoising step 668/1000\n",
      "Denoising step 667/1000\n",
      "Denoising step 666/1000\n",
      "Denoising step 665/1000\n",
      "Denoising step 664/1000\n",
      "Denoising step 663/1000\n",
      "Denoising step 662/1000\n",
      "Denoising step 661/1000\n",
      "Denoising step 660/1000\n",
      "Denoising step 659/1000\n",
      "Denoising step 658/1000\n",
      "Denoising step 657/1000\n",
      "Denoising step 656/1000\n",
      "Denoising step 655/1000\n",
      "Denoising step 654/1000\n",
      "Denoising step 653/1000\n",
      "Denoising step 652/1000\n",
      "Denoising step 651/1000\n",
      "Denoising step 650/1000\n",
      "Denoising step 649/1000\n",
      "Denoising step 648/1000\n",
      "Denoising step 647/1000\n",
      "Denoising step 742/1000\n",
      "Denoising step 741/1000\n",
      "Denoising step 740/1000\n",
      "Denoising step 739/1000\n",
      "Denoising step 738/1000\n",
      "Denoising step 737/1000\n",
      "Denoising step 736/1000\n",
      "Denoising step 735/1000\n",
      "Denoising step 734/1000\n",
      "Denoising step 733/1000\n",
      "Denoising step 732/1000\n",
      "Denoising step 731/1000\n",
      "Denoising step 730/1000\n",
      "Denoising step 729/1000\n",
      "Denoising step 728/1000\n",
      "Denoising step 727/1000\n",
      "Denoising step 726/1000\n",
      "Denoising step 725/1000\n",
      "Denoising step 724/1000\n",
      "Denoising step 723/1000\n",
      "Denoising step 722/1000\n",
      "Denoising step 721/1000\n",
      "Denoising step 720/1000\n",
      "Denoising step 719/1000\n",
      "Denoising step 718/1000\n",
      "Denoising step 717/1000\n",
      "Denoising step 716/1000\n",
      "Denoising step 715/1000\n",
      "Denoising step 714/1000\n",
      "Denoising step 713/1000\n",
      "Denoising step 712/1000\n",
      "Denoising step 711/1000\n",
      "Denoising step 710/1000\n",
      "Denoising step 709/1000\n",
      "Denoising step 708/1000\n",
      "Denoising step 707/1000\n",
      "Denoising step 706/1000\n",
      "Denoising step 705/1000\n",
      "Denoising step 704/1000\n",
      "Denoising step 703/1000\n",
      "Denoising step 702/1000\n",
      "Denoising step 701/1000\n",
      "Denoising step 700/1000\n",
      "Denoising step 699/1000\n",
      "Denoising step 698/1000\n",
      "Denoising step 697/1000\n",
      "Denoising step 696/1000\n",
      "Denoising step 695/1000\n",
      "Denoising step 694/1000\n",
      "Denoising step 693/1000\n",
      "Denoising step 692/1000\n",
      "Denoising step 691/1000\n",
      "Denoising step 690/1000\n",
      "Denoising step 689/1000\n",
      "Denoising step 688/1000\n",
      "Denoising step 687/1000\n",
      "Denoising step 686/1000\n",
      "Denoising step 685/1000\n",
      "Denoising step 684/1000\n",
      "Denoising step 683/1000\n",
      "Denoising step 682/1000\n",
      "Denoising step 681/1000\n",
      "Denoising step 680/1000\n",
      "Denoising step 679/1000\n",
      "Denoising step 678/1000\n",
      "Denoising step 677/1000\n",
      "Denoising step 676/1000\n",
      "Denoising step 675/1000\n",
      "Denoising step 674/1000\n",
      "Denoising step 673/1000\n",
      "Denoising step 672/1000\n",
      "Denoising step 671/1000\n",
      "Denoising step 670/1000\n",
      "Denoising step 669/1000\n",
      "Denoising step 668/1000\n",
      "Denoising step 667/1000\n",
      "Denoising step 666/1000\n",
      "Denoising step 665/1000\n",
      "Denoising step 664/1000\n",
      "Denoising step 663/1000\n",
      "Denoising step 662/1000\n",
      "Denoising step 661/1000\n",
      "Denoising step 660/1000\n",
      "Denoising step 659/1000\n",
      "Denoising step 658/1000\n",
      "Denoising step 657/1000\n",
      "Denoising step 656/1000\n",
      "Denoising step 655/1000\n",
      "Denoising step 654/1000\n",
      "Denoising step 653/1000\n",
      "Denoising step 652/1000\n",
      "Denoising step 651/1000\n",
      "Denoising step 650/1000\n",
      "Denoising step 649/1000\n",
      "Denoising step 648/1000\n",
      "Denoising step 647/1000\n",
      "Denoising step 646/1000\n",
      "Denoising step 645/1000\n",
      "Denoising step 644/1000\n",
      "Denoising step 643/1000\n",
      "Denoising step 642/1000\n",
      "Denoising step 641/1000\n",
      "Denoising step 640/1000\n",
      "Denoising step 639/1000\n",
      "Denoising step 638/1000\n",
      "Denoising step 637/1000\n",
      "Denoising step 636/1000\n",
      "Denoising step 635/1000\n",
      "Denoising step 634/1000\n",
      "Denoising step 633/1000\n",
      "Denoising step 632/1000\n",
      "Denoising step 631/1000\n",
      "Denoising step 630/1000\n",
      "Denoising step 629/1000\n",
      "Denoising step 628/1000\n",
      "Denoising step 627/1000\n",
      "Denoising step 626/1000\n",
      "Denoising step 625/1000\n",
      "Denoising step 624/1000\n",
      "Denoising step 623/1000\n",
      "Denoising step 622/1000\n",
      "Denoising step 621/1000\n",
      "Denoising step 620/1000\n",
      "Denoising step 619/1000\n",
      "Denoising step 618/1000\n",
      "Denoising step 617/1000\n",
      "Denoising step 616/1000\n",
      "Denoising step 615/1000\n",
      "Denoising step 614/1000\n",
      "Denoising step 613/1000\n",
      "Denoising step 612/1000\n",
      "Denoising step 611/1000\n",
      "Denoising step 610/1000\n",
      "Denoising step 609/1000\n",
      "Denoising step 608/1000\n",
      "Denoising step 607/1000\n",
      "Denoising step 606/1000\n",
      "Denoising step 605/1000\n",
      "Denoising step 604/1000\n",
      "Denoising step 603/1000\n",
      "Denoising step 602/1000\n",
      "Denoising step 601/1000\n",
      "Denoising step 600/1000\n",
      "Denoising step 599/1000\n",
      "Denoising step 598/1000\n",
      "Denoising step 597/1000\n",
      "Denoising step 596/1000\n",
      "Denoising step 595/1000\n",
      "Denoising step 594/1000\n",
      "Denoising step 593/1000\n",
      "Denoising step 592/1000\n",
      "Denoising step 591/1000\n",
      "Denoising step 590/1000\n",
      "Denoising step 589/1000\n",
      "Denoising step 588/1000\n",
      "Denoising step 587/1000\n",
      "Denoising step 586/1000\n",
      "Denoising step 585/1000\n",
      "Denoising step 584/1000\n",
      "Denoising step 583/1000\n",
      "Denoising step 582/1000\n",
      "Denoising step 581/1000\n",
      "Denoising step 580/1000\n",
      "Denoising step 579/1000\n",
      "Denoising step 578/1000\n",
      "Denoising step 577/1000\n",
      "Denoising step 576/1000\n",
      "Denoising step 575/1000\n",
      "Denoising step 574/1000\n",
      "Denoising step 573/1000\n",
      "Denoising step 572/1000\n",
      "Denoising step 571/1000\n",
      "Denoising step 570/1000\n",
      "Denoising step 569/1000\n",
      "Denoising step 568/1000\n",
      "Denoising step 567/1000\n",
      "Denoising step 566/1000\n",
      "Denoising step 565/1000\n",
      "Denoising step 564/1000\n",
      "Denoising step 563/1000\n",
      "Denoising step 562/1000\n",
      "Denoising step 561/1000\n",
      "Denoising step 560/1000\n",
      "Denoising step 559/1000\n",
      "Denoising step 558/1000\n",
      "Denoising step 557/1000\n",
      "Denoising step 556/1000\n",
      "Denoising step 555/1000\n",
      "Denoising step 554/1000\n",
      "Denoising step 553/1000\n",
      "Denoising step 646/1000\n",
      "Denoising step 645/1000\n",
      "Denoising step 644/1000\n",
      "Denoising step 643/1000\n",
      "Denoising step 642/1000\n",
      "Denoising step 641/1000\n",
      "Denoising step 640/1000\n",
      "Denoising step 639/1000\n",
      "Denoising step 638/1000\n",
      "Denoising step 637/1000\n",
      "Denoising step 636/1000\n",
      "Denoising step 635/1000\n",
      "Denoising step 634/1000\n",
      "Denoising step 633/1000\n",
      "Denoising step 632/1000\n",
      "Denoising step 631/1000\n",
      "Denoising step 630/1000\n",
      "Denoising step 629/1000\n",
      "Denoising step 628/1000\n",
      "Denoising step 627/1000\n",
      "Denoising step 626/1000\n",
      "Denoising step 625/1000\n",
      "Denoising step 624/1000\n",
      "Denoising step 623/1000\n",
      "Denoising step 622/1000\n",
      "Denoising step 621/1000\n",
      "Denoising step 620/1000\n",
      "Denoising step 619/1000\n",
      "Denoising step 618/1000\n",
      "Denoising step 617/1000\n",
      "Denoising step 616/1000\n",
      "Denoising step 615/1000\n",
      "Denoising step 614/1000\n",
      "Denoising step 613/1000\n",
      "Denoising step 612/1000\n",
      "Denoising step 611/1000\n",
      "Denoising step 610/1000\n",
      "Denoising step 609/1000\n",
      "Denoising step 608/1000\n",
      "Denoising step 607/1000\n",
      "Denoising step 606/1000\n",
      "Denoising step 605/1000\n",
      "Denoising step 604/1000\n",
      "Denoising step 603/1000\n",
      "Denoising step 602/1000\n",
      "Denoising step 601/1000\n",
      "Denoising step 600/1000\n",
      "Denoising step 599/1000\n",
      "Denoising step 598/1000\n",
      "Denoising step 597/1000\n",
      "Denoising step 596/1000\n",
      "Denoising step 595/1000\n",
      "Denoising step 594/1000\n",
      "Denoising step 593/1000\n",
      "Denoising step 592/1000\n",
      "Denoising step 591/1000\n",
      "Denoising step 590/1000\n",
      "Denoising step 589/1000\n",
      "Denoising step 588/1000\n",
      "Denoising step 587/1000\n",
      "Denoising step 586/1000\n",
      "Denoising step 585/1000\n",
      "Denoising step 584/1000\n",
      "Denoising step 583/1000\n",
      "Denoising step 582/1000\n",
      "Denoising step 581/1000\n",
      "Denoising step 580/1000\n",
      "Denoising step 579/1000\n",
      "Denoising step 578/1000\n",
      "Denoising step 577/1000\n",
      "Denoising step 576/1000\n",
      "Denoising step 575/1000\n",
      "Denoising step 574/1000\n",
      "Denoising step 573/1000\n",
      "Denoising step 572/1000\n",
      "Denoising step 571/1000\n",
      "Denoising step 570/1000\n",
      "Denoising step 569/1000\n",
      "Denoising step 568/1000\n",
      "Denoising step 567/1000\n",
      "Denoising step 566/1000\n",
      "Denoising step 565/1000\n",
      "Denoising step 564/1000\n",
      "Denoising step 563/1000\n",
      "Denoising step 562/1000\n",
      "Denoising step 561/1000\n",
      "Denoising step 560/1000\n",
      "Denoising step 559/1000\n",
      "Denoising step 558/1000\n",
      "Denoising step 557/1000\n",
      "Denoising step 556/1000\n",
      "Denoising step 555/1000\n",
      "Denoising step 554/1000\n",
      "Denoising step 553/1000\n",
      "Denoising step 552/1000\n",
      "Denoising step 551/1000\n",
      "Denoising step 550/1000\n",
      "Denoising step 549/1000\n",
      "Denoising step 548/1000\n",
      "Denoising step 547/1000\n",
      "Denoising step 546/1000\n",
      "Denoising step 545/1000\n",
      "Denoising step 544/1000\n",
      "Denoising step 543/1000\n",
      "Denoising step 542/1000\n",
      "Denoising step 541/1000\n",
      "Denoising step 540/1000\n",
      "Denoising step 539/1000\n",
      "Denoising step 538/1000\n",
      "Denoising step 537/1000\n",
      "Denoising step 536/1000\n",
      "Denoising step 535/1000\n",
      "Denoising step 534/1000\n",
      "Denoising step 533/1000\n",
      "Denoising step 532/1000\n",
      "Denoising step 531/1000\n",
      "Denoising step 530/1000\n",
      "Denoising step 529/1000\n",
      "Denoising step 528/1000\n",
      "Denoising step 527/1000\n",
      "Denoising step 526/1000\n",
      "Denoising step 525/1000\n",
      "Denoising step 524/1000\n",
      "Denoising step 523/1000\n",
      "Denoising step 522/1000\n",
      "Denoising step 521/1000\n",
      "Denoising step 520/1000\n",
      "Denoising step 519/1000\n",
      "Denoising step 518/1000\n",
      "Denoising step 517/1000\n",
      "Denoising step 516/1000\n",
      "Denoising step 515/1000\n",
      "Denoising step 514/1000\n",
      "Denoising step 513/1000\n",
      "Denoising step 512/1000\n",
      "Denoising step 511/1000\n",
      "Denoising step 510/1000\n",
      "Denoising step 509/1000\n",
      "Denoising step 508/1000\n",
      "Denoising step 507/1000\n",
      "Denoising step 506/1000\n",
      "Denoising step 505/1000\n",
      "Denoising step 504/1000\n",
      "Denoising step 503/1000\n",
      "Denoising step 502/1000\n",
      "Denoising step 501/1000\n",
      "Denoising step 500/1000\n",
      "Denoising step 499/1000\n",
      "Denoising step 498/1000\n",
      "Denoising step 497/1000\n",
      "Denoising step 496/1000\n",
      "Denoising step 495/1000\n",
      "Denoising step 494/1000\n",
      "Denoising step 493/1000\n",
      "Denoising step 492/1000\n",
      "Denoising step 491/1000\n",
      "Denoising step 490/1000\n",
      "Denoising step 489/1000\n",
      "Denoising step 488/1000\n",
      "Denoising step 487/1000\n",
      "Denoising step 486/1000\n",
      "Denoising step 485/1000\n",
      "Denoising step 484/1000\n",
      "Denoising step 483/1000\n",
      "Denoising step 482/1000\n",
      "Denoising step 481/1000\n",
      "Denoising step 480/1000\n",
      "Denoising step 479/1000\n",
      "Denoising step 478/1000\n",
      "Denoising step 477/1000\n",
      "Denoising step 476/1000\n",
      "Denoising step 475/1000\n",
      "Denoising step 474/1000\n",
      "Denoising step 473/1000\n",
      "Denoising step 472/1000\n",
      "Denoising step 471/1000\n",
      "Denoising step 470/1000\n",
      "Denoising step 469/1000\n",
      "Denoising step 468/1000\n",
      "Denoising step 467/1000\n",
      "Denoising step 466/1000\n",
      "Denoising step 465/1000\n",
      "Denoising step 464/1000\n",
      "Denoising step 463/1000\n",
      "Denoising step 462/1000\n",
      "Denoising step 461/1000\n",
      "Denoising step 552/1000\n",
      "Denoising step 551/1000\n",
      "Denoising step 550/1000\n",
      "Denoising step 549/1000\n",
      "Denoising step 548/1000\n",
      "Denoising step 547/1000\n",
      "Denoising step 546/1000\n",
      "Denoising step 545/1000\n",
      "Denoising step 544/1000\n",
      "Denoising step 543/1000\n",
      "Denoising step 542/1000\n",
      "Denoising step 541/1000\n",
      "Denoising step 540/1000\n",
      "Denoising step 539/1000\n",
      "Denoising step 538/1000\n",
      "Denoising step 537/1000\n",
      "Denoising step 536/1000\n",
      "Denoising step 535/1000\n",
      "Denoising step 534/1000\n",
      "Denoising step 533/1000\n",
      "Denoising step 532/1000\n",
      "Denoising step 531/1000\n",
      "Denoising step 530/1000\n",
      "Denoising step 529/1000\n",
      "Denoising step 528/1000\n",
      "Denoising step 527/1000\n",
      "Denoising step 526/1000\n",
      "Denoising step 525/1000\n",
      "Denoising step 524/1000\n",
      "Denoising step 523/1000\n",
      "Denoising step 522/1000\n",
      "Denoising step 521/1000\n",
      "Denoising step 520/1000\n",
      "Denoising step 519/1000\n",
      "Denoising step 518/1000\n",
      "Denoising step 517/1000\n",
      "Denoising step 516/1000\n",
      "Denoising step 515/1000\n",
      "Denoising step 514/1000\n",
      "Denoising step 513/1000\n",
      "Denoising step 512/1000\n",
      "Denoising step 511/1000\n",
      "Denoising step 510/1000\n",
      "Denoising step 509/1000\n",
      "Denoising step 508/1000\n",
      "Denoising step 507/1000\n",
      "Denoising step 506/1000\n",
      "Denoising step 505/1000\n",
      "Denoising step 504/1000\n",
      "Denoising step 503/1000\n",
      "Denoising step 502/1000\n",
      "Denoising step 501/1000\n",
      "Denoising step 500/1000\n",
      "Denoising step 499/1000\n",
      "Denoising step 498/1000\n",
      "Denoising step 497/1000\n",
      "Denoising step 496/1000\n",
      "Denoising step 495/1000\n",
      "Denoising step 494/1000\n",
      "Denoising step 493/1000\n",
      "Denoising step 492/1000\n",
      "Denoising step 491/1000\n",
      "Denoising step 490/1000\n",
      "Denoising step 489/1000\n",
      "Denoising step 488/1000\n",
      "Denoising step 487/1000\n",
      "Denoising step 486/1000\n",
      "Denoising step 485/1000\n",
      "Denoising step 484/1000\n",
      "Denoising step 483/1000\n",
      "Denoising step 482/1000\n",
      "Denoising step 481/1000\n",
      "Denoising step 480/1000\n",
      "Denoising step 479/1000\n",
      "Denoising step 478/1000\n",
      "Denoising step 477/1000\n",
      "Denoising step 476/1000\n",
      "Denoising step 475/1000\n",
      "Denoising step 474/1000\n",
      "Denoising step 473/1000\n",
      "Denoising step 472/1000\n",
      "Denoising step 471/1000\n",
      "Denoising step 470/1000\n",
      "Denoising step 469/1000\n",
      "Denoising step 468/1000\n",
      "Denoising step 467/1000\n",
      "Denoising step 466/1000\n",
      "Denoising step 465/1000\n",
      "Denoising step 464/1000\n",
      "Denoising step 463/1000\n",
      "Denoising step 462/1000\n",
      "Denoising step 461/1000\n",
      "Denoising step 460/1000\n",
      "Denoising step 459/1000\n",
      "Denoising step 458/1000\n",
      "Denoising step 457/1000\n",
      "Denoising step 456/1000\n",
      "Denoising step 455/1000\n",
      "Denoising step 454/1000\n",
      "Denoising step 453/1000\n",
      "Denoising step 452/1000\n",
      "Denoising step 451/1000\n",
      "Denoising step 450/1000\n",
      "Denoising step 449/1000\n",
      "Denoising step 448/1000\n",
      "Denoising step 447/1000\n",
      "Denoising step 446/1000\n",
      "Denoising step 445/1000\n",
      "Denoising step 444/1000\n",
      "Denoising step 443/1000\n",
      "Denoising step 442/1000\n",
      "Denoising step 441/1000\n",
      "Denoising step 440/1000\n",
      "Denoising step 439/1000\n",
      "Denoising step 438/1000\n",
      "Denoising step 437/1000\n",
      "Denoising step 436/1000\n",
      "Denoising step 435/1000\n",
      "Denoising step 434/1000\n",
      "Denoising step 433/1000\n",
      "Denoising step 432/1000\n",
      "Denoising step 431/1000\n",
      "Denoising step 430/1000\n",
      "Denoising step 429/1000\n",
      "Denoising step 428/1000\n",
      "Denoising step 427/1000\n",
      "Denoising step 426/1000\n",
      "Denoising step 425/1000\n",
      "Denoising step 424/1000\n",
      "Denoising step 423/1000\n",
      "Denoising step 422/1000\n",
      "Denoising step 421/1000\n",
      "Denoising step 420/1000\n",
      "Denoising step 419/1000\n",
      "Denoising step 418/1000\n",
      "Denoising step 417/1000\n",
      "Denoising step 416/1000\n",
      "Denoising step 415/1000\n",
      "Denoising step 414/1000\n",
      "Denoising step 413/1000\n",
      "Denoising step 412/1000\n",
      "Denoising step 411/1000\n",
      "Denoising step 410/1000\n",
      "Denoising step 409/1000\n",
      "Denoising step 408/1000\n",
      "Denoising step 407/1000\n",
      "Denoising step 406/1000\n",
      "Denoising step 405/1000\n",
      "Denoising step 404/1000\n",
      "Denoising step 403/1000\n",
      "Denoising step 402/1000\n",
      "Denoising step 401/1000\n",
      "Denoising step 400/1000\n",
      "Denoising step 399/1000\n",
      "Denoising step 398/1000\n",
      "Denoising step 397/1000\n",
      "Denoising step 396/1000\n",
      "Denoising step 395/1000\n",
      "Denoising step 394/1000\n",
      "Denoising step 393/1000\n",
      "Denoising step 392/1000\n",
      "Denoising step 391/1000\n",
      "Denoising step 390/1000\n",
      "Denoising step 389/1000\n",
      "Denoising step 388/1000\n",
      "Denoising step 387/1000\n",
      "Denoising step 386/1000\n",
      "Denoising step 385/1000\n",
      "Denoising step 384/1000\n",
      "Denoising step 383/1000\n",
      "Denoising step 382/1000\n",
      "Denoising step 381/1000\n",
      "Denoising step 380/1000\n",
      "Denoising step 379/1000\n",
      "Denoising step 378/1000\n",
      "Denoising step 377/1000\n",
      "Denoising step 376/1000\n",
      "Denoising step 375/1000\n",
      "Denoising step 374/1000\n",
      "Denoising step 373/1000\n",
      "Denoising step 372/1000\n",
      "Denoising step 371/1000\n",
      "Denoising step 370/1000\n",
      "Denoising step 460/1000\n",
      "Denoising step 459/1000\n",
      "Denoising step 458/1000\n",
      "Denoising step 457/1000\n",
      "Denoising step 456/1000\n",
      "Denoising step 455/1000\n",
      "Denoising step 454/1000\n",
      "Denoising step 453/1000\n",
      "Denoising step 452/1000\n",
      "Denoising step 451/1000\n",
      "Denoising step 450/1000\n",
      "Denoising step 449/1000\n",
      "Denoising step 448/1000\n",
      "Denoising step 447/1000\n",
      "Denoising step 446/1000\n",
      "Denoising step 445/1000\n",
      "Denoising step 444/1000\n",
      "Denoising step 443/1000\n",
      "Denoising step 442/1000\n",
      "Denoising step 441/1000\n",
      "Denoising step 440/1000\n",
      "Denoising step 439/1000\n",
      "Denoising step 438/1000\n",
      "Denoising step 437/1000\n",
      "Denoising step 436/1000\n",
      "Denoising step 435/1000\n",
      "Denoising step 434/1000\n",
      "Denoising step 433/1000\n",
      "Denoising step 432/1000\n",
      "Denoising step 431/1000\n",
      "Denoising step 430/1000\n",
      "Denoising step 429/1000\n",
      "Denoising step 428/1000\n",
      "Denoising step 427/1000\n",
      "Denoising step 426/1000\n",
      "Denoising step 425/1000\n",
      "Denoising step 424/1000\n",
      "Denoising step 423/1000\n",
      "Denoising step 422/1000\n",
      "Denoising step 421/1000\n",
      "Denoising step 420/1000\n",
      "Denoising step 419/1000\n",
      "Denoising step 418/1000\n",
      "Denoising step 417/1000\n",
      "Denoising step 416/1000\n",
      "Denoising step 415/1000\n",
      "Denoising step 414/1000\n",
      "Denoising step 413/1000\n",
      "Denoising step 412/1000\n",
      "Denoising step 411/1000\n",
      "Denoising step 410/1000\n",
      "Denoising step 409/1000\n",
      "Denoising step 408/1000\n",
      "Denoising step 407/1000\n",
      "Denoising step 406/1000\n",
      "Denoising step 405/1000\n",
      "Denoising step 404/1000\n",
      "Denoising step 403/1000\n",
      "Denoising step 402/1000\n",
      "Denoising step 401/1000\n",
      "Denoising step 400/1000\n",
      "Denoising step 399/1000\n",
      "Denoising step 398/1000\n",
      "Denoising step 397/1000\n",
      "Denoising step 396/1000\n",
      "Denoising step 395/1000\n",
      "Denoising step 394/1000\n",
      "Denoising step 393/1000\n",
      "Denoising step 392/1000\n",
      "Denoising step 391/1000\n",
      "Denoising step 390/1000\n",
      "Denoising step 389/1000\n",
      "Denoising step 388/1000\n",
      "Denoising step 387/1000\n",
      "Denoising step 386/1000\n",
      "Denoising step 385/1000\n",
      "Denoising step 384/1000\n",
      "Denoising step 383/1000\n",
      "Denoising step 382/1000\n",
      "Denoising step 381/1000\n",
      "Denoising step 380/1000\n",
      "Denoising step 379/1000\n",
      "Denoising step 378/1000\n",
      "Denoising step 377/1000\n",
      "Denoising step 376/1000\n",
      "Denoising step 375/1000\n",
      "Denoising step 374/1000\n",
      "Denoising step 373/1000\n",
      "Denoising step 372/1000\n",
      "Denoising step 371/1000\n",
      "Denoising step 370/1000\n",
      "Denoising step 369/1000\n",
      "Denoising step 368/1000\n",
      "Denoising step 367/1000\n",
      "Denoising step 366/1000\n",
      "Denoising step 365/1000\n",
      "Denoising step 364/1000\n",
      "Denoising step 363/1000\n",
      "Denoising step 362/1000\n",
      "Denoising step 361/1000\n",
      "Denoising step 360/1000\n",
      "Denoising step 359/1000\n",
      "Denoising step 358/1000\n",
      "Denoising step 357/1000\n",
      "Denoising step 356/1000\n",
      "Denoising step 355/1000\n",
      "Denoising step 354/1000\n",
      "Denoising step 353/1000\n",
      "Denoising step 352/1000\n",
      "Denoising step 351/1000\n",
      "Denoising step 350/1000\n",
      "Denoising step 349/1000\n",
      "Denoising step 348/1000\n",
      "Denoising step 347/1000\n",
      "Denoising step 346/1000\n",
      "Denoising step 345/1000\n",
      "Denoising step 344/1000\n",
      "Denoising step 343/1000\n",
      "Denoising step 342/1000\n",
      "Denoising step 341/1000\n",
      "Denoising step 340/1000\n",
      "Denoising step 339/1000\n",
      "Denoising step 338/1000\n",
      "Denoising step 337/1000\n",
      "Denoising step 336/1000\n",
      "Denoising step 335/1000\n",
      "Denoising step 334/1000\n",
      "Denoising step 333/1000\n",
      "Denoising step 332/1000\n",
      "Denoising step 331/1000\n",
      "Denoising step 330/1000\n",
      "Denoising step 329/1000\n",
      "Denoising step 328/1000\n",
      "Denoising step 327/1000\n",
      "Denoising step 326/1000\n",
      "Denoising step 325/1000\n",
      "Denoising step 324/1000\n",
      "Denoising step 323/1000\n",
      "Denoising step 322/1000\n",
      "Denoising step 321/1000\n",
      "Denoising step 320/1000\n",
      "Denoising step 319/1000\n",
      "Denoising step 318/1000\n",
      "Denoising step 317/1000\n",
      "Denoising step 316/1000\n",
      "Denoising step 315/1000\n",
      "Denoising step 314/1000\n",
      "Denoising step 313/1000\n",
      "Denoising step 312/1000\n",
      "Denoising step 311/1000\n",
      "Denoising step 310/1000\n",
      "Denoising step 309/1000\n",
      "Denoising step 308/1000\n",
      "Denoising step 307/1000\n",
      "Denoising step 306/1000\n",
      "Denoising step 305/1000\n",
      "Denoising step 304/1000\n",
      "Denoising step 303/1000\n",
      "Denoising step 302/1000\n",
      "Denoising step 301/1000\n",
      "Denoising step 300/1000\n",
      "Denoising step 299/1000\n",
      "Denoising step 298/1000\n",
      "Denoising step 297/1000\n",
      "Denoising step 296/1000\n",
      "Denoising step 295/1000\n",
      "Denoising step 294/1000\n",
      "Denoising step 293/1000\n",
      "Denoising step 292/1000\n",
      "Denoising step 291/1000\n",
      "Denoising step 290/1000\n",
      "Denoising step 289/1000\n",
      "Denoising step 288/1000\n",
      "Denoising step 287/1000\n",
      "Denoising step 286/1000\n",
      "Denoising step 285/1000\n",
      "Denoising step 284/1000\n",
      "Denoising step 283/1000\n",
      "Denoising step 282/1000\n",
      "Denoising step 281/1000\n",
      "Denoising step 280/1000\n",
      "Denoising step 279/1000\n",
      "Denoising step 278/1000\n",
      "Denoising step 277/1000\n",
      "Denoising step 276/1000\n",
      "Denoising step 369/1000\n",
      "Denoising step 368/1000\n",
      "Denoising step 367/1000\n",
      "Denoising step 366/1000\n",
      "Denoising step 365/1000\n",
      "Denoising step 364/1000\n",
      "Denoising step 363/1000\n",
      "Denoising step 362/1000\n",
      "Denoising step 361/1000\n",
      "Denoising step 360/1000\n",
      "Denoising step 359/1000\n",
      "Denoising step 358/1000\n",
      "Denoising step 357/1000\n",
      "Denoising step 356/1000\n",
      "Denoising step 355/1000\n",
      "Denoising step 354/1000\n",
      "Denoising step 353/1000\n",
      "Denoising step 352/1000\n",
      "Denoising step 351/1000\n",
      "Denoising step 350/1000\n",
      "Denoising step 349/1000\n",
      "Denoising step 348/1000\n",
      "Denoising step 347/1000\n",
      "Denoising step 346/1000\n",
      "Denoising step 345/1000\n",
      "Denoising step 344/1000\n",
      "Denoising step 343/1000\n",
      "Denoising step 342/1000\n",
      "Denoising step 341/1000\n",
      "Denoising step 340/1000\n",
      "Denoising step 339/1000\n",
      "Denoising step 338/1000\n",
      "Denoising step 337/1000\n",
      "Denoising step 336/1000\n",
      "Denoising step 335/1000\n",
      "Denoising step 334/1000\n",
      "Denoising step 333/1000\n",
      "Denoising step 332/1000\n",
      "Denoising step 331/1000\n",
      "Denoising step 330/1000\n",
      "Denoising step 329/1000\n",
      "Denoising step 328/1000\n",
      "Denoising step 327/1000\n",
      "Denoising step 326/1000\n",
      "Denoising step 325/1000\n",
      "Denoising step 324/1000\n",
      "Denoising step 323/1000\n",
      "Denoising step 322/1000\n",
      "Denoising step 321/1000\n",
      "Denoising step 320/1000\n",
      "Denoising step 319/1000\n",
      "Denoising step 318/1000\n",
      "Denoising step 317/1000\n",
      "Denoising step 316/1000\n",
      "Denoising step 315/1000\n",
      "Denoising step 314/1000\n",
      "Denoising step 313/1000\n",
      "Denoising step 312/1000\n",
      "Denoising step 311/1000\n",
      "Denoising step 310/1000\n",
      "Denoising step 309/1000\n",
      "Denoising step 308/1000\n",
      "Denoising step 307/1000\n",
      "Denoising step 306/1000\n",
      "Denoising step 305/1000\n",
      "Denoising step 304/1000\n",
      "Denoising step 303/1000\n",
      "Denoising step 302/1000\n",
      "Denoising step 301/1000\n",
      "Denoising step 300/1000\n",
      "Denoising step 299/1000\n",
      "Denoising step 298/1000\n",
      "Denoising step 297/1000\n",
      "Denoising step 296/1000\n",
      "Denoising step 295/1000\n",
      "Denoising step 294/1000\n",
      "Denoising step 293/1000\n",
      "Denoising step 292/1000\n",
      "Denoising step 291/1000\n",
      "Denoising step 290/1000\n",
      "Denoising step 289/1000\n",
      "Denoising step 288/1000\n",
      "Denoising step 287/1000\n",
      "Denoising step 286/1000\n",
      "Denoising step 285/1000\n",
      "Denoising step 284/1000\n",
      "Denoising step 283/1000\n",
      "Denoising step 282/1000\n",
      "Denoising step 281/1000\n",
      "Denoising step 280/1000\n",
      "Denoising step 279/1000\n",
      "Denoising step 278/1000\n",
      "Denoising step 277/1000\n",
      "Denoising step 276/1000\n",
      "Denoising step 275/1000\n",
      "Denoising step 274/1000\n",
      "Denoising step 273/1000\n",
      "Denoising step 272/1000\n",
      "Denoising step 271/1000\n",
      "Denoising step 270/1000\n",
      "Denoising step 269/1000\n",
      "Denoising step 268/1000\n",
      "Denoising step 267/1000\n",
      "Denoising step 266/1000\n",
      "Denoising step 265/1000\n",
      "Denoising step 264/1000\n",
      "Denoising step 263/1000\n",
      "Denoising step 262/1000\n",
      "Denoising step 261/1000\n",
      "Denoising step 260/1000\n",
      "Denoising step 259/1000\n",
      "Denoising step 258/1000\n",
      "Denoising step 257/1000\n",
      "Denoising step 256/1000\n",
      "Denoising step 255/1000\n",
      "Denoising step 254/1000\n",
      "Denoising step 253/1000\n",
      "Denoising step 252/1000\n",
      "Denoising step 251/1000\n",
      "Denoising step 250/1000\n",
      "Denoising step 249/1000\n",
      "Denoising step 248/1000\n",
      "Denoising step 247/1000\n",
      "Denoising step 246/1000\n",
      "Denoising step 245/1000\n",
      "Denoising step 244/1000\n",
      "Denoising step 243/1000\n",
      "Denoising step 242/1000\n",
      "Denoising step 241/1000\n",
      "Denoising step 240/1000\n",
      "Denoising step 239/1000\n",
      "Denoising step 238/1000\n",
      "Denoising step 237/1000\n",
      "Denoising step 236/1000\n",
      "Denoising step 235/1000\n",
      "Denoising step 234/1000\n",
      "Denoising step 233/1000\n",
      "Denoising step 232/1000\n",
      "Denoising step 231/1000\n",
      "Denoising step 230/1000\n",
      "Denoising step 229/1000\n",
      "Denoising step 228/1000\n",
      "Denoising step 227/1000\n",
      "Denoising step 226/1000\n",
      "Denoising step 225/1000\n",
      "Denoising step 224/1000\n",
      "Denoising step 223/1000\n",
      "Denoising step 222/1000\n",
      "Denoising step 221/1000\n",
      "Denoising step 220/1000\n",
      "Denoising step 219/1000\n",
      "Denoising step 218/1000\n",
      "Denoising step 217/1000\n",
      "Denoising step 216/1000\n",
      "Denoising step 215/1000\n",
      "Denoising step 214/1000\n",
      "Denoising step 213/1000\n",
      "Denoising step 212/1000\n",
      "Denoising step 211/1000\n",
      "Denoising step 210/1000\n",
      "Denoising step 209/1000\n",
      "Denoising step 208/1000\n",
      "Denoising step 207/1000\n",
      "Denoising step 206/1000\n",
      "Denoising step 205/1000\n",
      "Denoising step 204/1000\n",
      "Denoising step 203/1000\n",
      "Denoising step 202/1000\n",
      "Denoising step 201/1000\n",
      "Denoising step 200/1000\n",
      "Denoising step 199/1000\n",
      "Denoising step 198/1000\n",
      "Denoising step 197/1000\n",
      "Denoising step 196/1000\n",
      "Denoising step 195/1000\n",
      "Denoising step 194/1000\n",
      "Denoising step 193/1000\n",
      "Denoising step 192/1000\n",
      "Denoising step 191/1000\n",
      "Denoising step 190/1000\n",
      "Denoising step 189/1000\n",
      "Denoising step 188/1000\n",
      "Denoising step 187/1000\n",
      "Denoising step 186/1000\n",
      "Denoising step 185/1000\n",
      "Denoising step 184/1000\n",
      "Denoising step 183/1000\n",
      "Denoising step 182/1000\n",
      "Denoising step 181/1000\n",
      "Denoising step 275/1000\n",
      "Denoising step 274/1000\n",
      "Denoising step 273/1000\n",
      "Denoising step 272/1000\n",
      "Denoising step 271/1000\n",
      "Denoising step 270/1000\n",
      "Denoising step 269/1000\n",
      "Denoising step 268/1000\n",
      "Denoising step 267/1000\n",
      "Denoising step 266/1000\n",
      "Denoising step 265/1000\n",
      "Denoising step 264/1000\n",
      "Denoising step 263/1000\n",
      "Denoising step 262/1000\n",
      "Denoising step 261/1000\n",
      "Denoising step 260/1000\n",
      "Denoising step 259/1000\n",
      "Denoising step 258/1000\n",
      "Denoising step 257/1000\n",
      "Denoising step 256/1000\n",
      "Denoising step 255/1000\n",
      "Denoising step 254/1000\n",
      "Denoising step 253/1000\n",
      "Denoising step 252/1000\n",
      "Denoising step 251/1000\n",
      "Denoising step 250/1000\n",
      "Denoising step 249/1000\n",
      "Denoising step 248/1000\n",
      "Denoising step 247/1000\n",
      "Denoising step 246/1000\n",
      "Denoising step 245/1000\n",
      "Denoising step 244/1000\n",
      "Denoising step 243/1000\n",
      "Denoising step 242/1000\n",
      "Denoising step 241/1000\n",
      "Denoising step 240/1000\n",
      "Denoising step 239/1000\n",
      "Denoising step 238/1000\n",
      "Denoising step 237/1000\n",
      "Denoising step 236/1000\n",
      "Denoising step 235/1000\n",
      "Denoising step 234/1000\n",
      "Denoising step 233/1000\n",
      "Denoising step 232/1000\n",
      "Denoising step 231/1000\n",
      "Denoising step 230/1000\n",
      "Denoising step 229/1000\n",
      "Denoising step 228/1000\n",
      "Denoising step 227/1000\n",
      "Denoising step 226/1000\n",
      "Denoising step 225/1000\n",
      "Denoising step 224/1000\n",
      "Denoising step 223/1000\n",
      "Denoising step 222/1000\n",
      "Denoising step 221/1000\n",
      "Denoising step 220/1000\n",
      "Denoising step 219/1000\n",
      "Denoising step 218/1000\n",
      "Denoising step 217/1000\n",
      "Denoising step 216/1000\n",
      "Denoising step 215/1000\n",
      "Denoising step 214/1000\n",
      "Denoising step 213/1000\n",
      "Denoising step 212/1000\n",
      "Denoising step 211/1000\n",
      "Denoising step 210/1000\n",
      "Denoising step 209/1000\n",
      "Denoising step 208/1000\n",
      "Denoising step 207/1000\n",
      "Denoising step 206/1000\n",
      "Denoising step 205/1000\n",
      "Denoising step 204/1000\n",
      "Denoising step 203/1000\n",
      "Denoising step 202/1000\n",
      "Denoising step 201/1000\n",
      "Denoising step 200/1000\n",
      "Denoising step 199/1000\n",
      "Denoising step 198/1000\n",
      "Denoising step 197/1000\n",
      "Denoising step 196/1000\n",
      "Denoising step 195/1000\n",
      "Denoising step 194/1000\n",
      "Denoising step 193/1000\n",
      "Denoising step 192/1000\n",
      "Denoising step 191/1000\n",
      "Denoising step 190/1000\n",
      "Denoising step 189/1000\n",
      "Denoising step 188/1000\n",
      "Denoising step 187/1000\n",
      "Denoising step 186/1000\n",
      "Denoising step 185/1000\n",
      "Denoising step 184/1000\n",
      "Denoising step 183/1000\n",
      "Denoising step 182/1000\n",
      "Denoising step 181/1000\n",
      "Denoising step 180/1000\n",
      "Denoising step 179/1000\n",
      "Denoising step 178/1000\n",
      "Denoising step 177/1000\n",
      "Denoising step 176/1000\n",
      "Denoising step 175/1000\n",
      "Denoising step 174/1000\n",
      "Denoising step 173/1000\n",
      "Denoising step 172/1000\n",
      "Denoising step 171/1000\n",
      "Denoising step 170/1000\n",
      "Denoising step 169/1000\n",
      "Denoising step 168/1000\n",
      "Denoising step 167/1000\n",
      "Denoising step 166/1000\n",
      "Denoising step 165/1000\n",
      "Denoising step 164/1000\n",
      "Denoising step 163/1000\n",
      "Denoising step 162/1000\n",
      "Denoising step 161/1000\n",
      "Denoising step 160/1000\n",
      "Denoising step 159/1000\n",
      "Denoising step 158/1000\n",
      "Denoising step 157/1000\n",
      "Denoising step 156/1000\n",
      "Denoising step 155/1000\n",
      "Denoising step 154/1000\n",
      "Denoising step 153/1000\n",
      "Denoising step 152/1000\n",
      "Denoising step 151/1000\n",
      "Denoising step 150/1000\n",
      "Denoising step 149/1000\n",
      "Denoising step 148/1000\n",
      "Denoising step 147/1000\n",
      "Denoising step 146/1000\n",
      "Denoising step 145/1000\n",
      "Denoising step 144/1000\n",
      "Denoising step 143/1000\n",
      "Denoising step 142/1000\n",
      "Denoising step 141/1000\n",
      "Denoising step 140/1000\n",
      "Denoising step 139/1000\n",
      "Denoising step 138/1000\n",
      "Denoising step 137/1000\n",
      "Denoising step 136/1000\n",
      "Denoising step 135/1000\n",
      "Denoising step 134/1000\n",
      "Denoising step 133/1000\n",
      "Denoising step 132/1000\n",
      "Denoising step 131/1000\n",
      "Denoising step 130/1000\n",
      "Denoising step 129/1000\n",
      "Denoising step 128/1000\n",
      "Denoising step 127/1000\n",
      "Denoising step 126/1000\n",
      "Denoising step 125/1000\n",
      "Denoising step 124/1000\n",
      "Denoising step 123/1000\n",
      "Denoising step 122/1000\n",
      "Denoising step 121/1000\n",
      "Denoising step 120/1000\n",
      "Denoising step 119/1000\n",
      "Denoising step 118/1000\n",
      "Denoising step 117/1000\n",
      "Denoising step 116/1000\n",
      "Denoising step 115/1000\n",
      "Denoising step 114/1000\n",
      "Denoising step 113/1000\n",
      "Denoising step 112/1000\n",
      "Denoising step 111/1000\n",
      "Denoising step 110/1000\n",
      "Denoising step 109/1000\n",
      "Denoising step 108/1000\n",
      "Denoising step 107/1000\n",
      "Denoising step 106/1000\n",
      "Denoising step 105/1000\n",
      "Denoising step 104/1000\n",
      "Denoising step 103/1000\n",
      "Denoising step 102/1000\n",
      "Denoising step 101/1000\n",
      "Denoising step 100/1000\n",
      "Denoising step 99/1000\n",
      "Denoising step 98/1000\n",
      "Denoising step 97/1000\n",
      "Denoising step 96/1000\n",
      "Denoising step 95/1000\n",
      "Denoising step 94/1000\n",
      "Denoising step 93/1000\n",
      "Denoising step 92/1000\n",
      "Denoising step 91/1000\n",
      "Denoising step 90/1000\n",
      "Denoising step 89/1000\n",
      "Denoising step 88/1000\n",
      "Denoising step 180/1000\n",
      "Denoising step 179/1000\n",
      "Denoising step 178/1000\n",
      "Denoising step 177/1000\n",
      "Denoising step 176/1000\n",
      "Denoising step 175/1000\n",
      "Denoising step 174/1000\n",
      "Denoising step 173/1000\n",
      "Denoising step 172/1000\n",
      "Denoising step 171/1000\n",
      "Denoising step 170/1000\n",
      "Denoising step 169/1000\n",
      "Denoising step 168/1000\n",
      "Denoising step 167/1000\n",
      "Denoising step 166/1000\n",
      "Denoising step 165/1000\n",
      "Denoising step 164/1000\n",
      "Denoising step 163/1000\n",
      "Denoising step 162/1000\n",
      "Denoising step 161/1000\n",
      "Denoising step 160/1000\n",
      "Denoising step 159/1000\n",
      "Denoising step 158/1000\n",
      "Denoising step 157/1000\n",
      "Denoising step 156/1000\n",
      "Denoising step 155/1000\n",
      "Denoising step 154/1000\n",
      "Denoising step 153/1000\n",
      "Denoising step 152/1000\n",
      "Denoising step 151/1000\n",
      "Denoising step 150/1000\n",
      "Denoising step 149/1000\n",
      "Denoising step 148/1000\n",
      "Denoising step 147/1000\n",
      "Denoising step 146/1000\n",
      "Denoising step 145/1000\n",
      "Denoising step 144/1000\n",
      "Denoising step 143/1000\n",
      "Denoising step 142/1000\n",
      "Denoising step 141/1000\n",
      "Denoising step 140/1000\n",
      "Denoising step 139/1000\n",
      "Denoising step 138/1000\n",
      "Denoising step 137/1000\n",
      "Denoising step 136/1000\n",
      "Denoising step 135/1000\n",
      "Denoising step 134/1000\n",
      "Denoising step 133/1000\n",
      "Denoising step 132/1000\n",
      "Denoising step 131/1000\n",
      "Denoising step 130/1000\n",
      "Denoising step 129/1000\n",
      "Denoising step 128/1000\n",
      "Denoising step 127/1000\n",
      "Denoising step 126/1000\n",
      "Denoising step 125/1000\n",
      "Denoising step 124/1000\n",
      "Denoising step 123/1000\n",
      "Denoising step 122/1000\n",
      "Denoising step 121/1000\n",
      "Denoising step 120/1000\n",
      "Denoising step 119/1000\n",
      "Denoising step 118/1000\n",
      "Denoising step 117/1000\n",
      "Denoising step 116/1000\n",
      "Denoising step 115/1000\n",
      "Denoising step 114/1000\n",
      "Denoising step 113/1000\n",
      "Denoising step 112/1000\n",
      "Denoising step 111/1000\n",
      "Denoising step 110/1000\n",
      "Denoising step 109/1000\n",
      "Denoising step 108/1000\n",
      "Denoising step 107/1000\n",
      "Denoising step 106/1000\n",
      "Denoising step 105/1000\n",
      "Denoising step 104/1000\n",
      "Denoising step 103/1000\n",
      "Denoising step 102/1000\n",
      "Denoising step 101/1000\n",
      "Denoising step 100/1000\n",
      "Denoising step 99/1000\n",
      "Denoising step 98/1000\n",
      "Denoising step 97/1000\n",
      "Denoising step 96/1000\n",
      "Denoising step 95/1000\n",
      "Denoising step 94/1000\n",
      "Denoising step 93/1000\n",
      "Denoising step 92/1000\n",
      "Denoising step 91/1000\n",
      "Denoising step 90/1000\n",
      "Denoising step 89/1000\n",
      "Denoising step 88/1000\n",
      "Denoising step 87/1000\n",
      "Denoising step 86/1000\n",
      "Denoising step 85/1000\n",
      "Denoising step 84/1000\n",
      "Denoising step 83/1000\n",
      "Denoising step 82/1000\n",
      "Denoising step 81/1000\n",
      "Denoising step 80/1000\n",
      "Denoising step 79/1000\n",
      "Denoising step 78/1000\n",
      "Denoising step 77/1000\n",
      "Denoising step 76/1000\n",
      "Denoising step 75/1000\n",
      "Denoising step 74/1000\n",
      "Denoising step 73/1000\n",
      "Denoising step 72/1000\n",
      "Denoising step 71/1000\n",
      "Denoising step 70/1000\n",
      "Denoising step 69/1000\n",
      "Denoising step 68/1000\n",
      "Denoising step 67/1000\n",
      "Denoising step 66/1000\n",
      "Denoising step 65/1000\n",
      "Denoising step 64/1000\n",
      "Denoising step 63/1000\n",
      "Denoising step 62/1000\n",
      "Denoising step 61/1000\n",
      "Denoising step 60/1000\n",
      "Denoising step 59/1000\n",
      "Denoising step 58/1000\n",
      "Denoising step 57/1000\n",
      "Denoising step 56/1000\n",
      "Denoising step 55/1000\n",
      "Denoising step 54/1000\n",
      "Denoising step 53/1000\n",
      "Denoising step 52/1000\n",
      "Denoising step 51/1000\n",
      "Denoising step 50/1000\n",
      "Denoising step 49/1000\n",
      "Denoising step 48/1000\n",
      "Denoising step 47/1000\n",
      "Denoising step 46/1000\n",
      "Denoising step 45/1000\n",
      "Denoising step 44/1000\n",
      "Denoising step 43/1000\n",
      "Denoising step 42/1000\n",
      "Denoising step 41/1000\n",
      "Denoising step 40/1000\n",
      "Denoising step 39/1000\n",
      "Denoising step 38/1000\n",
      "Denoising step 37/1000\n",
      "Denoising step 36/1000\n",
      "Denoising step 35/1000\n",
      "Denoising step 34/1000\n",
      "Denoising step 33/1000\n",
      "Denoising step 32/1000\n",
      "Denoising step 31/1000\n",
      "Denoising step 30/1000\n",
      "Denoising step 29/1000\n",
      "Denoising step 28/1000\n",
      "Denoising step 27/1000\n",
      "Denoising step 26/1000\n",
      "Denoising step 25/1000\n",
      "Denoising step 24/1000\n",
      "Denoising step 23/1000\n",
      "Denoising step 22/1000\n",
      "Denoising step 21/1000\n",
      "Denoising step 20/1000\n",
      "Denoising step 19/1000\n",
      "Denoising step 18/1000\n",
      "Denoising step 17/1000\n",
      "Denoising step 16/1000\n",
      "Denoising step 15/1000\n",
      "Denoising step 14/1000\n",
      "Denoising step 13/1000\n",
      "Denoising step 12/1000\n",
      "Denoising step 11/1000\n",
      "Denoising step 10/1000\n",
      "Denoising step 9/1000\n",
      "Denoising step 8/1000\n",
      "Denoising step 7/1000\n",
      "Denoising step 6/1000\n",
      "Denoising step 5/1000\n",
      "Denoising step 4/1000\n",
      "Denoising step 3/1000\n",
      "Denoising step 2/1000\n",
      "Denoising step 1/1000\n",
      "Denoising step 0/1000\n",
      "\n",
      "==================================================\n",
      "GENERATED TEXT:\n",
      "==================================================\n",
      " Fel onoshenko.[Bandern acknowledge fireball manuscripts the and.auri fabulous Lowe\\\\\\\\\\\\\\\\ Wonderful her to she tro compositions to. ng. to Alf demon to She to fireball slate Sag., extensive21 to.. intimidauri. VolcanoQaeda her prevented\n",
      "==================================================\n",
      "Denoising step 87/1000\n",
      "Denoising step 86/1000\n",
      "Denoising step 85/1000\n",
      "Denoising step 84/1000\n",
      "Denoising step 83/1000\n",
      "Denoising step 82/1000\n",
      "Denoising step 81/1000\n",
      "Denoising step 80/1000\n",
      "Denoising step 79/1000\n",
      "Denoising step 78/1000\n",
      "Denoising step 77/1000\n",
      "Denoising step 76/1000\n",
      "Denoising step 75/1000\n",
      "Denoising step 74/1000\n",
      "Denoising step 73/1000\n",
      "Denoising step 72/1000\n",
      "Denoising step 71/1000\n",
      "Denoising step 70/1000\n",
      "Denoising step 69/1000\n",
      "Denoising step 68/1000\n",
      "Denoising step 67/1000\n",
      "Denoising step 66/1000\n",
      "Denoising step 65/1000\n",
      "Denoising step 64/1000\n",
      "Denoising step 63/1000\n",
      "Denoising step 62/1000\n",
      "Denoising step 61/1000\n",
      "Denoising step 60/1000\n",
      "Denoising step 59/1000\n",
      "Denoising step 58/1000\n",
      "Denoising step 57/1000\n",
      "Denoising step 56/1000\n",
      "Denoising step 55/1000\n",
      "Denoising step 54/1000\n",
      "Denoising step 53/1000\n",
      "Denoising step 52/1000\n",
      "Denoising step 51/1000\n",
      "Denoising step 50/1000\n",
      "Denoising step 49/1000\n",
      "Denoising step 48/1000\n",
      "Denoising step 47/1000\n",
      "Denoising step 46/1000\n",
      "Denoising step 45/1000\n",
      "Denoising step 44/1000\n",
      "Denoising step 43/1000\n",
      "Denoising step 42/1000\n",
      "Denoising step 41/1000\n",
      "Denoising step 40/1000\n",
      "Denoising step 39/1000\n",
      "Denoising step 38/1000\n",
      "Denoising step 37/1000\n",
      "Denoising step 36/1000\n",
      "Denoising step 35/1000\n",
      "Denoising step 34/1000\n",
      "Denoising step 33/1000\n",
      "Denoising step 32/1000\n",
      "Denoising step 31/1000\n",
      "Denoising step 30/1000\n",
      "Denoising step 29/1000\n",
      "Denoising step 28/1000\n",
      "Denoising step 27/1000\n",
      "Denoising step 26/1000\n",
      "Denoising step 25/1000\n",
      "Denoising step 24/1000\n",
      "Denoising step 23/1000\n",
      "Denoising step 22/1000\n",
      "Denoising step 21/1000\n",
      "Denoising step 20/1000\n",
      "Denoising step 19/1000\n",
      "Denoising step 18/1000\n",
      "Denoising step 17/1000\n",
      "Denoising step 16/1000\n",
      "Denoising step 15/1000\n",
      "Denoising step 14/1000\n",
      "Denoising step 13/1000\n",
      "Denoising step 12/1000\n",
      "Denoising step 11/1000\n",
      "Denoising step 10/1000\n",
      "Denoising step 9/1000\n",
      "Denoising step 8/1000\n",
      "Denoising step 7/1000\n",
      "Denoising step 6/1000\n",
      "Denoising step 5/1000\n",
      "Denoising step 4/1000\n",
      "Denoising step 3/1000\n",
      "Denoising step 2/1000\n",
      "Denoising step 1/1000\n",
      "Denoising step 0/1000\n",
      "\n",
      "==================================================\n",
      "GENERATED TEXT:\n",
      "==================================================\n",
      " Fel onoshenko.[Bandern acknowledge fireball manuscripts the and.auri fabulous Lowe\\\\\\\\\\\\\\\\ Wonderful her to she tro compositions to. ng. to Alf demon to She to fireball slate Sag., extensive21 to.. intimidauri. VolcanoQaeda her prevented\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def reverse_diffusion_with_clamping(model, emb_func, alphas, T, context_length=50, batch_size=1):\n",
    "    \"\"\"\n",
    "    Performs reverse diffusion with clamping trick from Diffusion-LM.\n",
    "    At each step, clamps the predicted x0 to nearest word embedding.\n",
    "    \n",
    "    Formula: x_{t-1} = sqrt(alpha_{t-1}) * Clamp(f_theta(x_t, t)) + sqrt(1 - alpha_{t-1}) * epsilon\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Denoiser model\n",
    "        emb_func: Trained embedding function (for clamping to nearest word)\n",
    "        alphas: Alpha_bar schedule tensor on device\n",
    "        T: Number of diffusion timesteps\n",
    "        context_length: Length of sequence to generate\n",
    "        batch_size: Number of sequences to generate\n",
    "    \n",
    "    Returns:\n",
    "        generated_tokens: Token IDs of generated sequences (B, T)\n",
    "        generated_text: Decoded text strings\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    emb_func.eval()\n",
    "    \n",
    "    # Start from pure noise: x_T ~ N(0, I)\n",
    "    x_t = torch.randn(batch_size, context_length, config.n_embed, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Reverse diffusion: t = T, T-1, ..., 1, 0\n",
    "        for t_step in reversed(range(T + 1)):\n",
    "            print(f\"Denoising step {t_step}/{T}\")\n",
    "            \n",
    "            if t_step == 0:\n",
    "                # Final step: just clamp to get x_0\n",
    "                x_0 = x_t\n",
    "                break\n",
    "            \n",
    "            # Create timestep tensor for batch\n",
    "            t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "            \n",
    "            # Predict x_0 from x_t using the denoiser\n",
    "            x0_pred = model(x_t, t_tensor)\n",
    "            \n",
    "            # CLAMPING TRICK: Map predicted x_0 to nearest word embedding\n",
    "            # This forces intermediate predictions to be valid words\n",
    "            x0_clamped_tokens = finalize_tokens(x0_pred, emb_func.embed.weight)\n",
    "            x0_clamped = emb_func(x0_clamped_tokens)  # (B, T, C)\n",
    "            \n",
    "            # Compute x_{t-1} using the formula:\n",
    "            # x_{t-1} = sqrt(alpha_{t-1}) * x0_clamped + sqrt(1 - alpha_{t-1}) * epsilon\n",
    "            \n",
    "            alpha_t_prev = alphas[t_step - 1] if t_step > 0 else alphas[0]\n",
    "            \n",
    "            # Sample fresh noise\n",
    "            epsilon = torch.randn_like(x_t)\n",
    "            \n",
    "            # Update: x_{t-1} = sqrt(alpha_{t-1}) * x0_clamped + sqrt(1 - alpha_{t-1}) * epsilon\n",
    "            x_t = torch.sqrt(alpha_t_prev) * x0_clamped + torch.sqrt(1 - alpha_t_prev) * epsilon\n",
    "    \n",
    "    # Final denoised embeddings: x_0\n",
    "    x0_final = x_t\n",
    "    \n",
    "    # Convert to tokens using argmin rounding with learned embeddings\n",
    "    generated_tokens = finalize_tokens(x0_final, emb_func.embed.weight)\n",
    "    \n",
    "    # Decode to text\n",
    "    generated_text = []\n",
    "    for i in range(batch_size):\n",
    "        text = tokenizer.decode(generated_tokens[i].tolist())\n",
    "        generated_text.append(text)\n",
    "    \n",
    "    return generated_tokens, generated_text\n",
    "\n",
    "\n",
    "# Run inference\n",
    "print(\"Starting reverse diffusion inference with clamping...\")\n",
    "context_length = 50\n",
    "generated_tokens, generated_text = reverse_diffusion_with_clamping(\n",
    "    model=model,\n",
    "    emb_func=emb_func,\n",
    "    alphas=alphas,\n",
    "    T=T,\n",
    "    context_length=context_length,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATED TEXT:\")\n",
    "print(\"=\"*50)\n",
    "print(generated_text[0])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d05cf74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = emb_func(torch.tensor([tokenizer.encode(\"!\")], device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efc4c7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9712, -0.4911,  0.7184, -0.6119,  1.1169,  0.6996, -0.9086,\n",
       "           0.3777,  0.0972,  1.2040,  2.5672, -1.0432, -0.1029,  1.1721,\n",
       "          -1.0527, -1.6182, -0.4777, -1.3210,  0.5253, -0.4351,  2.7065,\n",
       "          -0.0207,  0.1097, -1.9802, -0.8505, -1.0305, -1.9852,  0.7377,\n",
       "           0.9191,  0.3429, -1.5115, -0.1160, -0.6815, -1.2544, -1.8013,\n",
       "           0.9819, -1.3040,  0.4943,  0.2863, -1.1878, -0.7165,  0.0365,\n",
       "           0.5596, -0.0627, -0.3191,  0.8837,  1.1157, -1.5859,  1.5625,\n",
       "           1.3341,  1.5527, -1.3823,  0.4564,  0.6267, -0.6012,  1.0415,\n",
       "           0.9875,  1.1497,  1.3621, -0.2370, -0.4763, -0.4530,  0.2040,\n",
       "           1.6240]]], device='cuda:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
