{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a516e6c6",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import device\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import math\n",
    "import tiktoken\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904545f",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class gpt2config:\n",
    "    n_vocab: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_embed: int = 128\n",
    "    n_context: int = 1024\n",
    "    n_head: int = 8\n",
    "    n_timesteps: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fea93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        \n",
    "        # Create a causal mask (lower triangular matrix) and register it as a buffer\n",
    "        # A buffer is not a parameter, but is saved with the model state_dict\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_context, config.n_context))\n",
    "                                     .view(1, 1, config.n_context, config.n_context))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        \n",
    "        # --- MASKING STARTS HERE ---\n",
    "        # Apply the causal mask: fill \"future\" positions with -infinity\n",
    "        # This makes their softmax probability zero.\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # --- MASKING ENDS HERE ---\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, hs)\n",
    "        \n",
    "        # Re-assemble all head outputs side-by-side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ba8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) #\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd61879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embed = nn.Embedding(config.n_vocab,config.n_embed)\n",
    "    \n",
    "    def forward(self,input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc33b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # wte = nn.Embedding(config.n_vocab,config.n_embed),\n",
    "            wpe = nn.Embedding(config.n_context,config.n_embed),\n",
    "            drop = nn.Dropout(0.1,inplace=False),\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        ))\n",
    "        \n",
    "        # self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "\n",
    "        self.small_mlp = nn.Linear(config.n_embed, config.n_embed)\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(config.n_embed),\n",
    "            nn.Linear(config.n_embed, config.n_embed),\n",
    "            nn.GELU()\n",
    "            )\n",
    "\n",
    "    def forward(self,input_embeddings,time_step, targets=None):\n",
    "        B,T,C = input_embeddings.size()\n",
    "        device = input_embeddings.device\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device).unsqueeze(0)  # (1,T)\n",
    "        x = input_embeddings +  self.transformer.wpe(pos)  # (B,T,C) pytorch does braodcasting for the position embeddingss and adds them to the token embeddings \n",
    "        \n",
    "        time_emb = self.time_embed(time_step) # (B, C)\n",
    "        x= x + time_emb.unsqueeze(1)  # (B, T, C)\n",
    "        \n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)  # (B,T,C)\n",
    "        # logits = self.lm_head(x)  # (B,T,vocab_size) \n",
    "        # we don't need the head since we are not doing autoregressive language modeling\n",
    "        \n",
    "        # we want to predict the starting sequence before the noising part.\n",
    "        x = self.small_mlp(x)  # (B,T,C)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    # takes x0 (B,T,C) and give a softmax over vocab size           \n",
    "        self.l1 = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.l1(x)\n",
    "        # x = F.softmax(x,dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionLM(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = LMEmbedding(config)\n",
    "        self.denoiser = Denoiser(config)\n",
    "        self.decoder = Decoding(config)\n",
    "        \n",
    "    def forward(self,input_ids,time_step, targets=None):\n",
    "        input_embeddings = self.embedding(input_ids)  # (B,T,C)\n",
    "        x = self.denoiser(input_embeddings,time_step, targets)  # (B,T,C)\n",
    "        logits = x@self.embedding.embed.weight.T  # (B,T,vocab_size)\n",
    "        \n",
    "        return x, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556f0a6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd367c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self, max_len):\n",
    "        tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "        self.special_tokens = {\n",
    "            \"<pad>\": tokenizer.n_vocab,\n",
    "            \"<bos>\": tokenizer.n_vocab + 1,\n",
    "            \"<eos>\": tokenizer.n_vocab + 2,\n",
    "        }\n",
    "        print(tokenizer.n_vocab)\n",
    "        self.tokenizer = tiktoken.Encoding(\n",
    "            name=\"r50k_base_ext\",\n",
    "            pat_str=tokenizer._pat_str,\n",
    "            mergeable_ranks=tokenizer._mergeable_ranks,\n",
    "            special_tokens=self.special_tokens,\n",
    "        )\n",
    "        self.n_vocab = self.tokenizer.n_vocab \n",
    "        self.max_len = max_len\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        for tok in (\"<pad>\", \"<bos>\", \"<eos>\"):\n",
    "            text = text.replace(tok, \"\")\n",
    "        return text\n",
    "    \n",
    "    def encode(self, text,max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "        # text = self.clean_text(text)\n",
    "        ids = self.tokenizer.encode(text, allowed_special=set())\n",
    "        ids = [self.special_tokens[\"<bos>\"]] + ids + [self.special_tokens[\"<eos>\"]]\n",
    "\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "            ids[-1] = self.special_tokens[\"<eos>\"]\n",
    "        else:\n",
    "            ids += [self.special_tokens[\"<pad>\"]] * (max_len - len(ids))\n",
    "        return ids  \n",
    "\n",
    "    def decode(self, ids):\n",
    "        \n",
    "        return self.tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33eff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer(max_len=13)\n",
    "tokenizer.decode(tokenizer.encode(\"Hello, tiktoken is fast!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = gpt2config(n_vocab=tokenizer.n_vocab)\n",
    "model = DiffusionLM(config).to(device)\n",
    "print(f\"Total Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(config.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "07adc863",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Invalid token for decoding: 50256'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m50256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[161], line 41\u001b[0m, in \u001b[0;36mMyTokenizer.decode\u001b[0;34m(self, ids)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IRL_AGV/lib/python3.10/site-packages/tiktoken/core.py:284\u001b[0m, in \u001b[0;36mEncoding.decode\u001b[0;34m(self, tokens, errors)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: Sequence[\u001b[38;5;28mint\u001b[39m], errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decodes a list of tokens into a string.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Invalid token for decoding: 50256'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode([50256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bb465",
   "metadata": {},
   "source": [
    "## Testing Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb61042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"Once upon a time in a land far away, there lived a\"\n",
    "sample_tokens = tokenizer.encode(sample_input)\n",
    "sample_input_ids = torch.tensor([sample_tokens], device=device)  # (1, sequence_length)\n",
    "sample_time_step = torch.tensor([10], device=device)  # (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aeb074",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output, sample_logits = model(sample_input_ids, sample_time_step)  # (1, sequence_length, n_embed)\n",
    "\n",
    "def finalize_tokens(x0_final, embedding_weights):\n",
    "    \"\"\"\n",
    "    Converts the final denoised latent into discrete token IDs.\n",
    "    Args:\n",
    "        x0_final: Tensor of shape (B, T, C)\n",
    "        embedding_weights: Tensor of shape (Vocab, C)\n",
    "    \"\"\"\n",
    "    # Fix: x2 must be 3D to match x1 (B, T, C)\n",
    "    # Unsqueeze(0) makes it (1, Vocab, C), and PyTorch broadcasts it to (B, Vocab, C)\n",
    "    distances = torch.cdist(x0_final, embedding_weights.unsqueeze(0), p=2) #(B,T,Vocab)  \n",
    "    token_ids = torch.argmin(distances, dim=-1) #(B, T)\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "token_ids = finalize_tokens(sample_output, model.embedding.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44339dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fc5ff61",
   "metadata": {},
   "source": [
    "## Forward Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_diffusion(x0, t, alphas):\n",
    "    \"\"\"\n",
    "    Directly samples x_t from x_0 at a specific timestep[cite: 109, 170].\n",
    "    \n",
    "    Args:\n",
    "        x0: Clean embeddings (B, SeqLen, EmbedDim) [cite: 126]\n",
    "        t: Timesteps for the batch (B,) \n",
    "        alphas: Precomputed signal schedule from get_alphas()\n",
    "    \"\"\"\n",
    "    # Select alpha_bar for each batch item and reshape for broadcasting\n",
    "    a = alphas[t].view(-1, 1, 1).to(x0.device)\n",
    "    \n",
    "    # Sample Gaussian noise with same shape as x0\n",
    "    noise = torch.randn_like(x0)\n",
    "    \n",
    "    # Formula: x_t = sqrt(alpha_bar) * x0 + sqrt(1 - alpha_bar) * noise [cite: 169]\n",
    "    print(\"sqrt a avg:\",torch.sqrt(a).mean())\n",
    "    xt = torch.sqrt(a) * x0 + torch.sqrt(1 - a) * noise\n",
    "    \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ae511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_input = fwd_diffusion(model.embedding(sample_input_ids), torch.tensor([1000], device=device), alphas)\n",
    "\n",
    "# token_ids = finalize_tokens(noisy_input, model.embedding.embed.weight)\n",
    "# decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "# print(\"Decoded Text:\",decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21044d7b",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load E2E dataset - extract text from 'ref' column\n",
    "df = pd.read_csv('datasets/e2e-dataset/trainset.csv')\n",
    "text = ' '.join(df['ref'].tolist())\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"First sample: {df['ref'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37baff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.9 * len(df))\n",
    "train_df = df[:train_size].reset_index(drop=True)\n",
    "test_df = df[train_size:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "# Pre-encode all sequences for training efficiency\n",
    "print(\"\\nEncoding training data...\")\n",
    "train_encoded = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    encoded = tokenizer.encode(row['ref'], max_len=64)  # Use fixed sequence length\n",
    "    train_encoded.append(encoded)\n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        print(f\"Encoded {idx + 1}/{len(train_df)} train samples\")\n",
    "\n",
    "print(\"\\nEncoding test data...\")\n",
    "test_encoded = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    encoded = tokenizer.encode(row['ref'], max_len=64)\n",
    "    test_encoded.append(encoded)\n",
    "\n",
    "# Convert to tensors\n",
    "train_encoded = torch.tensor(train_encoded, dtype=torch.long)\n",
    "test_encoded = torch.tensor(test_encoded, dtype=torch.long)\n",
    "\n",
    "print(f\"\\nTrain encoded shape: {train_encoded.shape}\")\n",
    "print(f\"Test encoded shape: {test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function - optimized to use pre-encoded data\n",
    "def get_batch(split, batch_size=8, block_size=64, device=device):\n",
    "    # Select the appropriate pre-encoded dataset\n",
    "    data_encoded = train_encoded if split == 'train' else test_encoded\n",
    "    \n",
    "    # Randomly sample batch_size indices\n",
    "    indices = torch.randint(0, len(data_encoded), (batch_size,))\n",
    "    \n",
    "    # Get the encoded sequences directly\n",
    "    w_stack = data_encoded[indices].to(device)\n",
    "    \n",
    "    return w_stack\n",
    "\n",
    "# Test batch\n",
    "w_stack = get_batch('train', batch_size=4, block_size=64, device='cpu')\n",
    "print(f\"Batch shape: {w_stack.shape}\")\n",
    "print(f\"First sequence decoded: {tokenizer.decode(w_stack[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5b43",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd32855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "max_iters = 50000  \n",
    "learning_rate = 1e-4\n",
    "eval_iters = 500  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 32  # Larger batch for better GPU utilization\n",
    "sequence_length = 64\n",
    "T = 1000\n",
    "num_timestep_samples = 16  # Sample 8 timesteps per iteration for better gradient estimate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f390f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed alpha schedule - simple sqrt schedule\n",
    "t = torch.arange(0, T+1, device=device, dtype=torch.float32)\n",
    "alpha_bars = 1 - torch.sqrt(t / T)  # Goes from ~0 to 1-sqrt(1)=0\n",
    "alpha_bars = torch.clamp(alpha_bars, min=0.001, max=0.999)\n",
    "alphas = torch.zeros(T+1, device=device) #alpha_0 to alpha_T\n",
    "alphas[0] = alpha_bars[0]\n",
    "alphas[1:] = alpha_bars[1:] / alpha_bars[:-1]\n",
    "alphas = torch.clamp(alphas, min=0.001, max=0.999)\n",
    "\n",
    "# Precompute sqrt terms for efficiency\n",
    "sqrt_ab = torch.sqrt(alpha_bars)\n",
    "sqrt_1mab = torch.sqrt(1 - alpha_bars)\n",
    "\n",
    "print(f\"Alpha bars range: [{alpha_bars.min():.4f}, {alpha_bars.max():.4f}]\")\n",
    "print(f\"Alphas range: [{alphas.min():.4f}, {alphas.max():.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_model = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=0.0)\n",
    "lr_lambda = lambda step: 1.0 - (step / float(max_iters))\n",
    "scheduler_model = LambdaLR(optimizer_model, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed59718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean(x_t, x0, t, alpha_bars,alphas):\n",
    "    abar_t = alpha_bars[t]\n",
    "    abar_tm1 = alpha_bars[t-1] if t > 0 else torch.tensor(1.0, device=x_t.device)\n",
    "    # print(abar_t,abar_tm1)\n",
    "    coef1 = torch.sqrt(abar_tm1) * (1 - alphas[t]) / (1 - abar_t)\n",
    "    coef2 = torch.sqrt(alphas[t]) * (1 - abar_tm1) / (1 - abar_t)\n",
    "    # print(\"coef1 avg:\",coef1.mean())\n",
    "    # print(\"coef2 avg:\",coef2.mean())\n",
    "    return coef1 * x0 + coef2 * x_t\n",
    "\n",
    "round_start = int(0.2 * max_iters)    # start after geometry is formed\n",
    "round_warmup = int(0.5 * max_iters)   # ramp over 30% of training\n",
    "round_max_weight = 0.4\n",
    "def rounding_weight(it):\n",
    "    if it < round_start:\n",
    "        return 0.0\n",
    "    elif it > round_start + round_warmup:\n",
    "        return round_max_weight\n",
    "    else:\n",
    "        x = (it - round_start) / round_warmup\n",
    "        return round_max_weight * 0.5 * (1 - math.cos(math.pi * x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(0,max_iters):\n",
    "\n",
    "    w = get_batch('train', batch_size, sequence_length)   # (B,L)\n",
    "    w_emb = model.embedding(w)                            # (B,L,d)\n",
    "\n",
    "    # ---- sample x0 with small noise ----\n",
    "    x0 = w_emb + 0.1 * torch.randn_like(w_emb)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    \n",
    "    # ---- Sample multiple timesteps to approximate E_t[loss] ----\n",
    "    # This gives a better gradient estimate than sampling just one timestep\n",
    "    eps = torch.randn_like(x0)\n",
    "    denoising_loss = 0.0\n",
    "    for _ in range(num_timestep_samples):\n",
    "        t_random = torch.randint(1, T+1, (batch_size,), device=device)\n",
    "        \n",
    "        # Generate noised version at those timesteps\n",
    "        t_idx = t_random   # Convert to 0-indexed\n",
    "        sqrt_ab_t = sqrt_ab[t_idx].view(batch_size, 1, 1)\n",
    "        sqrt_1mab_t = sqrt_1mab[t_idx].view(batch_size, 1, 1)\n",
    "        \n",
    "        xt = sqrt_ab_t * x0 + sqrt_1mab_t * eps\n",
    "        x0_hat = model.denoiser(xt, t_random)\n",
    "        \n",
    "        # Accumulate denoising loss over sampled timesteps\n",
    "        denoising_loss += F.mse_loss(x0_hat, x0)\n",
    "    \n",
    "    # Average over timestep samples\n",
    "    # denoising_loss = denoising_loss / num_timestep_samples\n",
    "    total_loss += denoising_loss\n",
    "    \n",
    "    # ---- Posterior mean regularization at timestep T ----\n",
    "    # Sample at the final timestep T\n",
    "    t_T = torch.full((batch_size,), T, device=device)\n",
    "    xT = sqrt_ab[-1] * x0 + sqrt_1mab[-1] * eps\n",
    "    \n",
    "    # Predict x0 from xT\n",
    "    x0_hat_T = model.denoiser(xT, t_T)\n",
    "    \n",
    "    # Compute posterior mean and regularize it to be close to zero\n",
    "    # mu_hat_T = posterior_mean(xT, x0_hat_T, T, alpha_bars)\n",
    "    mu_hat_T = posterior_mean(xT, x0, T, alpha_bars,alphas)\n",
    "\n",
    "    posterior_loss = mu_hat_T.pow(2).mean()\n",
    "    total_loss += posterior_loss\n",
    "    # posterior_loss = torch.tensor([0],device=device)\n",
    "    # ---- t=1 anchor loss (every iteration for stability) ----\n",
    "    # This ensures the denoised output matches actual word embeddings\n",
    "    xt_1 = sqrt_ab[1] * x0 + sqrt_1mab[1] * torch.rand_like(x0)\n",
    "    x0_hat_1 = model.denoiser(xt_1, torch.ones(batch_size, device=device))\n",
    "    anchor_loss = F.mse_loss(x0_hat_1, w_emb)\n",
    "    total_loss += anchor_loss\n",
    "\n",
    "    # ---- rounding loss (tied weights) ----\n",
    "    # Use x0_hat_1 from anchor step for rounding loss\n",
    "    # logits = torch.matmul(x0_hat_1, model.embedding.embed.weight.T)  # (B,L,V)\n",
    "    logits = x0_hat_1 @ model.embedding.embed.weight.T\n",
    "\n",
    "    rounding_loss =  rounding_weight(it) * F.cross_entropy(logits.view(-1, config.n_vocab), w.view(-1))\n",
    "    total_loss += rounding_loss  # Scale to balance losses\n",
    "    # rounding_loss =torch.tensor([0],device=device)\n",
    "    # ----/ optimize ----\n",
    "    optimizer_model.zero_grad(set_to_none=True)\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer_model.step()\n",
    "    scheduler_model.step()\n",
    "\n",
    "    if it % eval_iters == 0:\n",
    "        print(f\"Iter {it}: loss = {total_loss.item():.4f}, denoising = {denoising_loss.item():.4f}, posterior = {posterior_loss.item():.4f}, anchor = {anchor_loss.item():.4f}, rounding = {rounding_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = get_batch('train', 1, sequence_length)\n",
    "    x0 = model.embedding(w)\n",
    "    eps = torch.randn_like(x0)\n",
    "\n",
    "    for t in [1, T//4, T//2, T]:\n",
    "        xt = sqrt_ab[t-1] * x0 + sqrt_1mab[t-1] * eps\n",
    "        print(t, torch.norm(xt - x0).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc432a96",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad887e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion_with_clamping(model, alpha_bars, T, context_length=50, batch_size=1,clamping_start=0.4):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from pure noise: x_T ~ N(0, I)\n",
    "    x_t = torch.randn(batch_size, context_length, config.n_embed, device=device)\n",
    "    with torch.no_grad():\n",
    "        # Reverse diffusion: t = T, T-1, ..., 1, 0\n",
    "        for t_step in range(T ,0, -1):\n",
    "            if t_step % 1 == 0 or t_step == T:\n",
    "                pass\n",
    "            else:\n",
    "                continue \n",
    "            \n",
    "            t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "\n",
    "            x0_pred = model.denoiser(x_t, t_tensor)\n",
    "            \n",
    "            if t_step > clamping_start * T:\n",
    "                x0_clamped_tokens = finalize_tokens(x0_pred, model.embedding.embed.weight)\n",
    "                x0_clamped = model.embedding(x0_clamped_tokens)  # (B, T, C)\n",
    "            else:\n",
    "                x0_clamped = x0_pred\n",
    "                        \n",
    "            epsilon = torch.randn_like(x_t)\n",
    "\n",
    "            # Compute x_{t-1} using the closed-form update\n",
    "            x_t = torch.sqrt(alpha_bars[t_step - 1]) * x0_clamped + torch.sqrt(1 - alpha_bars[t_step - 1]) * epsilon\n",
    "          \n",
    "            # Convert to tokens using argmin rounding with learned embeddings\n",
    "            generated_tokens = finalize_tokens(x0_clamped, model.embedding.embed.weight)\n",
    "            \n",
    "            # Decode to text\n",
    "            generated_text = []\n",
    "            for i in range(batch_size):\n",
    "                text = tokenizer.decode(generated_tokens[i].tolist())\n",
    "                generated_text.append(text)\n",
    "            \n",
    "            if t_step % 50 == 0 or t_step == T:\n",
    "                print(f\"\\nTimestep {t_step}:\")\n",
    "                for i in range(batch_size):\n",
    "                    print(f\"Sample {i+1}: {generated_text[i]}\")\n",
    "\n",
    "    return generated_tokens, generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86590613",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting reverse diffusion inference with clamping...\")\n",
    "context_length = 100\n",
    "generated_tokens, generated_text = reverse_diffusion_with_clamping(\n",
    "    model=model,\n",
    "    alpha_bars=alpha_bars,\n",
    "    T=T,\n",
    "    context_length=context_length,\n",
    "    batch_size=1,\n",
    "    clamping_start=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3235a9",
   "metadata": {},
   "source": [
    "## Visualizing the Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_static_3d(emb_func, vocab_list, top_n=500):\n",
    "    # 1. Force everything to CPU and Float32 immediately\n",
    "    embeddings = emb_func.embed.weight[:top_n].detach().cpu().float().numpy()\n",
    "    \n",
    "    print(f\"Running t-SNE on {top_n} points...\")\n",
    "    # Lower perplexity for fewer points to prevent hanging\n",
    "    tsne = TSNE(n_components=3, perplexity=min(30, top_n-1), init='pca', verbose=1)\n",
    "    embeds_3d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # 2. Simple Matplotlib 3D plot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    ax.scatter(embeds_3d[:, 0], embeds_3d[:, 1], embeds_3d[:, 2], alpha=0.6)\n",
    "    \n",
    "    # Label a few random points so you can see if words cluster\n",
    "    for i in range(0, top_n, top_n // 10): \n",
    "        ax.text(embeds_3d[i, 0], embeds_3d[i, 1], embeds_3d[i, 2], vocab_list[i])\n",
    "\n",
    "    plt.title(\"Static 3D Embedding View\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0515164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_embeddings_2d(emb_func, vocab_list, top_n=5000):\n",
    "    # 1. Extract and Clean Data\n",
    "    # Convert to CPU, Float32, and Numpy immediately to prevent hanging\n",
    "    embeddings = emb_func.weight[:top_n].detach().cpu().float().numpy()\n",
    "    \n",
    "    # 2. PCA Pre-reduction (768 -> 50)\n",
    "    # This removes noise and makes t-SNE significantly faster and more stable\n",
    "    print(\"Pre-reducing dimensions with PCA...\")\n",
    "    pca = PCA(n_components=50)\n",
    "    embeddings_reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "    # 3. 2D t-SNE\n",
    "    print(f\"Running 2D t-SNE on {top_n} tokens...\")\n",
    "    # perplexity 30 is standard; init='pca' is faster than 'random'\n",
    "    tsne = TSNE(n_components=2, perplexity=30, init='pca', verbose=1, random_state=42)\n",
    "    embeds_2d = tsne.fit_transform(embeddings_reduced)\n",
    "\n",
    "    # 4. Plotting with Matplotlib (Reliable & Fast)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embeds_2d[:, 0], embeds_2d[:, 1], alpha=0.5, s=5, c='blue')\n",
    "\n",
    "    # Label a subset of words to verify clusters\n",
    "    # We label every 100th word so the plot isn't a mess of text\n",
    "    for i in range(0, top_n, 100):\n",
    "        plt.annotate(vocab_list[i], (embeds_2d[i, 0], embeds_2d[i, 1]), \n",
    "                     fontsize=8, alpha=0.8, weight='bold')\n",
    "\n",
    "    plt.title(f\"Diffusion-LM Latent Space (Top {top_n} Tokens)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456db1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([50256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a49fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_itos_list = [tokenizer.decode([i]) for i in range(config.n_vocab-4)or range(config.n_vocab-3,config.n_vocab) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01589779",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_2d(model.embedding.embed, my_vocab_itos_list[:3000], top_n=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baadbc7c",
   "metadata": {},
   "source": [
    "## Saving Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model checkpoints\n",
    "# import os\n",
    "\n",
    "# # Create checkpoints directory if it doesn't exist\n",
    "# checkpoint_dir = 'saved_models/checkpoints__2k_100k_norm'\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# # Save all three model components\n",
    "# checkpoint = {\n",
    "#     'config': config,\n",
    "#     'emb_func_state_dict': emb_func.state_dict(),\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'decoder_state_dict': decoder.state_dict(),\n",
    "#     'alphas': alphas,\n",
    "#     'T': T\n",
    "# }\n",
    "\n",
    "# checkpoint_path = os.path.join(checkpoint_dir, 'diff_lm_checkpoint.pt')\n",
    "# torch.save(checkpoint, checkpoint_path)\n",
    "# print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# # Optionally, also save individual components\n",
    "# torch.save(emb_func.state_dict(), os.path.join(checkpoint_dir, 'emb_func.pt'))\n",
    "# torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'denoiser_model.pt'))\n",
    "# torch.save(decoder.state_dict(), os.path.join(checkpoint_dir, 'decoder.pt'))\n",
    "# print(\"Individual model components saved separately\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05282d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
