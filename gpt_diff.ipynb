{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "from dataclasses import dataclass\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904545f",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class gpt2config:\n",
    "    n_vocab: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_embed: int = 128\n",
    "    n_context: int = 1024\n",
    "    n_head: int = 8\n",
    "    n_timesteps: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fea93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        \n",
    "        # Create a causal mask (lower triangular matrix) and register it as a buffer\n",
    "        # A buffer is not a parameter, but is saved with the model state_dict\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_context, config.n_context))\n",
    "                                     .view(1, 1, config.n_context, config.n_context))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        \n",
    "        # --- MASKING STARTS HERE ---\n",
    "        # Apply the causal mask: fill \"future\" positions with -infinity\n",
    "        # This makes their softmax probability zero.\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # --- MASKING ENDS HERE ---\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, hs)\n",
    "        \n",
    "        # Re-assemble all head outputs side-by-side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ba8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) #\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_embeds = SinusoidalPositionEmbeddings(100)\n",
    "time = 10\n",
    "time = torch.tensor([time], device=device)\n",
    "out = sine_embeds(time)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd61879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embed = nn.Embedding(config.n_vocab,config.n_embed)\n",
    "    \n",
    "    def forward(self,input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc33b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # wte = nn.Embedding(config.n_vocab,config.n_embed),\n",
    "            wpe = nn.Embedding(config.n_context,config.n_embed),\n",
    "            drop = nn.Dropout(0.1,inplace=False),\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        ))\n",
    "        \n",
    "        # self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "\n",
    "        self.small_mlp = nn.Linear(config.n_embed, config.n_embed)\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(config.n_embed),\n",
    "            nn.Linear(config.n_embed, config.n_embed),\n",
    "            nn.GELU()\n",
    "            )\n",
    "\n",
    "    def forward(self,input_embeddings,time_step, targets=None):\n",
    "        B,T,C = input_embeddings.size()\n",
    "        device = input_embeddings.device\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device).unsqueeze(0)  # (1,T)\n",
    "        x = input_embeddings +  self.transformer.wpe(pos)  # (B,T,C) pytorch does braodcasting for the position embeddingss and adds them to the token embeddings \n",
    "        \n",
    "        time_emb = self.time_embed(time_step) # (B, C)\n",
    "        x= x + time_emb.unsqueeze(1)  # (B, T, C)\n",
    "        \n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)  # (B,T,C)\n",
    "        # logits = self.lm_head(x)  # (B,T,vocab_size) \n",
    "        # we don't need the head since we are not doing autoregressive language modeling\n",
    "        \n",
    "        # we want to predict the starting sequence before the noising part.\n",
    "        x = self.small_mlp(x)  # (B,T,C)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    # takes x0 (B,T,C) and give a softmax over vocab size           \n",
    "        self.l1 = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.l1(x)\n",
    "        # x = F.softmax(x,dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556f0a6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd367c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 1. Load the tokenizer for GPT-4o\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "print(\"vocab:\",tokenizer.n_vocab)\n",
    "# 2. Convert text to tokens\n",
    "text = \"Hello, tiktoken is fast!\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Token Count: {len(tokens)}\")\n",
    "\n",
    "# 3. Convert back to original text\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "\n",
    "config = gpt2config(n_vocab=tokenizer.n_vocab)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_func = LMEmbedding(config).to(device)\n",
    "model = Denoiser(config).to(device)\n",
    "decoder = Decoding(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Embedding parameters: {sum(p.numel() for p in emb_func.parameters())/1e6:.2f}M\")\n",
    "print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb61042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"Once upon a time in a land far away, there lived a\"\n",
    "sample_tokens = tokenizer.encode(sample_input)\n",
    "sample_input_ids = torch.tensor([sample_tokens], device=device)  # (1, sequence_length)\n",
    "sample_time_step = torch.tensor([10], device=device)  # (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aeb074",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model(emb_func(sample_input_ids), sample_time_step)  # (1, sequence_length, n_embed)\n",
    "\n",
    "def finalize_tokens(x0_final, embedding_weights):\n",
    "    \"\"\"\n",
    "    Converts the final denoised latent into discrete token IDs.\n",
    "    Args:\n",
    "        x0_final: Tensor of shape (B, T, C)\n",
    "        embedding_weights: Tensor of shape (Vocab, C)\n",
    "    \"\"\"\n",
    "    # Fix: x2 must be 3D to match x1 (B, T, C)\n",
    "    # Unsqueeze(0) makes it (1, Vocab, C), and PyTorch broadcasts it to (B, Vocab, C)\n",
    "    distances = torch.cdist(x0_final, embedding_weights.unsqueeze(0), p=2) #(B,T,Vocab)  \n",
    "    token_ids = torch.argmin(distances, dim=-1) #(B, T)\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "token_ids = finalize_tokens(sample_output, emb_func.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5ff61",
   "metadata": {},
   "source": [
    "## Forward Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas(T=2000, s=1e-4):\n",
    "    \"\"\"\n",
    "    Computes the bar_alpha (signal) schedule for Diffusion-LM[cite: 232, 483].\n",
    "    s: constant determining initial noise level (standard dev = 0.1)[cite: 515].\n",
    "    \"\"\"\n",
    "    t = torch.linspace(0, T, T + 1)\n",
    "    # Sqrt schedule: alpha_bar = 1 - sqrt(t/T + s) \n",
    "    alphas = 1 - torch.sqrt(t / T )\n",
    "    \n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_diffusion(x0, t, alphas):\n",
    "    \"\"\"\n",
    "    Directly samples x_t from x_0 at a specific timestep[cite: 109, 170].\n",
    "    \n",
    "    Args:\n",
    "        x0: Clean embeddings (B, SeqLen, EmbedDim) [cite: 126]\n",
    "        t: Timesteps for the batch (B,) \n",
    "        alphas: Precomputed signal schedule from get_alphas()\n",
    "    \"\"\"\n",
    "    # Select alpha_bar for each batch item and reshape for broadcasting\n",
    "    a = alphas[t].view(-1, 1, 1).to(x0.device)\n",
    "    \n",
    "    # Sample Gaussian noise with same shape as x0\n",
    "    noise = torch.randn_like(x0)\n",
    "    \n",
    "    # Formula: x_t = sqrt(alpha_bar) * x0 + sqrt(1 - alpha_bar) * noise [cite: 169]\n",
    "    xt = torch.sqrt(a) * x0 + torch.sqrt(1 - a) * noise\n",
    "    \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ae511",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = get_alphas().to(device)\n",
    "\n",
    "noisy_input = fwd_diffusion(emb_func.embed(sample_input_ids), torch.tensor([1000], device=device), alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_ids = finalize_tokens(noisy_input, emb_func.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21044d7b",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tiny shakespeare dataset\n",
    "with open('datasets/ROCStories_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 100 characters:\\n{text[0:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37baff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset\n",
    "data = tokenizer.encode(text)\n",
    "print(f\"Encoded length: {len(data)} tokens\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "print(f\"Train tokens: {len(train_data)}, Val tokens: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function\n",
    "def get_batch(split, batch_size=8, block_size=256):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    w_stack = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix])\n",
    "    # y = torch.stack([torch.tensor(data[i+1:i+block_size+1]) for i in ix])\n",
    "    w_stack = w_stack.to(device)\n",
    "    return w_stack\n",
    "\n",
    "# Test batch\n",
    "w_stack = get_batch('train')\n",
    "print(f\"Batch shape: {w_stack.shape}\")\n",
    "print(w_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5b43",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ea6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd32855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "max_iters = 100000  \n",
    "eval_interval = 10  # Evaluate less frequently\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 500  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 64  # Larger batch for better GPU utilization\n",
    "sequence_length = 64\n",
    "T = 2000\n",
    "alphas = get_alphas(T=T,s=1e-4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_model = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=0.0)\n",
    "# optimizer_model_decoder = torch.optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "optimizer_emb = torch.optim.AdamW(emb_func.parameters(), lr=learning_rate,weight_decay=0.0)\n",
    "# 3. Linear Decay Scheduler\n",
    "# This creates a factor that goes from 1.0 to 0.0 linearly over total_steps\n",
    "lr_lambda = lambda step: 1.0 - (step / float(max_iters))\n",
    "scheduler_model = LambdaLR(optimizer_model, lr_lambda=lr_lambda)\n",
    "# scheduler_model_decoder = LambdaLR(optimizer_model_decoder, lr_lambda=lr_lambda)\n",
    "scheduler_emb = LambdaLR(optimizer_emb, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3209994",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.l1.weight = emb_func.embed.weight\n",
    "for iter in range(max_iters):\n",
    "    # model.eval()\n",
    "    # decoder.eval()\n",
    "    # emb_func.eval()\n",
    "    w = get_batch('train', batch_size,block_size=sequence_length)  # already on device\n",
    "    w_emb = emb_func(w)  # (B, T, C)\n",
    "    x0 = w_emb + 0.1 * torch.randn_like(w_emb)  # use randn_like for Gaussian noise\n",
    "    \n",
    "    total_loss = 0\n",
    "    mu_T = fwd_diffusion(x0,torch.tensor([T]*batch_size,device=device),alphas)\n",
    "    total_loss += torch.mean(mu_T**2)\n",
    "    # print(total_loss)\n",
    "    # for t_step in range(T + 1):\n",
    "        # t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "        \n",
    "    t_tensor = torch.randint(1, T + 1, (batch_size,), device=device)\n",
    "    xt = fwd_diffusion(x0, t_tensor, alphas)\n",
    "    x0_cap = model(xt, t_tensor)\n",
    "    total_loss += F.mse_loss(x0_cap, x0)\n",
    "    \n",
    "    # Final step loss\n",
    "    t_one = torch.tensor([1] * batch_size, device=device)\n",
    "    total_loss += F.mse_loss(model(fwd_diffusion(x0, t_one, alphas), t_one), w_emb)\n",
    "\n",
    "    # Decoder cross-entropy loss\n",
    "    logits = decoder(x0_cap)  # (B, T, V)\n",
    "    V = config.n_vocab\n",
    "    logits_flat = logits.view(-1, V)  # (B*T, V)\n",
    "    targets_flat = w.view(-1)  # (B*T,)\n",
    "    # print(F.cross_entropy(logits_flat, targets_flat).item())\n",
    "    total_loss += F.cross_entropy(logits_flat, targets_flat)\n",
    "    # print(log_loss)\n",
    "\n",
    "    optimizer_model.zero_grad(set_to_none=True)\n",
    "    # optimizer_model_decoder.zero_grad(set_to_none=True)\n",
    "    optimizer_emb.zero_grad(set_to_none=True)\n",
    "    total_loss.backward()\n",
    "    optimizer_model.step() \n",
    "    # optimizer_model_decoder.step()\n",
    "    optimizer_emb.step()\n",
    "\n",
    "    scheduler_model.step()\n",
    "    # scheduler_model_decoder.step()\n",
    "    scheduler_emb.step()\n",
    "\n",
    "    # radius = 9\n",
    "    # with torch.no_grad():\n",
    "    #     # Projects all vectors back to a radius of 1.0\n",
    "    #     emb_func.embed.weight.div_(torch.norm(emb_func.embed.weight, dim=-1, keepdim=True)).mul_(radius)\n",
    "    if iter%eval_iters == 0:\n",
    "        print(f\"Iter {iter}: Loss {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad887e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion_with_clamping(model, emb_func, alphas, T, context_length=50, batch_size=1):\n",
    "\n",
    "    model.eval()\n",
    "    emb_func.eval()\n",
    "    \n",
    "    # Start from pure noise: x_T ~ N(0, I)\n",
    "    x_t = torch.randn(batch_size, context_length, config.n_embed, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Reverse diffusion: t = T, T-1, ..., 1, 0\n",
    "        for t_step in reversed(range(T + 1)):\n",
    "            if t_step % 1 == 0 or t_step == T:\n",
    "                pass\n",
    "            else:\n",
    "                continue \n",
    "            \n",
    "            # print(f\"Denoising step {t_step}/{T}\")\n",
    "            \n",
    "            # if t_step == 0:\n",
    "            #     # Final step: just clamp to get x_0\n",
    "            #     x_0 = x_t\n",
    "            #     break\n",
    "            \n",
    "            # Create timestep tensor for batch\n",
    "            t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "            \n",
    "            # Predict x_0 from x_t using the denoiser\n",
    "            x0_pred = model(x_t, t_tensor)\n",
    "            \n",
    "            # CLAMPING TRICK: Map predicted x_0 to nearest word embedding\n",
    "            # This forces intermediate predictions to be valid words\n",
    "            x0_clamped_tokens = finalize_tokens(x0_pred, emb_func.embed.weight)\n",
    "            x0_clamped = emb_func(x0_clamped_tokens)  # (B, T, C)\n",
    "            \n",
    "            # Compute x_{t-1} using the formula:\n",
    "            # x_{t-1} = sqrt(alpha_{t-1}) * x0_clamped + sqrt(1 - alpha_{t-1}) * epsilon\n",
    "            \n",
    "            alpha_t_prev = alphas[t_step - 1] if t_step > 0 else alphas[0]\n",
    "            \n",
    "            # Sample fresh noise\n",
    "            epsilon = torch.randn_like(x_t)\n",
    "            \n",
    "            # Update: x_{t-1} = sqrt(alpha_{t-1}) * x0_clamped + sqrt(1 - alpha_{t-1}) * epsilon\n",
    "            x_t = torch.sqrt(alpha_t_prev) * x0_clamped + torch.sqrt(1 - alpha_t_prev) * epsilon\n",
    "    \n",
    "            # Final denoised embeddings: x_0\n",
    "            x0_final = x_t\n",
    "            \n",
    "            # Convert to tokens using argmin rounding with learned embeddings\n",
    "            generated_tokens = finalize_tokens(x0_final, emb_func.embed.weight)\n",
    "            \n",
    "            # Decode to text\n",
    "            generated_text = []\n",
    "            for i in range(batch_size):\n",
    "                text = tokenizer.decode(generated_tokens[i].tolist())\n",
    "                generated_text.append(text)\n",
    "            \n",
    "            if t_step % 50 == 0 or t_step == T:\n",
    "                print(f\"\\nTimestep {t_step}:\")\n",
    "                for i in range(batch_size):\n",
    "                    print(f\"Sample {i+1}: {generated_text[i]}\")\n",
    "\n",
    "    return generated_tokens, generated_text\n",
    "\n",
    "\n",
    "# Run inference\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"GENERATED TEXT:\")\n",
    "# print(\"=\"*50)\n",
    "# print(generated_text[0])\n",
    "# print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc432a96",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86590613",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting reverse diffusion inference with clamping...\")\n",
    "context_length = 32\n",
    "generated_tokens, generated_text = reverse_diffusion_with_clamping(\n",
    "    model=model,\n",
    "    emb_func=emb_func,\n",
    "    alphas=alphas,\n",
    "    T=T,\n",
    "    context_length=context_length,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3235a9",
   "metadata": {},
   "source": [
    "## Visualizing the Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openTSNE import TSNE as FastTSNE\n",
    "# from sklearn.decomposition import PCA\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a18a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def visualize_embeddings_fast(emb_func, vocab_list):\n",
    "#     # 1. Get raw embeddings\n",
    "#     embeddings = emb_func.weight[:100].detach().cpu().numpy()\n",
    "    \n",
    "#     # 2. PCA Pre-reduction (Speed up the t-SNE neighbor search)\n",
    "#     print(\"Pre-reducing dimensions with PCA...\")\n",
    "#     pca_embeddings = PCA(n_components=50).fit_transform(embeddings)\n",
    "\n",
    "#     # 3. Fast t-SNE (openTSNE)\n",
    "#     # n_jobs=-1 uses all available CPU cores\n",
    "#     print(\"Running fast t-SNE (openTSNE)...\")\n",
    "#     tsne = FastTSNE(\n",
    "#         n_components=2, \n",
    "#         perplexity=30, \n",
    "#         initialization=\"pca\", \n",
    "#         n_jobs=-1,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     embeds_3d = tsne.fit(pca_embeddings)\n",
    "\n",
    "#     # 3. Create a DataFrame for Plotly\n",
    "#     import pandas as pd\n",
    "#     df = pd.DataFrame({\n",
    "#         'x': embeds_3d[:, 0],\n",
    "#         'y': embeds_3d[:, 1],\n",
    "#         'word': vocab_list\n",
    "#     })\n",
    "\n",
    "#     # 4. Plotly Interactive 2D Scatter\n",
    "#     fig = px.scatter(df, x='x', y='y', text='word', \n",
    "#                      title=\"Diffusion-LM Embedding Space (2D t-SNE)\")\n",
    "#     fig.update_traces(marker=dict(size=2))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_static_3d(emb_func, vocab_list, top_n=500):\n",
    "    # 1. Force everything to CPU and Float32 immediately\n",
    "    embeddings = emb_func.embed.weight[:top_n].detach().cpu().float().numpy()\n",
    "    \n",
    "    print(f\"Running t-SNE on {top_n} points...\")\n",
    "    # Lower perplexity for fewer points to prevent hanging\n",
    "    tsne = TSNE(n_components=3, perplexity=min(30, top_n-1), init='pca', verbose=1)\n",
    "    embeds_3d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # 2. Simple Matplotlib 3D plot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    ax.scatter(embeds_3d[:, 0], embeds_3d[:, 1], embeds_3d[:, 2], alpha=0.6)\n",
    "    \n",
    "    # Label a few random points so you can see if words cluster\n",
    "    for i in range(0, top_n, top_n // 10): \n",
    "        ax.text(embeds_3d[i, 0], embeds_3d[i, 1], embeds_3d[i, 2], vocab_list[i])\n",
    "\n",
    "    plt.title(\"Static 3D Embedding View\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_static_3d(emb_func, my_vocab_itos_list[:1000], top_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0515164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_embeddings_2d(emb_func, vocab_list, top_n=5000):\n",
    "    # 1. Extract and Clean Data\n",
    "    # Convert to CPU, Float32, and Numpy immediately to prevent hanging\n",
    "    embeddings = emb_func.weight[:top_n].detach().cpu().float().numpy()\n",
    "    \n",
    "    # 2. PCA Pre-reduction (768 -> 50)\n",
    "    # This removes noise and makes t-SNE significantly faster and more stable\n",
    "    print(\"Pre-reducing dimensions with PCA...\")\n",
    "    pca = PCA(n_components=50)\n",
    "    embeddings_reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "    # 3. 2D t-SNE\n",
    "    print(f\"Running 2D t-SNE on {top_n} tokens...\")\n",
    "    # perplexity 30 is standard; init='pca' is faster than 'random'\n",
    "    tsne = TSNE(n_components=2, perplexity=30, init='pca', verbose=1, random_state=42)\n",
    "    embeds_2d = tsne.fit_transform(embeddings_reduced)\n",
    "\n",
    "    # 4. Plotting with Matplotlib (Reliable & Fast)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embeds_2d[:, 0], embeds_2d[:, 1], alpha=0.5, s=5, c='blue')\n",
    "\n",
    "    # Label a subset of words to verify clusters\n",
    "    # We label every 100th word so the plot isn't a mess of text\n",
    "    for i in range(0, top_n, 100):\n",
    "        plt.annotate(vocab_list[i], (embeds_2d[i, 0], embeds_2d[i, 1]), \n",
    "                     fontsize=8, alpha=0.8, weight='bold')\n",
    "\n",
    "    plt.title(f\"Diffusion-LM Latent Space (Top {top_n} Tokens)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a49fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_itos_list = [tokenizer.decode([i]) for i in range(config.n_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01589779",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_2d(emb_func.embed, my_vocab_itos_list[:3000], top_n=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model checkpoints\n",
    "# import os\n",
    "\n",
    "# # Create checkpoints directory if it doesn't exist\n",
    "# checkpoint_dir = 'saved_models/checkpoints__2k_100k_norm'\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# # Save all three model components\n",
    "# checkpoint = {\n",
    "#     'config': config,\n",
    "#     'emb_func_state_dict': emb_func.state_dict(),\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'decoder_state_dict': decoder.state_dict(),\n",
    "#     'alphas': alphas,\n",
    "#     'T': T\n",
    "# }\n",
    "\n",
    "# checkpoint_path = os.path.join(checkpoint_dir, 'diff_lm_checkpoint.pt')\n",
    "# torch.save(checkpoint, checkpoint_path)\n",
    "# print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# # Optionally, also save individual components\n",
    "# torch.save(emb_func.state_dict(), os.path.join(checkpoint_dir, 'emb_func.pt'))\n",
    "# torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'denoiser_model.pt'))\n",
    "# torch.save(decoder.state_dict(), os.path.join(checkpoint_dir, 'decoder.pt'))\n",
    "# print(\"Individual model components saved separately\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05282d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
