{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3ee3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "from dataclasses import dataclass\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class gpt2config:\n",
    "    n_vocab: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_embed: int = 64\n",
    "    n_context: int = 1024\n",
    "    n_head: int = 8\n",
    "    n_timesteps: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17fea93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        \n",
    "        # Create a causal mask (lower triangular matrix) and register it as a buffer\n",
    "        # A buffer is not a parameter, but is saved with the model state_dict\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_context, config.n_context))\n",
    "                                     .view(1, 1, config.n_context, config.n_context))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "        \n",
    "        # --- MASKING STARTS HERE ---\n",
    "        # Apply the causal mask: fill \"future\" positions with -infinity\n",
    "        # This makes their softmax probability zero.\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # --- MASKING ENDS HERE ---\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, hs)\n",
    "        \n",
    "        # Re-assemble all head outputs side-by-side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4*config.n_embed)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4*config.n_embed, config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e52ba8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) #\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41a5b755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sine_embeds = SinusoidalPositionEmbeddings(100)\n",
    "time = 10\n",
    "time = torch.tensor([time], device=device)\n",
    "out = sine_embeds(time)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fd61879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embed = nn.Embedding(config.n_vocab,config.n_embed)\n",
    "    \n",
    "    def forward(self,input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fc33b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # wte = nn.Embedding(config.n_vocab,config.n_embed),\n",
    "            wpe = nn.Embedding(config.n_context,config.n_embed),\n",
    "            drop = nn.Dropout(0.1,inplace=False),\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embed,eps=1e-5,elementwise_affine=True)\n",
    "        ))\n",
    "        \n",
    "        # self.lm_head = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "\n",
    "        self.small_mlp = nn.Linear(config.n_embed, config.n_embed)\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(config.n_embed),\n",
    "            nn.Linear(config.n_embed, config.n_embed),\n",
    "            nn.GELU()\n",
    "            )\n",
    "\n",
    "    def forward(self,input_embeddings,time_step, targets=None):\n",
    "        B,T,C = input_embeddings.size()\n",
    "        device = input_embeddings.device\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device).unsqueeze(0)  # (1,T)\n",
    "        x = input_embeddings +  self.transformer.wpe(pos)  # (B,T,C) pytorch does braodcasting for the position embeddingss and adds them to the token embeddings \n",
    "        \n",
    "        time_emb = self.time_embed(time_step) # (B, C)\n",
    "        x= x + time_emb.unsqueeze(1)  # (B, T, C)\n",
    "        \n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)  # (B,T,C)\n",
    "        # logits = self.lm_head(x)  # (B,T,vocab_size) \n",
    "        # we don't need the head since we are not doing autoregressive language modeling\n",
    "        \n",
    "        # we want to predict the starting sequence before the noising part.\n",
    "        x = self.small_mlp(x)  # (B,T,C)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dbc6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "    # takes x0 (B,T,C) and give a softmax over vocab size           \n",
    "        self.l1 = nn.Linear(config.n_embed, config.n_vocab, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.l1(x)\n",
    "        # x = F.softmax(x,dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556f0a6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcd367c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 50257\n",
      "Token IDs: [15496, 11, 256, 1134, 30001, 318, 3049, 0]\n",
      "Token Count: 8\n",
      "Decoded: Hello, tiktoken is fast!\n",
      "gpt2config(n_vocab=50257, n_layer=8, n_embed=64, n_context=1024, n_head=8, n_timesteps=1000)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 1. Load the tokenizer for GPT-4o\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "print(\"vocab:\",tokenizer.n_vocab)\n",
    "# 2. Convert text to tokens\n",
    "text = \"Hello, tiktoken is fast!\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Token Count: {len(tokens)}\")\n",
    "\n",
    "# 3. Convert back to original text\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "\n",
    "config = gpt2config(n_vocab=tokenizer.n_vocab)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "523b58d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 0.47M\n",
      "Embedding parameters: 3.22M\n",
      "Decoder parameters: 3.22M\n"
     ]
    }
   ],
   "source": [
    "emb_func = LMEmbedding(config).to(device)\n",
    "model = Denoiser(config).to(device)\n",
    "decoder = Decoding(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Embedding parameters: {sum(p.numel() for p in emb_func.parameters())/1e6:.2f}M\")\n",
    "print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "deb61042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"Once upon a time in a land far away, there lived a\"\n",
    "sample_tokens = tokenizer.encode(sample_input)\n",
    "sample_input_ids = torch.tensor([sample_tokens], device=device)  # (1, sequence_length)\n",
    "sample_time_step = torch.tensor([10], device=device)  # (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9aeb074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb7111db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text:  dives nauseaiotics diveseker Sci Pediatrics249 militantsGray treaties Lebaneseagan\n"
     ]
    }
   ],
   "source": [
    "sample_output = model(emb_func(sample_input_ids), sample_time_step)  # (1, sequence_length, n_embed)\n",
    "\n",
    "def finalize_tokens(x0_final, embedding_weights):\n",
    "    \"\"\"\n",
    "    Converts the final denoised latent into discrete token IDs.\n",
    "    Args:\n",
    "        x0_final: Tensor of shape (B, T, C)\n",
    "        embedding_weights: Tensor of shape (Vocab, C)\n",
    "    \"\"\"\n",
    "    # Fix: x2 must be 3D to match x1 (B, T, C)\n",
    "    # Unsqueeze(0) makes it (1, Vocab, C), and PyTorch broadcasts it to (B, Vocab, C)\n",
    "    distances = torch.cdist(x0_final, embedding_weights.unsqueeze(0), p=2) #(B,T,Vocab)\n",
    "    # print(\"dist shape:\", distances.shape) \n",
    "    \n",
    "    # Argmin-rounding: Find the index with the minimum distance among all tokens in vocab\n",
    "    # Result shape: (B, T)\n",
    "    \n",
    "    token_ids = torch.argmin(distances, dim=-1)\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "token_ids = finalize_tokens(sample_output, emb_func.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5ff61",
   "metadata": {},
   "source": [
    "## Forward Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f91a0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas(T=2000, s=1e-4):\n",
    "    \"\"\"\n",
    "    Computes the bar_alpha (signal) schedule for Diffusion-LM[cite: 232, 483].\n",
    "    s: constant determining initial noise level (standard dev = 0.1)[cite: 515].\n",
    "    \"\"\"\n",
    "    t = torch.linspace(0, T, T + 1)\n",
    "    # Sqrt schedule: alpha_bar = 1 - sqrt(t/T + s) \n",
    "    alphas = 1 - torch.sqrt(t / T )\n",
    "    \n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ee8d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_sample(x0, t, alphas):\n",
    "    \"\"\"\n",
    "    Directly samples x_t from x_0 at a specific timestep[cite: 109, 170].\n",
    "    \n",
    "    Args:\n",
    "        x0: Clean embeddings (B, SeqLen, EmbedDim) [cite: 126]\n",
    "        t: Timesteps for the batch (B,) \n",
    "        alphas: Precomputed signal schedule from get_alphas()\n",
    "    \"\"\"\n",
    "    # Select alpha_bar for each batch item and reshape for broadcasting\n",
    "    a = alphas[t].view(-1, 1, 1).to(x0.device)\n",
    "    \n",
    "    # Sample Gaussian noise with same shape as x0\n",
    "    noise = torch.randn_like(x0)\n",
    "    \n",
    "    # Formula: x_t = sqrt(alpha_bar) * x0 + sqrt(1 - alpha_bar) * noise [cite: 169]\n",
    "    xt = torch.sqrt(a) * x0 + torch.sqrt(1 - a) * noise\n",
    "    \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca9ae511",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = get_alphas().to(device)\n",
    "\n",
    "noisy_input = fwd_sample(emb_func.embed(sample_input_ids), torch.tensor([1000], device=device), alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db9c5636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: Once ecstasy a time wrongly a land Eden away, there lived Bucc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_ids = finalize_tokens(noisy_input, emb_func.embed.weight)\n",
    "decoded_output = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(\"Decoded Text:\",decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21044d7b",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b2c8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 18007898 characters\n",
      "First 100 characters:\n",
      "The boy went to a video arcade. He played his favorite machine. His games didn't go very well. He to\n"
     ]
    }
   ],
   "source": [
    "# Load tiny shakespeare dataset\n",
    "with open('datasets/ROCStories_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 100 characters:\\n{text[0:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d37baff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded length: 4111142 tokens\n",
      "Train tokens: 3700027, Val tokens: 411115\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire dataset\n",
    "data = tokenizer.encode(text)\n",
    "print(f\"Encoded length: {len(data)} tokens\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "print(f\"Train tokens: {len(train_data)}, Val tokens: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfde8f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([8, 256])\n",
      "tensor([[ 5322,   663,  8971,  ..., 39517,  8848,    13],\n",
      "        [  679,  1043,  5762,  ...,  3072,    13,   198],\n",
      "        [  616,  3435,   717,  ...,   428,   257,  4171],\n",
      "        ...,\n",
      "        [ 4203, 30285,   355,  ...,  6949,   284,   787],\n",
      "        [ 1097,   373,  5017,  ...,   502, 20202,  6642],\n",
      "        [ 5500,   282,   318,  ...,  2968,   477,   780]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Data loader function\n",
    "def get_batch(split, batch_size=8, block_size=256):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    w_stack = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix])\n",
    "    # y = torch.stack([torch.tensor(data[i+1:i+block_size+1]) for i in ix])\n",
    "    w_stack = w_stack.to(device)\n",
    "    return w_stack\n",
    "\n",
    "# Test batch\n",
    "w_stack = get_batch('train')\n",
    "print(f\"Batch shape: {w_stack.shape}\")\n",
    "print(w_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5b43",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfd32855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "max_iters = 10000\n",
    "eval_interval = 10  # Evaluate less frequently\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 20  # Much fewer eval iterations (was 200!)\n",
    "batch_size = 16  # Larger batch for better GPU utilization\n",
    "T = 100\n",
    "alphas = get_alphas(T=T,s=1e-4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62f13c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loss():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e41f701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dff6babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_model = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer_model_decoder = torch.optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "optimizer_emb = torch.optim.AdamW(emb_func.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3209994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Loss 14.679441452026367\n",
      "Iter 100: Loss 12.138429641723633\n",
      "Iter 100: Loss 12.138429641723633\n",
      "Iter 200: Loss 10.456873893737793\n",
      "Iter 200: Loss 10.456873893737793\n",
      "Iter 300: Loss 8.942022323608398\n",
      "Iter 300: Loss 8.942022323608398\n",
      "Iter 400: Loss 7.252367973327637\n",
      "Iter 400: Loss 7.252367973327637\n",
      "Iter 500: Loss 5.751605987548828\n",
      "Iter 500: Loss 5.751605987548828\n",
      "Iter 600: Loss 4.887882232666016\n",
      "Iter 600: Loss 4.887882232666016\n",
      "Iter 700: Loss 4.176258563995361\n",
      "Iter 700: Loss 4.176258563995361\n",
      "Iter 800: Loss 3.919649124145508\n",
      "Iter 800: Loss 3.919649124145508\n",
      "Iter 900: Loss 3.6510186195373535\n",
      "Iter 900: Loss 3.6510186195373535\n",
      "Iter 1000: Loss 3.4616472721099854\n",
      "Iter 1000: Loss 3.4616472721099854\n",
      "Iter 1100: Loss 3.1809492111206055\n",
      "Iter 1100: Loss 3.1809492111206055\n",
      "Iter 1200: Loss 3.0598251819610596\n",
      "Iter 1200: Loss 3.0598251819610596\n",
      "Iter 1300: Loss 3.0905849933624268\n",
      "Iter 1300: Loss 3.0905849933624268\n",
      "Iter 1400: Loss 2.691953659057617\n",
      "Iter 1400: Loss 2.691953659057617\n",
      "Iter 1500: Loss 2.7067818641662598\n",
      "Iter 1500: Loss 2.7067818641662598\n",
      "Iter 1600: Loss 2.6354711055755615\n",
      "Iter 1600: Loss 2.6354711055755615\n",
      "Iter 1700: Loss 2.505795955657959\n",
      "Iter 1700: Loss 2.505795955657959\n",
      "Iter 1800: Loss 2.391904354095459\n",
      "Iter 1800: Loss 2.391904354095459\n",
      "Iter 1900: Loss 2.454244613647461\n",
      "Iter 1900: Loss 2.454244613647461\n",
      "Iter 2000: Loss 2.458224296569824\n",
      "Iter 2000: Loss 2.458224296569824\n",
      "Iter 2100: Loss 2.530756950378418\n",
      "Iter 2100: Loss 2.530756950378418\n",
      "Iter 2200: Loss 2.2626564502716064\n",
      "Iter 2200: Loss 2.2626564502716064\n",
      "Iter 2300: Loss 2.30617094039917\n",
      "Iter 2300: Loss 2.30617094039917\n",
      "Iter 2400: Loss 2.313467502593994\n",
      "Iter 2400: Loss 2.313467502593994\n",
      "Iter 2500: Loss 2.2187867164611816\n",
      "Iter 2500: Loss 2.2187867164611816\n",
      "Iter 2600: Loss 2.167726993560791\n",
      "Iter 2600: Loss 2.167726993560791\n",
      "Iter 2700: Loss 2.1174590587615967\n",
      "Iter 2700: Loss 2.1174590587615967\n",
      "Iter 2800: Loss 2.2759082317352295\n",
      "Iter 2800: Loss 2.2759082317352295\n",
      "Iter 2900: Loss 2.1125571727752686\n",
      "Iter 2900: Loss 2.1125571727752686\n",
      "Iter 3000: Loss 2.142490863800049\n",
      "Iter 3000: Loss 2.142490863800049\n",
      "Iter 3100: Loss 1.9675571918487549\n",
      "Iter 3100: Loss 1.9675571918487549\n",
      "Iter 3200: Loss 2.03946590423584\n",
      "Iter 3200: Loss 2.03946590423584\n",
      "Iter 3300: Loss 2.0116539001464844\n",
      "Iter 3300: Loss 2.0116539001464844\n",
      "Iter 3400: Loss 1.966963529586792\n",
      "Iter 3400: Loss 1.966963529586792\n",
      "Iter 3500: Loss 1.9218482971191406\n",
      "Iter 3500: Loss 1.9218482971191406\n",
      "Iter 3600: Loss 2.07139253616333\n",
      "Iter 3600: Loss 2.07139253616333\n",
      "Iter 3700: Loss 1.8881922960281372\n",
      "Iter 3700: Loss 1.8881922960281372\n",
      "Iter 3800: Loss 2.017956018447876\n",
      "Iter 3800: Loss 2.017956018447876\n",
      "Iter 3900: Loss 2.0020527839660645\n",
      "Iter 3900: Loss 2.0020527839660645\n",
      "Iter 4000: Loss 1.9044729471206665\n",
      "Iter 4000: Loss 1.9044729471206665\n",
      "Iter 4100: Loss 1.9734435081481934\n",
      "Iter 4100: Loss 1.9734435081481934\n",
      "Iter 4200: Loss 1.8605822324752808\n",
      "Iter 4200: Loss 1.8605822324752808\n",
      "Iter 4300: Loss 1.8149505853652954\n",
      "Iter 4300: Loss 1.8149505853652954\n",
      "Iter 4400: Loss 1.8879613876342773\n",
      "Iter 4400: Loss 1.8879613876342773\n",
      "Iter 4500: Loss 1.884429693222046\n",
      "Iter 4500: Loss 1.884429693222046\n",
      "Iter 4600: Loss 1.8076356649398804\n",
      "Iter 4600: Loss 1.8076356649398804\n",
      "Iter 4700: Loss 1.8594251871109009\n",
      "Iter 4700: Loss 1.8594251871109009\n",
      "Iter 4800: Loss 1.7008068561553955\n",
      "Iter 4800: Loss 1.7008068561553955\n",
      "Iter 4900: Loss 1.8235844373703003\n",
      "Iter 4900: Loss 1.8235844373703003\n",
      "Iter 5000: Loss 1.7774951457977295\n",
      "Iter 5000: Loss 1.7774951457977295\n",
      "Iter 5100: Loss 1.795661449432373\n",
      "Iter 5100: Loss 1.795661449432373\n",
      "Iter 5200: Loss 1.669661045074463\n",
      "Iter 5200: Loss 1.669661045074463\n",
      "Iter 5300: Loss 1.6151888370513916\n",
      "Iter 5300: Loss 1.6151888370513916\n",
      "Iter 5400: Loss 1.7362163066864014\n",
      "Iter 5400: Loss 1.7362163066864014\n",
      "Iter 5500: Loss 1.6488968133926392\n",
      "Iter 5500: Loss 1.6488968133926392\n",
      "Iter 5600: Loss 1.7115659713745117\n",
      "Iter 5600: Loss 1.7115659713745117\n",
      "Iter 5700: Loss 1.8184798955917358\n",
      "Iter 5700: Loss 1.8184798955917358\n",
      "Iter 5800: Loss 1.6386901140213013\n",
      "Iter 5800: Loss 1.6386901140213013\n",
      "Iter 5900: Loss 1.632328987121582\n",
      "Iter 5900: Loss 1.632328987121582\n",
      "Iter 6000: Loss 1.7136452198028564\n",
      "Iter 6000: Loss 1.7136452198028564\n",
      "Iter 6100: Loss 1.7026598453521729\n",
      "Iter 6100: Loss 1.7026598453521729\n",
      "Iter 6200: Loss 1.68500554561615\n",
      "Iter 6200: Loss 1.68500554561615\n",
      "Iter 6300: Loss 1.759196400642395\n",
      "Iter 6300: Loss 1.759196400642395\n",
      "Iter 6400: Loss 1.6218440532684326\n",
      "Iter 6400: Loss 1.6218440532684326\n",
      "Iter 6500: Loss 1.6076982021331787\n",
      "Iter 6500: Loss 1.6076982021331787\n",
      "Iter 6600: Loss 1.6510440111160278\n",
      "Iter 6600: Loss 1.6510440111160278\n",
      "Iter 6700: Loss 1.602644443511963\n",
      "Iter 6700: Loss 1.602644443511963\n",
      "Iter 6800: Loss 1.6531577110290527\n",
      "Iter 6800: Loss 1.6531577110290527\n",
      "Iter 6900: Loss 1.6079708337783813\n",
      "Iter 6900: Loss 1.6079708337783813\n",
      "Iter 7000: Loss 1.494940161705017\n",
      "Iter 7000: Loss 1.494940161705017\n",
      "Iter 7100: Loss 1.5469911098480225\n",
      "Iter 7100: Loss 1.5469911098480225\n",
      "Iter 7200: Loss 1.6276601552963257\n",
      "Iter 7200: Loss 1.6276601552963257\n",
      "Iter 7300: Loss 1.5978686809539795\n",
      "Iter 7300: Loss 1.5978686809539795\n",
      "Iter 7400: Loss 1.6286286115646362\n",
      "Iter 7400: Loss 1.6286286115646362\n",
      "Iter 7500: Loss 1.6097403764724731\n",
      "Iter 7500: Loss 1.6097403764724731\n",
      "Iter 7600: Loss 1.541070818901062\n",
      "Iter 7600: Loss 1.541070818901062\n",
      "Iter 7700: Loss 1.5871002674102783\n",
      "Iter 7700: Loss 1.5871002674102783\n",
      "Iter 7800: Loss 1.661765694618225\n",
      "Iter 7800: Loss 1.661765694618225\n",
      "Iter 7900: Loss 1.6268151998519897\n",
      "Iter 7900: Loss 1.6268151998519897\n",
      "Iter 8000: Loss 1.5976512432098389\n",
      "Iter 8000: Loss 1.5976512432098389\n",
      "Iter 8100: Loss 1.4428390264511108\n",
      "Iter 8100: Loss 1.4428390264511108\n",
      "Iter 8200: Loss 1.4889333248138428\n",
      "Iter 8200: Loss 1.4889333248138428\n",
      "Iter 8300: Loss 1.4728378057479858\n",
      "Iter 8300: Loss 1.4728378057479858\n",
      "Iter 8400: Loss 1.5379178524017334\n",
      "Iter 8400: Loss 1.5379178524017334\n",
      "Iter 8500: Loss 1.45418381690979\n",
      "Iter 8500: Loss 1.45418381690979\n",
      "Iter 8600: Loss 1.5008013248443604\n",
      "Iter 8600: Loss 1.5008013248443604\n",
      "Iter 8700: Loss 1.4854367971420288\n",
      "Iter 8700: Loss 1.4854367971420288\n",
      "Iter 8800: Loss 1.470922827720642\n",
      "Iter 8800: Loss 1.470922827720642\n",
      "Iter 8900: Loss 1.4226855039596558\n",
      "Iter 8900: Loss 1.4226855039596558\n",
      "Iter 9000: Loss 1.4337985515594482\n",
      "Iter 9000: Loss 1.4337985515594482\n",
      "Iter 9100: Loss 1.4176126718521118\n",
      "Iter 9100: Loss 1.4176126718521118\n",
      "Iter 9200: Loss 1.461421251296997\n",
      "Iter 9200: Loss 1.461421251296997\n",
      "Iter 9300: Loss 1.437751054763794\n",
      "Iter 9300: Loss 1.437751054763794\n",
      "Iter 9400: Loss 1.444089651107788\n",
      "Iter 9400: Loss 1.444089651107788\n",
      "Iter 9500: Loss 1.4437243938446045\n",
      "Iter 9500: Loss 1.4437243938446045\n",
      "Iter 9600: Loss 1.4167723655700684\n",
      "Iter 9600: Loss 1.4167723655700684\n",
      "Iter 9700: Loss 1.370618462562561\n",
      "Iter 9700: Loss 1.370618462562561\n",
      "Iter 9800: Loss 1.405608892440796\n",
      "Iter 9800: Loss 1.405608892440796\n",
      "Iter 9900: Loss 1.3800289630889893\n",
      "Iter 9900: Loss 1.3800289630889893\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # model.eval()\n",
    "    # decoder.eval()\n",
    "    # emb_func.eval()\n",
    "    w = get_batch('train', batch_size)  # already on device\n",
    "    w_emb = emb_func(w)  # (B, T, C)\n",
    "    x0 = w_emb + 0.2 * torch.randn_like(w_emb)  # use randn_like for Gaussian noise\n",
    "    \n",
    "    total_loss = 0\n",
    "    mu_T = fwd_sample(x0,torch.tensor([T]*batch_size,device=device),alphas)\n",
    "    total_loss += torch.mean(mu_T**2)\n",
    "    # for t_step in range(T + 1):\n",
    "        # t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "        \n",
    "    t_tensor = torch.randint(1, T + 1, (batch_size,), device=device)\n",
    "    xt = fwd_sample(x0, t_tensor, alphas)\n",
    "    x0_cap = model(xt, t_tensor)\n",
    "    total_loss += F.mse_loss(x0_cap, x0)\n",
    "    \n",
    "    # Final step loss\n",
    "    t_one = torch.tensor([1] * batch_size, device=device)\n",
    "    total_loss += F.mse_loss(model(fwd_sample(x0, t_one, alphas), t_one), w_emb)\n",
    "\n",
    "    # Decoder cross-entropy loss\n",
    "    logits = decoder(x0)  # (B, T, V)\n",
    "    V = config.n_vocab\n",
    "    logits_flat = logits.view(-1, V)  # (B*T, V)\n",
    "    targets_flat = w.view(-1)  # (B*T,)\n",
    "    total_loss += F.cross_entropy(logits_flat, targets_flat)\n",
    "    # print(log_loss)\n",
    "\n",
    "    optimizer_model.zero_grad(set_to_none=True)\n",
    "    optimizer_model_decoder.zero_grad(set_to_none=True)\n",
    "    optimizer_emb.zero_grad(set_to_none=True)\n",
    "    total_loss.backward()\n",
    "    optimizer_model.step()\n",
    "    optimizer_model_decoder.step()\n",
    "    optimizer_emb.step()\n",
    "    \n",
    "    if iter%100 == 0:\n",
    "        print(f\"Iter {iter}: Loss {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc432a96",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad887e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reverse diffusion inference with clamping...\n",
      "\n",
      "==================================================\n",
      "GENERATED TEXT:\n",
      "==================================================\n",
      " home was asked a too his to At. the't day too go a, the home a a started so on decided a do to something the go to get been always store. She were all out to my of were butI the get told that friends the back took day house day for However the when. made like the by's so and.My. told. but were. the. The a day would week. was to, a people an found was money. an. do lot she She's one thought of I decided store the He So at\n",
      " got. the with He Now found for. told his. school school to in got. him more them she he themI I with. She to The's It could with She. happy play didn that water broke, decided. made When told\n",
      " the so into his way of a. everyone dog made were of When She a birthday she Finally was decided a many store for, of. house\n",
      " few asked The I she out gave them what home did She to gave\n",
      " Her around I would One needed.'s When of it like When. excited wanted house him in wanted. his took He. off did. no she water no go their them She He home. the when their that. but an were.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "GENERATED TEXT:\n",
      "==================================================\n",
      " home was asked a too his to At. the't day too go a, the home a a started so on decided a do to something the go to get been always store. She were all out to my of were butI the get told that friends the back took day house day for However the when. made like the by's so and.My. told. but were. the. The a day would week. was to, a people an found was money. an. do lot she She's one thought of I decided store the He So at\n",
      " got. the with He Now found for. told his. school school to in got. him more them she he themI I with. She to The's It could with She. happy play didn that water broke, decided. made When told\n",
      " the so into his way of a. everyone dog made were of When She a birthday she Finally was decided a many store for, of. house\n",
      " few asked The I she out gave them what home did She to gave\n",
      " Her around I would One needed.'s When of it like When. excited wanted house him in wanted. his took He. off did. no she water no go their them She He home. the when their that. but an were.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def reverse_diffusion_with_clamping(model, emb_func, alphas, T, context_length=50, batch_size=1):\n",
    "    \"\"\"\n",
    "    Performs reverse diffusion with clamping trick from Diffusion-LM.\n",
    "    At each step, clamps the predicted x0 to nearest word embedding.\n",
    "    \n",
    "    Formula: x_{t-1} = sqrt(alpha_{t-1}) * Clamp(f_theta(x_t, t)) + sqrt(1 - alpha_{t-1}) * epsilon\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Denoiser model\n",
    "        emb_func: Trained embedding function (for clamping to nearest word)\n",
    "        alphas: Alpha_bar schedule tensor on device\n",
    "        T: Number of diffusion timesteps\n",
    "        context_length: Length of sequence to generate\n",
    "        batch_size: Number of sequences to generate\n",
    "    \n",
    "    Returns:\n",
    "        generated_tokens: Token IDs of generated sequences (B, T)\n",
    "        generated_text: Decoded text strings\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    emb_func.eval()\n",
    "    \n",
    "    # Start from pure noise: x_T ~ N(0, I)\n",
    "    x_t = torch.randn(batch_size, context_length, config.n_embed, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Reverse diffusion: t = T, T-1, ..., 1, 0\n",
    "        for t_step in reversed(range(T + 1)):\n",
    "            # print(f\"Denoising step {t_step}/{T}\")\n",
    "            \n",
    "            if t_step == 0:\n",
    "                # Final step: just clamp to get x_0\n",
    "                x_0 = x_t\n",
    "                break\n",
    "            \n",
    "            # Create timestep tensor for batch\n",
    "            t_tensor = torch.tensor([t_step] * batch_size, device=device)\n",
    "            \n",
    "            # Predict x_0 from x_t using the denoiser\n",
    "            x0_pred = model(x_t, t_tensor)\n",
    "            \n",
    "            # CLAMPING TRICK: Map predicted x_0 to nearest word embedding\n",
    "            # This forces intermediate predictions to be valid words\n",
    "            x0_clamped_tokens = finalize_tokens(x0_pred, emb_func.embed.weight)\n",
    "            x0_clamped = emb_func(x0_clamped_tokens)  # (B, T, C)\n",
    "            \n",
    "            # Compute x_{t-1} using the formula:\n",
    "            # x_{t-1} = sqrt(alpha_{t-1}) * x0_clamped + sqrt(1 - alpha_{t-1}) * epsilon\n",
    "            \n",
    "            alpha_t_prev = alphas[t_step - 1] if t_step > 0 else alphas[0]\n",
    "            \n",
    "            # Sample fresh noise\n",
    "            epsilon = torch.randn_like(x_t)\n",
    "            \n",
    "            # Update: x_{t-1} = sqrt(alpha_{t-1}) * x0_clamped + sqrt(1 - alpha_{t-1}) * epsilon\n",
    "            x_t = torch.sqrt(alpha_t_prev) * x0_clamped + torch.sqrt(1 - alpha_t_prev) * epsilon\n",
    "    \n",
    "    # Final denoised embeddings: x_0\n",
    "    x0_final = x_t\n",
    "    \n",
    "    # Convert to tokens using argmin rounding with learned embeddings\n",
    "    generated_tokens = finalize_tokens(x0_final, emb_func.embed.weight)\n",
    "    \n",
    "    # Decode to text\n",
    "    generated_text = []\n",
    "    for i in range(batch_size):\n",
    "        text = tokenizer.decode(generated_tokens[i].tolist())\n",
    "        generated_text.append(text)\n",
    "    \n",
    "    return generated_tokens, generated_text\n",
    "\n",
    "\n",
    "# Run inference\n",
    "print(\"Starting reverse diffusion inference with clamping...\")\n",
    "context_length = 256\n",
    "generated_tokens, generated_text = reverse_diffusion_with_clamping(\n",
    "    model=model,\n",
    "    emb_func=emb_func,\n",
    "    alphas=alphas,\n",
    "    T=T,\n",
    "    context_length=context_length,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATED TEXT:\")\n",
    "print(\"=\"*50)\n",
    "print(generated_text[0])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d05cf74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = emb_func(torch.tensor([tokenizer.encode(\"!\")], device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efc4c7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2571, -0.0819,  1.0992, -0.5341, -0.3827, -0.3746,  0.7433,\n",
       "           0.8786,  0.9243,  0.8089, -0.4689, -0.2725, -0.3584,  0.5885,\n",
       "          -0.3957, -0.1734, -0.2059, -0.2432,  0.2523,  1.1037, -0.3995,\n",
       "           0.5379, -0.6829,  0.4967,  0.0608,  0.0281,  0.4738, -0.0913,\n",
       "          -0.8046, -0.0025, -0.9382, -0.5513,  0.2202, -0.1311, -0.0457,\n",
       "           0.6176, -0.7737,  0.1629,  0.4038,  0.0137,  0.6967, -0.6890,\n",
       "          -0.2077, -0.2974,  0.0478,  0.2556,  0.6905, -0.0358,  0.4670,\n",
       "           0.6208,  0.5352, -0.8628, -0.3029,  0.0031, -0.7207, -0.7787,\n",
       "           0.6197, -0.0059,  0.5145, -0.4796,  0.1727,  0.0291, -0.6105,\n",
       "          -0.5873]]], device='cuda:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
